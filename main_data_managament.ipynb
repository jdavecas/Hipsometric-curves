{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWOT DATA MANAGEMENT\n",
    "by J. Daniel Velez\n",
    "\n",
    "The code is split in several sections:\n",
    "\n",
    "**1. Import SWOT files based on the passes overlapping a basin:** Passes should be typing manually or using the column pass in a shapefile.\n",
    "\n",
    "**2. Data arrange:** Is intended to arrange the shapefiles in a suitable format for processing\n",
    "\n",
    "**3. Estimation of Statistics:** This code filter out the River product shapes based, identify and eliminate outliers, calculates the correlation coefficients, the max and min width values (range), the median, 1-sigma and 2-sigma,  and plot CDFs estimates the Spearman and Pearson correlation coefficients.\n",
    "\n",
    "    * Data will be filtering using the bitwise \"quality_q_b\" column flags\n",
    "\n",
    "    * Outliers will be identified using the interquartile range (IQR) method\n",
    "    \n",
    "    * Correlation coefficients methods used in this code: Spearman and Pearson. (A new correlation coefficient method will be tested, Î¾)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from f_download_data import *\n",
    "from f_data_arrange import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Define the basin to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuenca = 'Ohio' #change the name of the basin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Identify and download passes\n",
    "If shapefiles already downloaded, this section can be skipped\n",
    "**function to identify the passes to download:** passes_to_download('Basin'). Currently Basin are: Atrato, Magdalena, Ohio, Yukon, Tanana, Po\n",
    "\n",
    "**function to download the passes for the specific basin:** download_data('Basin', passes_in_basin, 'continent', 'start_date',' reach_node')\n",
    "\n",
    "        - 'Basin': as described above (string)\n",
    "\n",
    "        - passes_in_basins: variable where the passes are stored (list)\n",
    "\n",
    "        - 'continent': You must enter the short name of the continent\n",
    "                    - AF: Africa\n",
    "                    - EU: Europe and Middle East\n",
    "                    - SI: Siberia\n",
    "                    - AS: Central and SE Asia\n",
    "                    - AU: Australia and Oceania\n",
    "                    - SA: South America\n",
    "                    - NA: North America and Caribbean\n",
    "                    - AR: North American Artic\n",
    "                    - GR: Greenland\n",
    "\n",
    "        - 'start_date': initial date to search for data, must follow this format '2023-07-23 00:00:00'\n",
    "\n",
    "        - 'reach_node': should specify if Node or Reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passes_in_basins = passes_to_download(cuenca) # if you don't know the pass, just hit enter key\n",
    "print(passes_in_basins)\n",
    "download_data(cuenca, passes_in_basins, 'NA', '2024-10-6', 'Node') # last date: 2024-09-02 00:00:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Arrange the data in json dictionary\n",
    "Take the selected shapes' columns in each basin and create a dictionary  whether from Nodes or Reaches, and save them as dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_shape(Basin, reach_node,r_n_id):\n",
    "dictio_basin = process_shape(cuenca, 'Node', 'node_id')\n",
    "print(dictio_basin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function in case that in the previous functions the unzip part doesn't work\n",
    "unzip_files_in_directory('/Users/josele/Library/CloudStorage/GoogleDrive-ejdvc757@gmail.com/Other computers/My MacBook Pro/PhD/Dissertation/0_data/External/Basins/Tamlin/shapes/Node')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_json(dictio_basin)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

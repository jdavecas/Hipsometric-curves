{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import hvplot.dask\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import requests\n",
    "\n",
    "import datetime\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=10)    # Set to n workers for number of CPUs or parallel processes or set to 1 worker to run things on a single thread\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign URLs to Variables for the APIs we use, FTS and Hydrocron\n",
    "FTS_URL = \"https://fts.podaac.earthdata.nasa.gov/v1\"  \n",
    "HYDROCRON_URL = \"https://soto.podaac.earthdatacloud.nasa.gov/hydrocron/v1/timeseries\"\n",
    "\n",
    "# BASIN or RIVER to query FTS for\n",
    "BASIN_IDENTIFIER = \"732\" # to search via basin ID, find within SWORD database\n",
    "#RIVER_NAME = \"Rhine\" # to search via river name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_fts(query_url, params):\n",
    "    \"\"\"Query Feature Translation Service (FTS) for reach identifers using the query_url parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_url: str - URL to use to query FTS\n",
    "    params: dict - Dictionary of parameters to pass to query\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of results: hits, page_size, page_number, reach_ids\n",
    "    \"\"\"\n",
    "\n",
    "    nodes = requests.get(query_url, params=params)\n",
    "    nodes_json = nodes.json()\n",
    "\n",
    "    hits = nodes_json['hits']\n",
    "    if 'search on' in nodes_json.keys():\n",
    "        page_size = nodes_json['search on']['page_size']\n",
    "        page_number = nodes_json['search on']['page_number']\n",
    "    else:\n",
    "        page_size = 0\n",
    "        page_number = 0\n",
    "\n",
    "    return {\n",
    "        \"hits\": hits,\n",
    "        \"page_size\": page_size,\n",
    "        \"page_number\": page_number,\n",
    "        \"node_ids\": [ item['node_id'] for item in nodes_json['results'] ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by basin code\n",
    "query_url = f\"{FTS_URL}/rivers/node/{BASIN_IDENTIFIER}\"\n",
    "print(f\"Searching by basin... {query_url}\")\n",
    "\n",
    "page_size = 100   # Retrieve 100 results per request\n",
    "page_number = 1   # Start with page 1\n",
    "node_ids = []    # Initialize empty list for IDs\n",
    "\n",
    "# --- Replace your previous loop with this improved version ---\n",
    "while True:\n",
    "    params = {\"page_size\": page_size, \"page_number\": page_number}\n",
    "    results = query_fts(query_url, params)\n",
    "\n",
    "    # Check if the response contains valid node_ids\n",
    "    if 'node_ids' not in results or not results['node_ids']:\n",
    "        print(f\"No more node IDs returned at page {page_number}. Ending loop.\")\n",
    "        break\n",
    "\n",
    "    hits = results.get('hits', 0)\n",
    "    node_ids.extend(results['node_ids'])\n",
    "\n",
    "    print(\n",
    "        f\"page_size: {page_size}, \"\n",
    "        f\"page_number: {page_number}, \"\n",
    "        f\"hits: {hits}, \"\n",
    "        f\"# node_ids: {len(node_ids)}\"\n",
    "    )\n",
    "\n",
    "    # Stop condition: all hits fetched\n",
    "    if len(node_ids) >= hits:\n",
    "        print(\"All available node IDs fetched.\")\n",
    "        break\n",
    "\n",
    "    page_number += 1\n",
    "\n",
    "# --- Your existing final checks remain unchanged ---\n",
    "print(\"Total number of node: \", len(node_ids))\n",
    "node_ids = list(set(node_ids))  # remove duplicates\n",
    "print(\"Total number of non-duplicate reaches: \", len(node_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def query_hydrocron(query_url, node_id, start_time, end_time, fields):\n",
    "    \"\"\"Query Hydrocron for node-level time series data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query_url: str - URL to use to query FTS\n",
    "    node_id: str - String SWORD node identifier\n",
    "    start_time: str - String time to start query\n",
    "    end_time: str - String time to end query\n",
    "    fields: list - List of fields to return in query response\n",
    "    empty_df: pandas.DataFrame that contains empty query results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame that contains query results\n",
    "    \"\"\"\n",
    "\n",
    "    params = {\n",
    "        \"feature\": \"Node\",\n",
    "        \"feature_id\": node_id,\n",
    "        \"output\": \"csv\",\n",
    "        \"start_time\": start_time,\n",
    "        \"end_time\": end_time,\n",
    "        \"fields\": fields\n",
    "    }\n",
    "    results = requests.get(query_url, params=params)\n",
    "    if \"results\" in results.json().keys():\n",
    "        results_csv = results.json()[\"results\"][\"csv\"]\n",
    "        df = pd.read_csv(StringIO(results_csv))\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "            \"node_id\": [np.int64(node_id)],\n",
    "            \"time_str\": [datetime.datetime(1900, 1, 1).strftime(\"%Y-%m-%dT%H:%M:%S\")],\n",
    "            \"wse\": [-999999999999.0],\n",
    "            \"wse_units\": [\"m\"]\n",
    "        })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import psutil\n",
    "\n",
    "# Define parameters\n",
    "start_time = \"2023-07-28T00:00:00Z\"\n",
    "end_time = \"2024-04-16T00:00:00Z\"\n",
    "fields = \"node_id,time_str,lat,lon,wse,width,node_q_b,dark_frac,ice_clim_f,ice_dyn_f,p_dist_out\"\n",
    "\n",
    "# Batch processing to reduce task graph size\n",
    "\n",
    "available_mem = psutil.virtual_memory().available  # Free RAM in bytes\n",
    "batch_size = min(500, max(100, available_mem // (10 * 1024**2)))  # 10MB per batch\n",
    "batched_results = []\n",
    "\n",
    "for i in range(0, len(node_ids), batch_size):\n",
    "    batch = node_ids[i : i + batch_size]\n",
    "    \n",
    "    # Create delayed queries for the batch\n",
    "    if __name__ == \"__main__\":\n",
    "        delayed_queries = [query_hydrocron(HYDROCRON_URL, node, start_time, end_time, fields) for node in batch]\n",
    "    \n",
    "    # Compute batch immediately to avoid an excessively large graph\n",
    "    batch_results = dask.compute(*delayed_queries)\n",
    "    \n",
    "    # Convert batch results to a Dask DataFrame\n",
    "    ddf_batch = dd.from_pandas(pd.concat(batch_results, ignore_index=True), npartitions=20)\n",
    "    batched_results.append(ddf_batch)\n",
    "\n",
    "# Combine batches into a final Dask DataFrame\n",
    "ddf = dd.concat(batched_results)\n",
    "\n",
    "# Show the first 20 rows (computed lazily)\n",
    "ddf.head(n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85550f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, mapping, LineString, MultiLineString, shape\n",
    "import rasterio\n",
    "from rasterio.mask import mask as rio_mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyproj import Transformer\n",
    "import glob\n",
    "from shapely.ops import unary_union\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import random\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import math\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1fe522",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c798b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77d0f2",
   "metadata": {},
   "source": [
    "PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d7ea551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/t6lktxg91c5b91pyb0rk6x080000gn/T/ipykernel_23721/850668533.py:2: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  regress_path = pd.read_csv(os.path.join(os.getcwd(), \"3_output/Global_22_07_25/B73_huber_reg.csv\"))#/dark_fr_030/8_bits/S3_15Obs_100W/jul_3_25/Po_River_huber_reg_3.csv\")) #No_Norm/dark_fr_030/8_bits/S3_15Obs_100W/jun_6_25/\n"
     ]
    }
   ],
   "source": [
    "# Regressions\n",
    "regress_path = pd.read_csv(os.path.join(os.getcwd(), \"3_output/Global_22_07_25/B73_huber_reg.csv\"))#/dark_fr_030/8_bits/S3_15Obs_100W/jul_3_25/Po_River_huber_reg_3.csv\")) #No_Norm/dark_fr_030/8_bits/S3_15Obs_100W/jun_6_25/\n",
    "                                        #\"Po_River_huber_reg.csv\"))\n",
    "poly_path = gpd.read_file(os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Cape_fear/Shps/Cape_cx.shp\"))\n",
    "tif_path = os.path.join(os.getcwd(), \"0_data/External/Bathymetries/namerica/NC/cape_fear_egm2008.tif\")\n",
    "base_folder = os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Cape_fear/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871c76",
   "metadata": {},
   "source": [
    "#### 1. Subsetting only the nodes lying inside the raster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d505ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209 nodes inside raster, 236234 total nodes\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    nod = src.nodata\n",
    "    # pull lon/lat from your DataFrame\n",
    "    xs = regress_path[\"lon\"].values\n",
    "    ys = regress_path[\"lat\"].values\n",
    "\n",
    "    # ‚îÄ‚îÄ‚îÄ NEW: reproject point coords into the raster‚Äôs CRS ‚îÄ‚îÄ‚îÄ\n",
    "    xs, ys = transform(\n",
    "        \"EPSG:4326\",    # source CRS of your lon/lat\n",
    "        src.crs,        # target CRS (raster.crs)\n",
    "        xs.tolist(),\n",
    "        ys.tolist(),\n",
    "    )\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Check raster bounds (now in raster CRS)\n",
    "    left, bottom, right, top = src.bounds\n",
    "    inside_bounds = (xs >= left) & (xs <= right) & (ys >= bottom) & (ys <= top)\n",
    "    \n",
    "    # Preallocate array for sampled values (nan for out-of-bounds)\n",
    "    sampled = np.full(xs.shape, np.nan)\n",
    "    xs_in = xs[inside_bounds]\n",
    "    ys_in = ys[inside_bounds]\n",
    "    idxs_in = np.where(inside_bounds)[0]\n",
    "    if len(xs_in) > 0:\n",
    "        vals = list(src.sample(zip(xs_in, ys_in)))\n",
    "        vals = np.array([v[0] for v in vals])\n",
    "        sampled[idxs_in] = vals\n",
    "\n",
    "    # Mask: inside bounds AND not nodata AND not nan\n",
    "    mask = inside_bounds & (~np.isnan(sampled)) & ((nod is None) | (sampled != nod))\n",
    "\n",
    "# subset and export\n",
    "regress_path_inside = regress_path[mask]\n",
    "print(f\"{regress_path_inside.shape[0]} nodes inside raster, {regress_path.shape[0]} total nodes\")\n",
    "\n",
    "regress_path_inside.to_csv(\n",
    "    os.path.join(os.getcwd(),\n",
    "        \"2_intermediate/Store/Validation/csv/Cape_fear/Cape_fear_validate_nodes.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde52126",
   "metadata": {},
   "source": [
    "#### Extract from bathymetry the bottom of the river channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    # 1) show me the raster‚Äôs CRS\n",
    "    print(\"Raster CRS is:\", src.crs)\n",
    "\n",
    "    # 2) if your poly_path is still EPSG:4326, move it into the raster‚Äôs CRS\n",
    "    if poly_path.crs != src.crs:\n",
    "        poly_path = poly_path.to_crs(src.crs)\n",
    "\n",
    "    results = []\n",
    "    for _, row in poly_path.iterrows():\n",
    "        node = row[\"node_id\"]\n",
    "        geom = [mapping(row.geometry)]\n",
    "        out_image, _ = rio_mask(\n",
    "            src,\n",
    "            geom,\n",
    "            crop=True,         # keeps memory use tiny\n",
    "            all_touched=True,\n",
    "            nodata=src.nodata,\n",
    "            filled=False\n",
    "        )\n",
    "        arr = out_image[0].astype(\"float32\")\n",
    "        arr = np.ma.masked_equal(arr, src.nodata)\n",
    "        arr = np.ma.masked_where(np.isnan(arr), arr)\n",
    "        min_val = arr.min() if arr.count() > 0 else np.nan\n",
    "        results.append({\"node_id\": node, \"min_value\": float(min_val)})\n",
    "\n",
    "lprb = pd.DataFrame(results)\n",
    "lprb.to_csv(\n",
    "    os.path.join(\n",
    "        os.getcwd(),\n",
    "        \"2_intermediate/Store/Validation/csv/Garonne/Garonne_channel_min_values.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprb = pd.read_csv(os.path.join(os.getcwd(), \"2_intermediate/Store/Validation/csv/Po/River_channel_min_values.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc401",
   "metadata": {},
   "source": [
    "Merge the two csv files, the one with the regression values and the one with the minimum values in the river bed, and then  \n",
    "estimate the difference and errors between the intercepts and the real bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77007700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure node_id is string in both DataFrames for a valid merge\n",
    "regress_path[\"node_id\"] = regress_path[\"node_id\"].astype(str)\n",
    "lprb[\"node_id\"] = lprb[\"node_id\"].astype(str)\n",
    "\n",
    "merged = regress_path.merge(\n",
    "    lprb[[\"node_id\", \"min_value\"]],\n",
    "    on=\"node_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "intercept = np.where(\n",
    "    merged[\"intercept2\"].isnull(),\n",
    "    merged[\"intercept1\"],\n",
    "    merged[\"intercept2\"]\n",
    ")\n",
    "\n",
    "merged[\"error\"] = intercept - merged[\"min_value\"]\n",
    "merged[\"abs_error\"] = merged[\"error\"].abs()\n",
    "merged[\"rel_error_%\"] = merged[\"error\"] / merged[\"min_value\"] * 100\n",
    "\n",
    "merged.to_csv(os.path.join(os.getcwd(), \"2_intermediate/Store/Validation/csv/Garonne/Garonne_reg_bottom_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b64e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_csv_path = os.path.join(os.getcwd(), \"2_intermediate/Store/Validation/csv/Garonne/Garonne_reg_bottom_error.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13409f",
   "metadata": {},
   "source": [
    "Create folders for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Base directory where you want to create the folders\n",
    "#    For example: \"/path/to/output_folders\"\n",
    "\n",
    "\n",
    "# 3) Read the master CSV into a DataFrame\n",
    "master_df = pd.read_csv(master_csv_path)\n",
    "\n",
    "# 4) Extract unique node_id values (as strings)\n",
    "unique_nodes = master_df[\"node_id\"].astype(str).unique()\n",
    "\n",
    "# 5) Write the list of node_id codes to a CSV (no header, one per line)\n",
    "list_csv_path = os.path.join(base_folder, \"node_ids_list.csv\")\n",
    "pd.Series(unique_nodes).to_csv(list_csv_path, index=False, header=False)\n",
    "\n",
    "# 6) Loop over each unique node_id and create a folder if it doesn't already exist\n",
    "for node_id in unique_nodes:\n",
    "    node_folder = os.path.join(base_folder, node_id)\n",
    "    # exist_ok=True means ‚Äúdo nothing if the folder already exists‚Äù\n",
    "    os.makedirs(node_folder, exist_ok=True)\n",
    "    print(f\"Ensured folder exists: {node_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0bcba",
   "metadata": {},
   "source": [
    "Create in each node_id folder, a csv file with a list of WSE extracted from the regression csv for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_wse_csvs(\n",
    "    master_csv_path: str,\n",
    "    base_folder: str,\n",
    "    subtract_value: float = 2.04,\n",
    "    apply_subtraction: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each node_id in master_df, write:\n",
    "        1) raw WSEs ‚Üí base_folder/node_id/node_id.csv\n",
    "        2) (optionally) adjusted WSEs (WSE - subtract_value)\n",
    "            ‚Üí base_folder/node_id/node_id_adjusted.csv\n",
    "\n",
    "    Args:\n",
    "        master_csv_path: path to CSV with columns 'node_id' and 'wse'\n",
    "        base_folder: directory that will contain subfolders named by node_id\n",
    "        subtract_value: the value to subtract from each WSE when apply_subtraction=True\n",
    "        apply_subtraction: if True, also create the adjusted CSV\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(master_csv_path):\n",
    "        print(f\"Error: File '{master_csv_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the master table\n",
    "    df = pd.read_csv(master_csv_path)\n",
    "\n",
    "    # Loop through each distinct node_id\n",
    "    for node_id in df[\"node_id\"].astype(str).unique():\n",
    "        # Grab all the WSE values for this node\n",
    "        wse_values = df.loc[df[\"node_id\"].astype(str) == node_id, \"wse\"]\n",
    "\n",
    "        # Ensure the node's folder exists\n",
    "        node_folder = os.path.join(base_folder, node_id)\n",
    "        os.makedirs(node_folder, exist_ok=True)\n",
    "\n",
    "        # 1) Write the raw WSEs\n",
    "        raw_path = os.path.join(node_folder, f\"{node_id}.csv\")\n",
    "        wse_values.to_csv(raw_path, index=False, header=False)\n",
    "        print(f\"‚úî Saved {len(wse_values)} raw WSEs to {raw_path}\")\n",
    "\n",
    "        # 2) If requested, write the adjusted WSEs\n",
    "        if apply_subtraction:\n",
    "            adjusted = wse_values - subtract_value\n",
    "            adj_path = os.path.join(node_folder, f\"{node_id}_adjusted.csv\")\n",
    "            adjusted.to_csv(adj_path, index=False, header=False)\n",
    "            print(f\"‚úî Saved {len(adjusted)} adjusted WSEs to {adj_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just export raw WSE lists:\n",
    "export_wse_csvs(master_csv_path, base_folder)\n",
    "\n",
    "# Export raw WSE lists *and* create adjusted CSVs (subtract 2.04):\n",
    "#export_wse_csvs(master_csv_path, base_folder, apply_subtraction=True)\n",
    "\n",
    "# Export with a different subtraction value, say 1.5:\n",
    "#export_wse_csvs(master_csv_path, base_folder, subtract_value=1.5, apply_subtraction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3e509",
   "metadata": {},
   "source": [
    "Create a mask for each WSE in each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4ef3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETTINGS ---\n",
    "\n",
    "# (Optional) If you only want to process a subset of node_ids, list them here.\n",
    "# Otherwise set `limit_to` = None to process every subfolder under `base_folder`.\n",
    "limit_to = None\n",
    "# Example to process only two nodes:\n",
    "# limit_to = [\"21406100080691\", \"21406100080702\"]\n",
    "\n",
    "\n",
    "# === STEP 1: Read the DEM ONCE ===\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    # Read the full DEM array at native resolution\n",
    "    dem = src.read(1, resampling=Resampling.nearest)\n",
    "    profile = src.profile.copy()\n",
    "    nodata_value = src.nodata if src.nodata is not None else -9999\n",
    "\n",
    "# Update the profile for writing uint8 masks with 1=water, 0=non-water, 255=nodata\n",
    "profile.update({\n",
    "    \"dtype\": rasterio.uint8,\n",
    "    \"count\": 1,\n",
    "    \"nodata\": 255,\n",
    "    \"compress\": \"lzw\"\n",
    "})\n",
    "\n",
    "# === STEP 2: Locate all node_id subfolders under `base_folder` ===\n",
    "\n",
    "# Use glob to find all directories directly under `base_folder`\n",
    "node_folders = [\n",
    "    d for d in glob.glob(os.path.join(base_folder, \"*\"))\n",
    "    if os.path.isdir(d)\n",
    "]\n",
    "\n",
    "# If user specified a subset, filter to only those names\n",
    "if limit_to is not None:\n",
    "    node_folders = [\n",
    "        d for d in node_folders\n",
    "        if os.path.basename(d) in limit_to\n",
    "    ]\n",
    "\n",
    "print(f\"üîç Found {len(node_folders)} node‚Äêfolders to process.\\n\")\n",
    "\n",
    "\n",
    "# === STEP 3: Loop over each node_folder ===\n",
    "\n",
    "for node_folder in node_folders:\n",
    "    node_id = os.path.basename(node_folder)\n",
    "    print(f\" Processing node_id = {node_id}\")\n",
    "\n",
    "    # 3a) Find all CSV files in this folder\n",
    "    csv_files = glob.glob(os.path.join(node_folder, \"*.csv\"))\n",
    "    if not csv_files:\n",
    "        print(f\" No CSVs found inside {node_folder}. Skipping.\\n\")\n",
    "        continue\n",
    "\n",
    "    # 3b) For each CSV, read its WSE values (single‚Äêcolumn, no header)\n",
    "    for csv_path in csv_files:\n",
    "        # We assume each CSV has one column with no header (just WSE floats/ints).\n",
    "        try:\n",
    "            df_wse = pd.read_csv(csv_path, header=None, names=[\"wse\"], dtype=float)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {csv_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        wse_values = df_wse[\"wse\"].dropna().values\n",
    "        if wse_values.size == 0:\n",
    "            print(f\"No valid WSE values in {csv_path}. Skipping this file.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Found {len(wse_values)} thresholds in {os.path.basename(csv_path)}\")\n",
    "\n",
    "        # 3c) Create an output subfolder for these masks, if desired.\n",
    "        #     You can write masks back into node_folder, or into a subfolder:\n",
    "        masks_folder = os.path.join(node_folder, \"masks\")\n",
    "        os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "        # 3d) For each WSE threshold, build a binary mask\n",
    "        for t in wse_values:\n",
    "            # Build a safe filename: replace dot with underscore, and strip any minus sign\n",
    "            t_str = str(t).replace(\".\", \"_\").replace(\"-\", \"m\")\n",
    "            out_name = f\"mask_{t_str}.tif\"\n",
    "            out_path = os.path.join(masks_folder, out_name)\n",
    "\n",
    "            # If the mask already exists, skip (do not overwrite)\n",
    "            if os.path.exists(out_path):\n",
    "                print(f\"      ‚Ä¢ {out_name} already exists. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Build the mask: water where DEM ‚â§ t, non-water otherwise\n",
    "            # First, initialize all to 0 (non-water)\n",
    "            mask = np.zeros_like(dem, dtype=np.uint8)\n",
    "\n",
    "            # Where DEM is valid and DEM ‚â§ threshold ‚Üí set to 1 (water)\n",
    "            valid = dem != nodata_value\n",
    "            mask[np.logical_and(valid, dem <= t)] = 1\n",
    "\n",
    "            # Where DEM is nodata ‚Üí set to 255\n",
    "            mask[dem == nodata_value] = 255\n",
    "\n",
    "            # Write out the mask TIFF\n",
    "            with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                dst.write(mask, 1)\n",
    "\n",
    "            print(f\"Saved {out_name}\")\n",
    "\n",
    "    print(\"\")  # blank line before next node\n",
    "\n",
    "print(\"All node‚Äêfolders processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_to    = None                     # or e.g. [\"21406100080691\", \"21406100080702\"]\n",
    "\n",
    "# Tunable: number of worker threads for writing masks\n",
    "_max_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "def _write_mask_single(out_path, profile, dem, valid, nodata_mask, nodata_value, t):\n",
    "    \"\"\"Build and write one mask TIFF for threshold t.\"\"\"\n",
    "    mask = np.zeros_like(dem, dtype=np.uint8)\n",
    "    np.less_equal(dem, t, where=valid, out=mask, casting=\"unsafe\")\n",
    "    mask[nodata_mask] = 255\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(mask, 1)\n",
    "    return out_path\n",
    "\n",
    "with rasterio.Env(GDAL_NUM_THREADS=\"ALL_CPUS\", VSI_CACHE=\"TRUE\", VSI_CACHE_SIZE=str(256*1024*1024)):\n",
    "    # === STEP 1: Read the DEM ONCE ===\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        dem = src.read(1, resampling=Resampling.nearest)\n",
    "        profile = src.profile.copy()\n",
    "        nodata_value = src.nodata if src.nodata is not None else -9999\n",
    "\n",
    "    profile.update({\n",
    "        \"dtype\": rasterio.uint8,\n",
    "        \"count\": 1,\n",
    "        \"nodata\": 255,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    nodata_mask = (dem == nodata_value)\n",
    "    valid = ~nodata_mask\n",
    "\n",
    "    # === STEP 2: Locate all node_id subfolders ===\n",
    "    node_folders = [d for d in glob.glob(os.path.join(base_folder, \"*\")) if os.path.isdir(d)]\n",
    "    if limit_to is not None:\n",
    "        node_folders = [d for d in node_folders if os.path.basename(d) in limit_to]\n",
    "\n",
    "    print(f\"üîç Found {len(node_folders)} node‚Äêfolders to process.\\n\")\n",
    "\n",
    "    # === STEP 3: Process each folder ===\n",
    "    for node_folder in node_folders:\n",
    "        node_id = os.path.basename(node_folder)\n",
    "        print(f\" Processing node_id = {node_id}\")\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(node_folder, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            print(f\" No CSVs found inside {node_folder}. Skipping.\\n\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_files:\n",
    "            try:\n",
    "                df_wse = pd.read_csv(csv_path, header=None, names=[\"wse\"], dtype=float)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {csv_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            wse_values = df_wse[\"wse\"].dropna().values\n",
    "            if wse_values.size == 0:\n",
    "                print(f\"No valid WSE values in {csv_path}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Found {len(wse_values)} thresholds in {os.path.basename(csv_path)}\")\n",
    "\n",
    "            masks_folder = os.path.join(node_folder, \"masks\")\n",
    "            os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "            futures = []\n",
    "            with ThreadPoolExecutor(max_workers=_max_workers) as ex:\n",
    "                for t in wse_values:\n",
    "                    t_str = str(t).replace(\".\", \"_\").replace(\"-\", \"m\")\n",
    "                    out_name = f\"mask_{t_str}.tif\"\n",
    "                    out_path = os.path.join(masks_folder, out_name)\n",
    "\n",
    "                    if os.path.exists(out_path):\n",
    "                        print(f\"      ‚Ä¢ {out_name} already exists. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    futures.append(\n",
    "                        ex.submit(_write_mask_single, out_path, profile, dem, valid, nodata_mask, nodata_value, t)\n",
    "                    )\n",
    "\n",
    "                for f in as_completed(futures):\n",
    "                    try:\n",
    "                        done_path = f.result()\n",
    "                        print(f\"Saved {os.path.basename(done_path)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ‚Ä¢ Failed to write a mask: {e}\")\n",
    "\n",
    "        print(\"\")  # blank line\n",
    "\n",
    "    print(\"All node‚Äêfolders processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454f46b",
   "metadata": {},
   "source": [
    "#### Measuring river widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "\n",
    "def ensure_mask_georeferenced(mask_path, ref_crs, ref_transform):\n",
    "    with rasterio.open(mask_path, 'r+') as dst:\n",
    "        needs_update = False\n",
    "        if dst.crs is None:\n",
    "            dst.crs = ref_crs\n",
    "            needs_update = True\n",
    "        if dst.transform.is_identity:\n",
    "            dst.transform = ref_transform\n",
    "            needs_update = True\n",
    "        if needs_update:\n",
    "            print(f\"Updated georeferencing for: {os.path.basename(mask_path)}\")\n",
    "\n",
    "def fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform):\n",
    "    for node_folder in os.listdir(base_folder):\n",
    "        masks_dir = os.path.join(base_folder, node_folder, \"masks\")\n",
    "        if os.path.isdir(masks_dir):\n",
    "            for mask_path in glob.glob(os.path.join(masks_dir, \"mask_*.tif\")):\n",
    "                ensure_mask_georeferenced(mask_path, ref_crs, ref_transform)\n",
    "\n",
    "def extract_wse(mask_filename):\n",
    "    # strip extension and split on underscores\n",
    "    name = os.path.splitext(os.path.basename(mask_filename))[0]\n",
    "    parts = name.split('_')   # [\"mask\", \"50\", \"535\"]\n",
    "    if len(parts) == 3:\n",
    "        return float(f\"{parts[1]}.{parts[2]}\")  # \"50\" + \".\" + \"535\" ‚Üí 50.535\n",
    "    return None\n",
    "\n",
    "def possible_node_id_strings(node_id):\n",
    "    candidates = set()\n",
    "    candidates.add(str(node_id))\n",
    "    try:\n",
    "        as_float = float(node_id)\n",
    "        candidates.add(str(int(as_float)))\n",
    "        candidates.add(f\"{as_float:.3f}\")\n",
    "        candidates.add(f\"{as_float:.1f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return list(candidates)\n",
    "\n",
    "def measure_mask_segments_by_sampling(polyline, mask_path, gdf_crs, spacing=1):\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        raster_crs = src.crs\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ REPLACED: use the mask/raster CRS (e.g. EPSG:6339) instead of Italy UTM ‚îÄ‚îÄ‚îÄ\n",
    "        utm_crs = raster_crs\n",
    "\n",
    "        if gdf_crs and gdf_crs != utm_crs:\n",
    "            polyline_proj = gpd.GeoSeries([polyline], crs=gdf_crs).to_crs(utm_crs).iloc[0]\n",
    "        else:\n",
    "            polyline_proj = polyline\n",
    "\n",
    "        length = polyline_proj.length\n",
    "        if length < spacing:\n",
    "            return []\n",
    "\n",
    "        num_points = int(length / spacing)\n",
    "        distances = np.linspace(0, length, num_points)\n",
    "        points = [polyline_proj.interpolate(d) for d in distances]\n",
    "\n",
    "        coords = gpd.GeoSeries(points, crs=utm_crs).to_crs(raster_crs)\n",
    "        coords = [(pt.x, pt.y) for pt in coords]\n",
    "\n",
    "        vals = [v[0] for v in src.sample(coords)]\n",
    "        vals = [int(round(v)) if v in [0, 1] else 0 for v in vals]\n",
    "\n",
    "        segments = []\n",
    "        in_segment = False\n",
    "        start_dist = None\n",
    "\n",
    "        for i in range(1, len(vals)):\n",
    "            prev, curr = vals[i - 1], vals[i]\n",
    "            if not in_segment and prev == 0 and curr == 1:\n",
    "                in_segment = True\n",
    "                start_dist = distances[i]\n",
    "            elif in_segment and prev == 1 and curr == 0:\n",
    "                end_dist = distances[i]\n",
    "                seg_len = end_dist - start_dist\n",
    "                if seg_len >= 20:\n",
    "                    segments.append(seg_len)\n",
    "                in_segment = False\n",
    "\n",
    "        return segments\n",
    "\n",
    "def process_single_polyline(args):\n",
    "    idx, row, base_folder, node_id_field, gdf_crs = args\n",
    "    node_id_raw = row[node_id_field]\n",
    "    possible_names = possible_node_id_strings(node_id_raw)\n",
    "    folder_path = None\n",
    "    used_node_id = None\n",
    "    for node_id in possible_names:\n",
    "        test_path = os.path.join(base_folder, node_id, 'masks')\n",
    "        if os.path.exists(test_path):\n",
    "            folder_path = test_path\n",
    "            used_node_id = node_id\n",
    "            break\n",
    "    if folder_path is None:\n",
    "        return []\n",
    "\n",
    "    polyline = row.geometry\n",
    "    mask_files = sorted(glob.glob(os.path.join(folder_path, 'mask_*.tif')))\n",
    "    if not mask_files:\n",
    "        return []\n",
    "\n",
    "    row_results = []\n",
    "    max_segments = 0\n",
    "    for mask_file in mask_files:\n",
    "        wse_val = extract_wse(os.path.basename(mask_file))\n",
    "        if wse_val is None:\n",
    "            continue\n",
    "        segment_lengths = measure_mask_segments_by_sampling(polyline, mask_file, gdf_crs)\n",
    "        if segment_lengths:\n",
    "            width = max(segment_lengths)\n",
    "            n_segments = len(segment_lengths)\n",
    "            others = sorted([s for s in segment_lengths if s != width], reverse=True)\n",
    "        else:\n",
    "            width = 0.0\n",
    "            n_segments = 0\n",
    "            others = []\n",
    "        max_segments = max(max_segments, len(others))\n",
    "        row_results.append([used_node_id, wse_val, width, n_segments] + others)\n",
    "    return row_results, max_segments\n",
    "\n",
    "def process_all_parallel(base_folder, shapefile_path, output_csv_path, reference_raster_path, node_id_field='node_id', n_workers=None):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_results = []\n",
    "    max_segments_overall = 0\n",
    "\n",
    "    with rasterio.open(reference_raster_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "\n",
    "    print(\"Checking and fixing georeferencing for all masks...\")\n",
    "    fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform)\n",
    "    print(\"Georeference check complete.\\n\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        args_iterable = [\n",
    "            (idx, row, base_folder, node_id_field, gdf.crs)\n",
    "            for idx, row in gdf.iterrows()\n",
    "        ]\n",
    "        futures = [executor.submit(process_single_polyline, args) for args in args_iterable]\n",
    "        for i, future in enumerate(as_completed(futures), 1):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                row_results, max_segs = res\n",
    "                all_results.extend(row_results)\n",
    "                max_segments_overall = max(max_segments_overall, max_segs)\n",
    "            if i % 5 == 0 or i == len(futures):\n",
    "                print(f\"Processed {i}/{len(futures)} polylines...\")\n",
    "\n",
    "    columns = ['node_id', 'wse', 'width', 'n_segments'] + [f'seg_{i+1}' for i in range(max_segments_overall)]\n",
    "    padded_results = []\n",
    "    for row in all_results:\n",
    "        base = row[:4]\n",
    "        segs = row[4:]\n",
    "        segs = segs + [float('nan')] * (max_segments_overall - len(segs))\n",
    "        padded_results.append(base + segs)\n",
    "\n",
    "    df = pd.DataFrame(padded_results, columns=columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "    print(f\"Results saved to {output_csv_path}\")\n",
    "\n",
    "\n",
    "# Example usage (edit paths as needed):\n",
    "path2poly = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Garonne/Shps/garonne_cx.shp\")\n",
    "process_all_parallel(base_folder, path2poly, os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Garonne/Bath_widths.csv\"), \n",
    "                    tif_path, n_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a3cab2",
   "metadata": {},
   "source": [
    "Alternative code to measure widths manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0892358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_mask_widths_for_polyline(polyline, mask_path):\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        arr = src.read(1)\n",
    "        transform = src.transform\n",
    "        crs = src.crs\n",
    "        mask_polys = [shape(geom) for geom, val in shapes(arr, mask=(arr == 1), transform=transform)]\n",
    "        if not mask_polys:\n",
    "            raise RuntimeError(\"No area with value=1 found in mask.\")\n",
    "        from shapely.ops import unary_union\n",
    "        mask_union = unary_union(mask_polys)\n",
    "        # Reproject polyline if needed\n",
    "        if polyline.crs and polyline.crs != crs:\n",
    "            line = polyline.to_crs(crs).geometry.iloc[0]\n",
    "        else:\n",
    "            line = polyline.geometry.iloc[0]\n",
    "        clipped = line.intersection(mask_union)\n",
    "        if clipped.is_empty:\n",
    "            raise RuntimeError(\"Polyline does not cross the mask '1' area.\")\n",
    "        # Collect all segment lengths\n",
    "        if isinstance(clipped, LineString):\n",
    "            segments = [clipped]\n",
    "        elif isinstance(clipped, MultiLineString):\n",
    "            segments = list(clipped.geoms)\n",
    "        else:\n",
    "            raise RuntimeError(\"Unexpected geometry after intersection.\")\n",
    "        segment_lengths = [seg.length for seg in segments]\n",
    "        return segment_lengths\n",
    "\n",
    "# --- User inputs ---\n",
    "mask_path = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Atrato/61100201100491/masks/mask_24_29153.tif\")\n",
    "#polyline_path = '/path/to/Atrato.shp'\n",
    "node_id_field = 'node_id'      # Change to your field name if different\n",
    "node_id_value = 61100201100491.0         # The node_id you want\n",
    "\n",
    "# Select polyline by node_id\n",
    "gdf = gpd.read_file(path2poly)\n",
    "# Robust matching: allow for int/float/string representations\n",
    "def node_id_match(x):\n",
    "    try:\n",
    "        return float(x) == float(node_id_value)\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "selected = gdf[gdf[node_id_field].apply(node_id_match)]\n",
    "print(\"Matching node_ids:\", selected[node_id_field].tolist())\n",
    "if len(selected) != 1:\n",
    "    raise ValueError(f\"node_id {node_id_value} not found or not unique in {path2poly}.\")\n",
    "\n",
    "segment_lengths = measure_mask_widths_for_polyline(selected, mask_path)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'node_id': [node_id_value]*len(segment_lengths),\n",
    "    'segment': range(1, len(segment_lengths)+1),\n",
    "    'width': segment_lengths\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb498b6",
   "metadata": {},
   "source": [
    "Calculate slopes from Huber simple and piecewise regressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71770f8f",
   "metadata": {},
   "source": [
    "One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_huber(X_input, y_input):\n",
    "    \"\"\"\n",
    "    Fit a simple Huber linear model to (X_input, y_input).\n",
    "    Returns the trained model, predictions, and residuals.\n",
    "    \"\"\"\n",
    "    model = HuberRegressor(epsilon=1.345, max_iter=500)\n",
    "    X_reshaped = X_input.reshape(-1, 1)\n",
    "    model.fit(X_reshaped, y_input)\n",
    "    y_pred = model.predict(X_reshaped)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(X, y):\n",
    "    \"\"\"\n",
    "    Always does a single-segment Huber fit.\n",
    "    Returns a dict with keys:\n",
    "    - 'bath_slope1': the slope\n",
    "    - 'bath_slope2': None\n",
    "    - 'r2': R¬≤ score\n",
    "    - 'type': 'simple'\n",
    "    - 'intercepts': (intercept, None)\n",
    "    - 'bp': None\n",
    "    \"\"\"\n",
    "    model_s, y_pred_s, res_s = fit_huber(X, y)\n",
    "    slope1 = model_s.coef_[0]\n",
    "    intercept1 = model_s.intercept_\n",
    "    r2 = r2_score(y, y_pred_s)\n",
    "    return {\n",
    "        'bath_slope1': slope1,\n",
    "        'bath_slope2': None,\n",
    "        'r2': r2,\n",
    "        'type': 'simple',\n",
    "        'intercepts': (intercept1, None),\n",
    "        'bp': None\n",
    "    }\n",
    "\n",
    "def groupwise_regression(csv_path):\n",
    "    \"\"\"\n",
    "    Read CSV with columns ['node_id','width','wse'], fit Huber per node_id group.\n",
    "    Returns (result_df, group_results).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        res = fit_regressions_for_group(X, y)\n",
    "        result_rows.append({\n",
    "            'node_id':     node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_slope2': res['bath_slope2'],\n",
    "            'r2':          res['r2'],\n",
    "            'type':        res['type']\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    \"\"\"\n",
    "    Scatter each group's (width, wse) and overlay the Huber fit line.\n",
    "    \"\"\"\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.scatter(X, y, label=\"Data\", alpha=0.6)\n",
    "        slope = res['bath_slope1']\n",
    "        intercept = res['intercepts'][0]\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "        y_fit = slope * x_plot + intercept\n",
    "        ax.plot(x_plot, y_fit, linewidth=2, label=f\"Simple: slope={slope:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR¬≤={res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Hide unused axes\n",
    "    for ax in axs[len(plot_ids):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "path2w_wse = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Cape_fear/Bath_widths.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760f9699",
   "metadata": {},
   "source": [
    "Two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6bc919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic(rss, n, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "def fit_huber(X_input, y_input):\n",
    "    model = HuberRegressor(epsilon=1.345, max_iter=500)\n",
    "    model.fit(X_input, y_input)\n",
    "    y_pred = model.predict(X_input)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(X, y):\n",
    "    n = len(X)\n",
    "    results = {'best_model_type': 'simple', 'bath_slope1': None, 'bath_slope2': None, 'r2': None}\n",
    "    model_s, y_pred_s, res_s = fit_huber(X.reshape(-1, 1), y)\n",
    "    rss_s = np.sum(res_s ** 2)\n",
    "    aic_s = compute_aic(rss_s, n, 2)\n",
    "    best_model_type = 'simple'\n",
    "    best_aic = aic_s\n",
    "    slope1 = model_s.coef_[0]\n",
    "    intercept1 = model_s.intercept_\n",
    "    slope2 = intercept2 = None\n",
    "    best_bp = None\n",
    "\n",
    "    unique_x = np.unique(X)\n",
    "    if len(unique_x) >= 3:\n",
    "        for bp in unique_x[1:-1]:\n",
    "            left_n = np.sum(X < bp)\n",
    "            right_n = np.sum(X >= bp)\n",
    "            if 10 <= n <= 15 and (left_n < 4 or right_n < 4):\n",
    "                continue\n",
    "            if n >= 16 and (left_n < 0.3 * n or right_n < 0.3 * n):\n",
    "                continue\n",
    "            X_pw = np.column_stack([X, np.maximum(0, X - bp)])\n",
    "            try:\n",
    "                model_pw, y_pred_pw, res_pw = fit_huber(X_pw, y)\n",
    "            except:\n",
    "                continue\n",
    "            h0, h1, delta = model_pw.intercept_, model_pw.coef_[0], model_pw.coef_[1]\n",
    "            if abs(delta) < 0.08:\n",
    "                continue\n",
    "            slope_left = h1\n",
    "            intercept_left = h0\n",
    "            slope_right = h1 + delta\n",
    "            intercept_right = h0 - delta * bp\n",
    "            if slope_left <= 0 or slope_right <= 0:\n",
    "                continue\n",
    "            rss_pw = np.sum(res_pw ** 2)\n",
    "            aic_pw = compute_aic(rss_pw, n, 3)\n",
    "            if aic_pw < best_aic:\n",
    "                best_model_type = 'piecewise'\n",
    "                best_aic = aic_pw\n",
    "                slope1, intercept1 = slope_right, intercept_right\n",
    "                slope2, intercept2 = slope_left, intercept_left\n",
    "                best_bp = bp\n",
    "\n",
    "    # bath_slope1 = higher widths (right segment), bath_slope2 = lower widths (left segment)\n",
    "    if best_model_type == 'piecewise':\n",
    "        if slope1 is not None and slope2 is not None:\n",
    "            results['bath_slope1'] = slope1\n",
    "            results['bath_slope2'] = slope2\n",
    "            X_pw = np.column_stack([X, np.maximum(0, X - best_bp)])\n",
    "            y_pred = slope2 * X + intercept2\n",
    "            y_pred[X >= best_bp] = slope1 * X[X >= best_bp] + intercept1\n",
    "            results['r2'] = r2_score(y, y_pred)\n",
    "        else:\n",
    "            results['bath_slope1'] = slope1\n",
    "            results['bath_slope2'] = None\n",
    "            results['r2'] = r2_score(y, model_s.predict(X.reshape(-1, 1)))\n",
    "    else:\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = None\n",
    "        results['r2'] = r2_score(y, model_s.predict(X.reshape(-1, 1)))\n",
    "    results['type'] = best_model_type\n",
    "    results['intercepts'] = (intercept1, intercept2)\n",
    "    results['bp'] = best_bp\n",
    "    return results\n",
    "\n",
    "def groupwise_regression(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        res = fit_regressions_for_group(X, y)\n",
    "        result_rows.append({\n",
    "            'node_id': node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_slope2': res['bath_slope2'],\n",
    "            'r2': res['r2'],\n",
    "            'type': res['type']\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "        if res['type'] == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.2f}\")\n",
    "        else:\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', linewidth=2, label=f\"Left: slope={slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linewidth=2, linestyle='--', label=f\"Right: slope={slope1:.2f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "        ax.set_title(f\"node_id={node_id}\\nR2={res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "path2w_wse = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Cape_fear/Bath_widths.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04c38e1",
   "metadata": {},
   "source": [
    "Three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546cc45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic(rss, n, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "def fit_huber(X_input, y_input):\n",
    "    model = HuberRegressor(epsilon=1.345, max_iter=500)\n",
    "    model.fit(X_input, y_input)\n",
    "    y_pred = model.predict(X_input)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(X, y, min_width_range=40):\n",
    "    n = len(X)\n",
    "    results = {\n",
    "        'best_model_type': 'simple',\n",
    "        'bath_slope1': None,\n",
    "        'bath_slope2': None,\n",
    "        'bath_slope3': None,\n",
    "        'r2': None,\n",
    "        'intercepts': (None, None, None),\n",
    "        'bp': None\n",
    "    }\n",
    "\n",
    "    def is_invalid_slope(s):\n",
    "        # invalid if None/NaN/inf or < 0.001 or negative\n",
    "        return (s is None) or np.isnan(s) or np.isinf(s) or (s < 0.001)\n",
    "\n",
    "    # === Simple model ===\n",
    "    model_s, y_pred_s, res_s = fit_huber(X.reshape(-1, 1), y)\n",
    "    rss_s = np.sum(res_s ** 2)\n",
    "    aic_s = compute_aic(rss_s, n, 2)\n",
    "    slope_simple = model_s.coef_[0]\n",
    "    intercept_simple = model_s.intercept_\n",
    "    best_model_type = 'simple'\n",
    "    best_aic = aic_s\n",
    "\n",
    "    slope1 = slope2 = slope3 = None\n",
    "    intercept1 = intercept2 = intercept3 = None\n",
    "    best_bp = None\n",
    "    best_bps = None\n",
    "\n",
    "    unique_x = np.unique(X)\n",
    "\n",
    "    # === Try 2 breakpoints (three segments) ===\n",
    "    if len(unique_x) >= 4:\n",
    "        for i in range(1, len(unique_x) - 2):\n",
    "            for j in range(i + 1, len(unique_x) - 1):\n",
    "                bp1 = unique_x[i]\n",
    "                bp2 = unique_x[j]\n",
    "                if bp1 >= bp2:\n",
    "                    continue\n",
    "\n",
    "                seg1 = X[X < bp1]\n",
    "                seg2 = X[(X >= bp1) & (X < bp2)]\n",
    "                seg3 = X[X >= bp2]\n",
    "\n",
    "                # need at least 2 points per side and sufficient width span\n",
    "                if any(seg.size < 2 for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "                if any(np.ptp(seg) < min_width_range for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "\n",
    "                X_pw2 = np.column_stack([X, np.maximum(0, X - bp1), np.maximum(0, X - bp2)])\n",
    "                try:\n",
    "                    model_pw2, y_pred_pw2, res_pw2 = fit_huber(X_pw2, y)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                h0 = model_pw2.intercept_\n",
    "                h1, d1, d2 = model_pw2.coef_\n",
    "\n",
    "                slope_left  = h1\n",
    "                slope_mid   = h1 + d1\n",
    "                slope_right = h1 + d1 + d2\n",
    "\n",
    "                # reject invalid / negative / too-flat segments\n",
    "                if any(is_invalid_slope(s) for s in (slope_left, slope_mid, slope_right)):\n",
    "                    continue\n",
    "\n",
    "                intercept_left  = h0\n",
    "                intercept_mid   = h0 - d1 * bp1\n",
    "                intercept_right = h0 - d1 * bp1 - d2 * bp2\n",
    "\n",
    "                rss_pw2 = np.sum(res_pw2 ** 2)\n",
    "                aic_pw2 = compute_aic(rss_pw2, n, 4)  # 4 params: h0, h1, d1, d2\n",
    "\n",
    "                if aic_pw2 < best_aic:\n",
    "                    best_model_type = 'piecewise_2bp'\n",
    "                    best_aic = aic_pw2\n",
    "                    # keep naming convention: slope1 = rightmost (higher widths)\n",
    "                    slope1, slope2, slope3 = slope_right, slope_mid, slope_left\n",
    "                    intercept1, intercept2, intercept3 = intercept_right, intercept_mid, intercept_left\n",
    "                    best_bps = (bp1, bp2)\n",
    "\n",
    "    # === Try 1 breakpoint (two segments) ===\n",
    "    if best_model_type == 'simple' and len(unique_x) >= 3:\n",
    "        for bp in unique_x[1:-1]:\n",
    "            seg_left = X[X < bp]\n",
    "            seg_right = X[X >= bp]\n",
    "\n",
    "            if seg_left.size < 2 or seg_right.size < 2:\n",
    "                continue\n",
    "            if (np.ptp(seg_left) < min_width_range) or (np.ptp(seg_right) < min_width_range):\n",
    "                continue\n",
    "\n",
    "            X_pw1 = np.column_stack([X, np.maximum(0, X - bp)])\n",
    "            try:\n",
    "                model_pw1, y_pred_pw1, res_pw1 = fit_huber(X_pw1, y)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            h0 = model_pw1.intercept_\n",
    "            h1, d1 = model_pw1.coef_\n",
    "\n",
    "            slope_left  = h1\n",
    "            slope_right = h1 + d1\n",
    "\n",
    "            if any(is_invalid_slope(s) for s in (slope_left, slope_right)):\n",
    "                continue\n",
    "\n",
    "            intercept_left  = h0\n",
    "            intercept_right = h0 - d1 * bp\n",
    "\n",
    "            rss_pw1 = np.sum(res_pw1 ** 2)\n",
    "            aic_pw1 = compute_aic(rss_pw1, n, 3)  # 3 params: h0, h1, d1\n",
    "\n",
    "            if aic_pw1 < best_aic:\n",
    "                best_model_type = 'piecewise'\n",
    "                best_aic = aic_pw1\n",
    "                # naming convention: slope1 = right (higher widths), slope2 = left\n",
    "                slope1, slope2 = slope_right, slope_left\n",
    "                intercept1, intercept2 = intercept_right, intercept_left\n",
    "                best_bp = bp\n",
    "\n",
    "    # === Store results (match original keys) ===\n",
    "    if best_model_type == 'piecewise_2bp':\n",
    "        results['type'] = 'piecewise_2bp'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['bath_slope3'] = slope3\n",
    "        results['intercepts'] = (intercept1, intercept2, intercept3)\n",
    "        results['bp'] = best_bps\n",
    "        y_pred = np.empty_like(y)\n",
    "        y_pred[:] = slope3 * X + intercept3\n",
    "        y_pred[X >= best_bps[0]] = slope2 * X[X >= best_bps[0]] + intercept2\n",
    "        y_pred[X >= best_bps[1]] = slope1 * X[X >= best_bps[1]] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    elif best_model_type == 'piecewise':\n",
    "        results['type'] = 'piecewise'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['intercepts'] = (intercept1, intercept2, None)\n",
    "        results['bp'] = best_bp\n",
    "        y_pred = slope2 * X + intercept2\n",
    "        y_pred[X >= best_bp] = slope1 * X[X >= best_bp] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    else:\n",
    "        results['type'] = 'simple'\n",
    "        results['bath_slope1'] = slope_simple\n",
    "        results['intercepts'] = (intercept_simple, None, None)\n",
    "        results['r2'] = r2_score(y, model_s.predict(X.reshape(-1, 1)))\n",
    "\n",
    "    return results\n",
    "\n",
    "def groupwise_regression(csv_path, min_width_range=40):\n",
    "    \"\"\"\n",
    "    min_width_range: minimum span in width (m) each segment must have\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        res = fit_regressions_for_group(X, y, min_width_range=min_width_range)\n",
    "        result_rows.append({\n",
    "            'node_id': node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_slope2': res['bath_slope2'],\n",
    "            'r2': res['r2'],\n",
    "            'type': res['type']\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "        if res['type'] == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.2f}\")\n",
    "        elif res['type'] == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', linewidth=2, label=f\"Left: slope={slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linewidth=2, linestyle='--', label=f\"Right: slope={slope1:.2f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "        else:  # 'piecewise_2bp'\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', linewidth=2, label=f\"Left: {slope3:.2f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linewidth=2, linestyle='--', label=f\"Mid: {slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linewidth=2, linestyle=':', label=f\"Right: {slope1:.2f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR2={res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path2w_wse = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Pee_dee/Bath_widths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the groupwise regression\n",
    "result_df, group_results = groupwise_regression(path2w_wse, min_width_range=40)\n",
    "\n",
    "# Show the DataFrame with slopes and r2\n",
    "result_df\n",
    "\n",
    "# Optionally, save to CSV\n",
    "result_df.to_csv(os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Pee_dee/Bath_slopes3.csv\"), index=False)\n",
    "\n",
    "# Plot 8 random node_id fits\n",
    "plot_random_regressions(group_results, n=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5c451",
   "metadata": {},
   "source": [
    "### Regression code for dynamic threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic(rss, n, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "def fit_huber(X_input, y_input):\n",
    "    model = HuberRegressor(epsilon=1.345, max_iter=500)\n",
    "    model.fit(X_input, y_input)\n",
    "    y_pred = model.predict(X_input)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(\n",
    "    X, y,\n",
    "    min_width_range=None,   # if None: dynamic span per group = alpha * {median|mean}(width)\n",
    "    alpha=0.30,             # fraction for dynamic span (default 30%)\n",
    "    basis='median'          # 'median' (default) or 'mean'\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits Huber regressions (simple / 1-BP / 2-BP).\n",
    "    A breakpoint is accepted only if EACH resulting segment's width span (np.ptp)\n",
    "    is >= threshold. Threshold is:\n",
    "        - min_width_range (absolute, if provided), otherwise\n",
    "        - alpha * {median|mean}(width) computed per group (default: 30% of median).\n",
    "\n",
    "    NOTE: Function name, keys, and overall behavior remain the same as your original.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    results = {\n",
    "        'best_model_type': 'simple',\n",
    "        'bath_slope1': None,\n",
    "        'bath_slope2': None,\n",
    "        'bath_slope3': None,\n",
    "        'r2': None,\n",
    "        'intercepts': (None, None, None),\n",
    "        'bp': None\n",
    "    }\n",
    "\n",
    "    # --- dynamic per-group span (default) ---\n",
    "    if min_width_range is None:\n",
    "        if basis == 'mean':\n",
    "            min_width_range = alpha * float(np.nanmean(X))\n",
    "        else:\n",
    "            min_width_range = alpha * float(np.nanmedian(X))\n",
    "\n",
    "    def is_invalid_slope(s):\n",
    "        # invalid if None/NaN/inf or < 0.001 or negative\n",
    "        return (s is None) or np.isnan(s) or np.isinf(s) or (s < 0.001)\n",
    "\n",
    "    # === Simple model ===\n",
    "    model_s, y_pred_s, res_s = fit_huber(X.reshape(-1, 1), y)\n",
    "    rss_s = np.sum(res_s ** 2)\n",
    "    aic_s = compute_aic(rss_s, n, 2)\n",
    "    slope_simple = model_s.coef_[0]\n",
    "    intercept_simple = model_s.intercept_\n",
    "    best_model_type = 'simple'\n",
    "    best_aic = aic_s\n",
    "\n",
    "    slope1 = slope2 = slope3 = None\n",
    "    intercept1 = intercept2 = intercept3 = None\n",
    "    best_bp = None\n",
    "    best_bps = None\n",
    "\n",
    "    unique_x = np.unique(X)\n",
    "\n",
    "    # === Try 2 breakpoints (three segments) ===\n",
    "    if len(unique_x) >= 4:\n",
    "        for i in range(1, len(unique_x) - 2):\n",
    "            for j in range(i + 1, len(unique_x) - 1):\n",
    "                bp1 = unique_x[i]\n",
    "                bp2 = unique_x[j]\n",
    "                if bp1 >= bp2:\n",
    "                    continue\n",
    "\n",
    "                seg1 = X[X < bp1]\n",
    "                seg2 = X[(X >= bp1) & (X < bp2)]\n",
    "                seg3 = X[X >= bp2]\n",
    "\n",
    "                # need at least 2 points per side and sufficient width span (dynamic per group)\n",
    "                if any(seg.size < 2 for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "                if any(np.ptp(seg) < min_width_range for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "\n",
    "                X_pw2 = np.column_stack([X, np.maximum(0, X - bp1), np.maximum(0, X - bp2)])\n",
    "                try:\n",
    "                    model_pw2, y_pred_pw2, res_pw2 = fit_huber(X_pw2, y)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                h0 = model_pw2.intercept_\n",
    "                h1, d1, d2 = model_pw2.coef_\n",
    "\n",
    "                slope_left  = h1\n",
    "                slope_mid   = h1 + d1\n",
    "                slope_right = h1 + d1 + d2\n",
    "\n",
    "                # reject invalid / negative / too-flat segments\n",
    "                if any(is_invalid_slope(s) for s in (slope_left, slope_mid, slope_right)):\n",
    "                    continue\n",
    "\n",
    "                intercept_left  = h0\n",
    "                intercept_mid   = h0 - d1 * bp1\n",
    "                intercept_right = h0 - d1 * bp1 - d2 * bp2\n",
    "\n",
    "                rss_pw2 = np.sum(res_pw2 ** 2)\n",
    "                aic_pw2 = compute_aic(rss_pw2, n, 4)  # 4 params: h0, h1, d1, d2\n",
    "\n",
    "                if aic_pw2 < best_aic:\n",
    "                    best_model_type = 'piecewise_2bp'\n",
    "                    best_aic = aic_pw2\n",
    "                    # keep naming convention: slope1 = rightmost (higher widths)\n",
    "                    slope1, slope2, slope3 = slope_right, slope_mid, slope_left\n",
    "                    intercept1, intercept2, intercept3 = intercept_right, intercept_mid, intercept_left\n",
    "                    best_bps = (bp1, bp2)\n",
    "\n",
    "    # === Try 1 breakpoint (two segments) ===\n",
    "    if best_model_type == 'simple' and len(unique_x) >= 3:\n",
    "        for bp in unique_x[1:-1]:\n",
    "            seg_left = X[X < bp]\n",
    "            seg_right = X[X >= bp]\n",
    "\n",
    "            if seg_left.size < 2 or seg_right.size < 2:\n",
    "                continue\n",
    "            if (np.ptp(seg_left) < min_width_range) or (np.ptp(seg_right) < min_width_range):\n",
    "                continue\n",
    "\n",
    "            X_pw1 = np.column_stack([X, np.maximum(0, X - bp)])\n",
    "            try:\n",
    "                model_pw1, y_pred_pw1, res_pw1 = fit_huber(X_pw1, y)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            h0 = model_pw1.intercept_\n",
    "            h1, d1 = model_pw1.coef_\n",
    "\n",
    "            slope_left  = h1\n",
    "            slope_right = h1 + d1\n",
    "\n",
    "            if any(is_invalid_slope(s) for s in (slope_left, slope_right)):\n",
    "                continue\n",
    "\n",
    "            intercept_left  = h0\n",
    "            intercept_right = h0 - d1 * bp\n",
    "\n",
    "            rss_pw1 = np.sum(res_pw1 ** 2)\n",
    "            aic_pw1 = compute_aic(rss_pw1, n, 3)  # 3 params: h0, h1, d1\n",
    "\n",
    "            if aic_pw1 < best_aic:\n",
    "                best_model_type = 'piecewise'\n",
    "                best_aic = aic_pw1\n",
    "                # naming convention: slope1 = right (higher widths), slope2 = left\n",
    "                slope1, slope2 = slope_right, slope_left\n",
    "                intercept1, intercept2 = intercept_right, intercept_left\n",
    "                best_bp = bp\n",
    "\n",
    "    # === Store results (match original keys) ===\n",
    "    if best_model_type == 'piecewise_2bp':\n",
    "        results['type'] = 'piecewise_2bp'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['bath_slope3'] = slope3\n",
    "        results['intercepts'] = (intercept1, intercept2, intercept3)\n",
    "        results['bp'] = best_bps\n",
    "        y_pred = np.empty_like(y)\n",
    "        y_pred[:] = slope3 * X + intercept3\n",
    "        y_pred[X >= best_bps[0]] = slope2 * X[X >= best_bps[0]] + intercept2\n",
    "        y_pred[X >= best_bps[1]] = slope1 * X[X >= best_bps[1]] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    elif best_model_type == 'piecewise':\n",
    "        results['type'] = 'piecewise'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['intercepts'] = (intercept1, intercept2, None)\n",
    "        results['bp'] = best_bp\n",
    "        y_pred = slope2 * X + intercept2\n",
    "        y_pred[X >= best_bp] = slope1 * X[X >= best_bp] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    else:\n",
    "        results['type'] = 'simple'\n",
    "        results['bath_slope1'] = slope_simple\n",
    "        results['intercepts'] = (intercept_simple, None, None)\n",
    "        results['r2'] = r2_score(y, model_s.predict(X.reshape(-1, 1)))\n",
    "\n",
    "    return results\n",
    "\n",
    "def groupwise_regression(\n",
    "    csv_path,\n",
    "    min_width_range=None,  # if provided: fixed absolute span (meters). If None: dynamic span.\n",
    "    alpha=0.30,            # fraction for dynamic span (default 30%)\n",
    "    basis='median',        # 'median' (default) or 'mean'\n",
    "    global_stat=False      # if True: use one global statistic for all groups\n",
    "):\n",
    "    \"\"\"\n",
    "    min_width_range:\n",
    "        - If provided (number), use a fixed minimum width span (meters) for all groups (original behavior).\n",
    "        - If None (default), use dynamic span = alpha * {median|mean}(width):\n",
    "            * per group (default), OR\n",
    "            * global across the whole dataset (if global_stat=True).\n",
    "\n",
    "    Prints, for transparency:\n",
    "      - Global median/mean (if global_stat=True)\n",
    "      - Per-group median/mean and the min_width_span_used\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "\n",
    "    # --- compute global statistic if needed ---\n",
    "    if global_stat:\n",
    "        if basis == 'mean':\n",
    "            global_val = float(np.nanmean(df['width']))\n",
    "        else:\n",
    "            global_val = float(np.nanmedian(df['width']))\n",
    "        print(f\"Global {basis} width = {global_val:.2f}\")\n",
    "    else:\n",
    "        global_val = None\n",
    "\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "\n",
    "        # compute width statistic used for this group\n",
    "        if global_stat:\n",
    "            group_stat = global_val\n",
    "        else:\n",
    "            if basis == 'mean':\n",
    "                group_stat = float(np.nanmean(X))\n",
    "            else:\n",
    "                group_stat = float(np.nanmedian(X))\n",
    "\n",
    "        # decide dynamic or fixed span\n",
    "        if min_width_range is None:\n",
    "            dyn_span = alpha * group_stat\n",
    "        else:\n",
    "            dyn_span = float(min_width_range)\n",
    "\n",
    "        # print per-group info\n",
    "        print(f\"node_id={node_id}: {basis} width = {group_stat:.2f}, min_width_span_used = {dyn_span:.2f}\")\n",
    "\n",
    "        res = fit_regressions_for_group(\n",
    "            X, y,\n",
    "            min_width_range=dyn_span,\n",
    "            alpha=alpha,\n",
    "            basis=basis\n",
    "        )\n",
    "        result_rows.append({\n",
    "            'node_id': node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_slope2': res['bath_slope2'],\n",
    "            'r2': res['r2'],\n",
    "            'type': res['type'],\n",
    "            'width_stat': group_stat,\n",
    "            'min_width_span_used': dyn_span\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "        if res['type'] == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.2f}\")\n",
    "        elif res['type'] == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', linewidth=2, label=f\"Left: slope={slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linewidth=2, linestyle='--', label=f\"Right: slope={slope1:.2f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "        else:  # 'piecewise_2bp'\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', linewidth=2, label=f\"Left: {slope3:.2f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linewidth=2, linestyle='--', label=f\"Mid: {slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linewidth=2, linestyle=':', label=f\"Right: {slope1:.2f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR2={res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "path2w_wse = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Po/Bath_widths.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, group_results = groupwise_regression(path2w_wse, global_stat=False, alpha=0.07, basis='mean')\n",
    "# Optionally, save to CSV\n",
    "result_df.to_csv(os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Garonne/Bath_slopes3.csv\"), index=False)\n",
    "# Plot 8 random node_id fits\n",
    "plot_random_regressions(group_results, n=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_regressions(group_results):\n",
    "    import math\n",
    "\n",
    "    n = len(group_results)\n",
    "    if n == 0:\n",
    "        print(\"No groups to plot.\")\n",
    "        return\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(n / ncols)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    # Ensure axs is always a flat iterable\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        axs = axs.flatten()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (node_id, (group, res)) in enumerate(group_results.items()):\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "\n",
    "        x_plot = np.linspace(X.min(), X.max(), 300)\n",
    "        model_type = res['type']\n",
    "\n",
    "        if model_type == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.4f}\")\n",
    "\n",
    "        elif model_type == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', label=f\"Left: slope={slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linestyle='--', label=f\"Right: slope={slope1:.4f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "\n",
    "        elif model_type == 'piecewise_2bp':\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', label=f\"Left: {slope3:.4f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linestyle='--', label=f\"Mid: {slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linestyle=':', label=f\"Right: {slope1:.4f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR¬≤ = {res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any extra unused subplots\n",
    "    total_axes = nrows * ncols\n",
    "    for j in range(n, total_axes):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_regressions(group_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aec3ba",
   "metadata": {},
   "source": [
    "Merge csv file with Bathymetry slopes with the Regression master csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(master_csv, complementary_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Merge two CSV files on 'node_id' and save the result.\n",
    "    \"\"\"\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "    comp_df = pd.read_csv(complementary_csv)\n",
    "\n",
    "    # Merge on 'node_id'\n",
    "    merged_df = master_df.merge(comp_df, on='node_id', how='inner')\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged CSV saved to {output_csv}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "path2slope = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Garonne/Bath_slopes3.csv\")\n",
    "outpath = os.path.join(os.getcwd(), \"2_intermediate/Store/Validation/csv/Garonne/reg_slope_bath3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26992158",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath = merge_csv(master_csv_path, path2slope, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bd0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort = reg_slope_bath.sort_values('p_dist_out', ascending=False).groupby('node_id', as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier removal in SWOT and bathymetry\n",
    "Q1, Q3 = reg_slope_bath_sort['slope1'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "l1, u1 = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "\n",
    "# bath_slope1 fences\n",
    "Q1b, Q3b = reg_slope_bath_sort['bath_slope1'].quantile([0.25, 0.75])\n",
    "IQRb   = Q3b - Q1b\n",
    "l2, u2 = Q1b - 1.5*IQRb, Q3b + 1.5*IQRb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b444e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (reg_slope_bath_sort['slope1']      >= l1) & (reg_slope_bath_sort['slope1']      <= u1) &\n",
    "    (reg_slope_bath_sort['bath_slope1'] >= l2) & (reg_slope_bath_sort['bath_slope1'] <= u2)\n",
    ")\n",
    "reg_slope_bath_sort = reg_slope_bath_sort[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db054cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Work on a copy so you can compare before/after if needed\n",
    "df = reg_slope_bath_sort.copy()\n",
    "\n",
    "# --- 1) Compute rel error (percent) in-place-friendly columns\n",
    "# (safe divide: ignore zero/NaN denominators in the trimming stats)\n",
    "rel = 100 * (df['slope1'] - df['bath_slope1']) / df['bath_slope1']\n",
    "df['rel_error'] = rel\n",
    "df['abs_rel_error'] = rel.abs()\n",
    "\n",
    "# --- 2) Build a UNIQUE view (like your CDF step) for threshold estimation only\n",
    "uniq = df[['slope1','bath_slope1','rel_error']].dropna().drop_duplicates(subset=['slope1','bath_slope1'])\n",
    "\n",
    "# --- 3) MAD right-tail rule (robust, no small-slope dropping)\n",
    "med  = uniq['rel_error'].median()\n",
    "mad0 = (uniq['rel_error'] - med).abs().median()\n",
    "mad  = 1.4826 * mad0                # #1.4826 makes MAD behave like std deviation\n",
    "k    = 3.0                          # tweakable: 2.5‚Äì4. tweak: bigger = keep more, smaller = remove more. it‚Äôs analogous to saying ‚Äúflag anything more than 3œÉ above the median.‚Äù\n",
    "thr_mad = med + k * mad\n",
    "\n",
    "mask_out_mad = uniq['rel_error'] > thr_mad\n",
    "\n",
    "# --- 4) Optional percentile tail shave (set q=None to skip)\n",
    "q = 0.99                            # top 1% cutoff; set to None to disable\n",
    "if q is not None:\n",
    "    thr_q = uniq['rel_error'].quantile(q)\n",
    "    mask_out_pct = uniq['rel_error'] > thr_q\n",
    "    mask_out_all = mask_out_mad | mask_out_pct\n",
    "else:\n",
    "    mask_out_all = mask_out_mad\n",
    "\n",
    "# --- 5) Map the unique outliers back to the full df (keep structure)\n",
    "out_pairs = uniq.loc[mask_out_all, ['slope1','bath_slope1']]\n",
    "out_pairs['__key__'] = 1\n",
    "df_key = df[['slope1','bath_slope1']].copy()\n",
    "df_key['__key__'] = 1\n",
    "\n",
    "# mark matches\n",
    "to_drop = df_key.merge(out_pairs, on=['__key__','slope1','bath_slope1'], how='left', indicator=True)\n",
    "is_rel_outlier = (to_drop['_merge'] == 'both').values\n",
    "\n",
    "# (optional) keep a flag and a log before dropping\n",
    "df['is_rel_outlier'] = is_rel_outlier\n",
    "removed = df[df['is_rel_outlier']].copy()\n",
    "kept    = df[~df['is_rel_outlier']].copy()\n",
    "\n",
    "# --- 6) Replace reg_slope_bath_sort with the cleaned version (same columns as before)\n",
    "# If you want the exact original schema, drop the helper columns:\n",
    "reg_slope_bath_sort = kept.drop(columns=['rel_error','abs_rel_error','is_rel_outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0748525",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9a546",
   "metadata": {},
   "source": [
    "Error metrics and coefficient of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f84571",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reg_slope_bath_sort['slope1'].values\n",
    "y = reg_slope_bath_sort['bath_slope1'].values\n",
    "\n",
    "# Classic metrics\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "mae = mean_absolute_error(y, x)\n",
    "rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "# Relative errors\n",
    "rel_error = 100 * (x - y) / y\n",
    "mean_bias = np.mean(rel_error)\n",
    "mean_abs_rel_error = np.mean(np.abs(rel_error))\n",
    "percentile_68 = np.percentile(np.abs(rel_error), 68)\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f} (p={pearson_p:.2g})\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f} (p={spearman_p:.2g})\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Relative Error (Bias): {mean_bias:.2f}%\")\n",
    "print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f}%\")\n",
    "print(f\"68th percentile Absolute Relative Error: {percentile_68:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8af05b2",
   "metadata": {},
   "source": [
    "By reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd940ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_by_spearman(\n",
    "    df: pd.DataFrame,\n",
    "    rho_threshold: float = 0.4,\n",
    "    pval_threshold: float = 0.05,\n",
    "    min_length: int = 10,\n",
    "    method: str = 'spearman'   # 'spearman' or 'pearson' for thresholding\n",
    "):\n",
    "    if method not in ('spearman','pearson'):\n",
    "        raise ValueError(\"method must be 'spearman' or 'pearson'\")\n",
    "    corr_test = spearmanr if method == 'spearman' else pearsonr\n",
    "\n",
    "    # Keep original index\n",
    "    df = df.copy()\n",
    "    orig_index = df.index.to_numpy()\n",
    "\n",
    "    n = len(df)\n",
    "    in_segment = np.zeros(n, dtype=bool)\n",
    "    seg_ids    = np.full(n, np.nan)\n",
    "    segments   = []\n",
    "    flagged_pts = []\n",
    "\n",
    "    i = 0\n",
    "    next_seg_id = 1\n",
    "    while i < n:\n",
    "        best_end = None\n",
    "\n",
    "        # Slide j to find the longest valid block\n",
    "        for j in range(i + min_length, n + 1):\n",
    "            block = df.iloc[i:j]\n",
    "            corr_val, pval = corr_test(block['slope1'], block['bath_slope1'])\n",
    "            if (corr_val >= rho_threshold) and (pval <= pval_threshold):\n",
    "                best_end = j\n",
    "\n",
    "        if best_end:\n",
    "            seg = df.iloc[i:best_end]\n",
    "            x, y = seg['slope1'].values, seg['bath_slope1'].values\n",
    "\n",
    "            # Report both metrics\n",
    "            pr, prp = pearsonr(x, y)\n",
    "            sr, srp = spearmanr(x, y)\n",
    "            mae     = mean_absolute_error(y, x)\n",
    "            rmse    = np.sqrt(mean_squared_error(y, x))\n",
    "            r2      = r2_score(y, x)\n",
    "\n",
    "            rel_err = 100 * (x - y) / y\n",
    "            mean_rel = float(np.mean(rel_err))\n",
    "            mean_abs = float(np.mean(np.abs(rel_err)))\n",
    "            pct68    = float(np.percentile(np.abs(rel_err), 68))\n",
    "\n",
    "            segments.append({\n",
    "                'segment_id':             next_seg_id,\n",
    "                'start_dist':             seg['p_dist_out'].iloc[0],\n",
    "                'end_dist':               seg['p_dist_out'].iloc[-1],\n",
    "                'n_points':               len(seg),\n",
    "                'pearson':                pr,\n",
    "                'pearson_p':              prp,\n",
    "                'spearman':               sr,\n",
    "                'spearman_p':             srp,\n",
    "                'mae':                    mae,\n",
    "                'rmse':                   rmse,\n",
    "                'r2':                     r2,\n",
    "                'mean_rel_error':         mean_rel,\n",
    "                'mean_abs_rel_error':     mean_abs,\n",
    "                'pctile_68_abs_rel_err':  pct68\n",
    "            })\n",
    "\n",
    "            # Mark membership\n",
    "            in_segment[i:best_end] = True\n",
    "            seg_ids[i:best_end]    = next_seg_id\n",
    "            next_seg_id += 1\n",
    "            i = best_end\n",
    "        else:\n",
    "            row = df.iloc[i].to_dict()\n",
    "            row['orig_index'] = int(orig_index[i])\n",
    "            row['flag_reason'] = 'no_valid_block'\n",
    "            flagged_pts.append(row)\n",
    "            i += 1\n",
    "\n",
    "    segments_df = pd.DataFrame(segments)\n",
    "    flagged_df  = pd.DataFrame(flagged_pts)\n",
    "\n",
    "    # Build the full annotated dataframe\n",
    "    annotated_df = df.copy()\n",
    "    annotated_df['segment_id'] = seg_ids\n",
    "    annotated_df['flagged'] = in_segment  # True = in a valid segment\n",
    "\n",
    "    # === PRINT REPORT AS BEFORE ===\n",
    "    print(\"=== Segments report ===\")\n",
    "    print(segments_df)\n",
    "    print(\"\\n=== Flagged points (couldn't join any valid block) ===\")\n",
    "    print(flagged_df)\n",
    "    print(f\"\\nTotal rows: {len(df)}, In segment: {in_segment.sum()}, Flagged: {(~in_segment).sum()}\")\n",
    "\n",
    "    return segments_df, flagged_df, annotated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbba3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "segments_df, flagged_df, annotated_df = segment_by_spearman(\n",
    "    reg_slope_bath_sort,\n",
    "    rho_threshold=0.4,\n",
    "    pval_threshold=0.05,\n",
    "    min_length=10,\n",
    "    method='spearman'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71090493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Export flagged_df with original column order ===\n",
    "cols = list(reg_slope_bath_sort.columns)   # original order\n",
    "extra = [c for c in ['segment_id','flagged'] if c not in cols]\n",
    "annotated_df[cols + extra].to_csv(os.path.join(os.getcwd(), \"2_intermediate/Store/Validation/csv/Pee_dee/flagged_nodes_80.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cbf64",
   "metadata": {},
   "source": [
    "Figures for all nodes, including bad spearmans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error = 100 * (reg_slope_bath_sort['slope1'] - reg_slope_bath_sort['bath_slope1']) / reg_slope_bath_sort['bath_slope1']\n",
    "abs_rel_error = np.abs(rel_error)\n",
    "\n",
    "# Drop duplicates for (slope1, bath_slope1) pairs (optional but matches your logic)\n",
    "df_rel = pd.DataFrame({\n",
    "    'slope1': reg_slope_bath_sort['slope1'],\n",
    "    'bath_slope1': reg_slope_bath_sort['bath_slope1'],\n",
    "    'rel_error': rel_error,\n",
    "    'abs_rel_error': abs_rel_error\n",
    "})\n",
    "df_rel_unique = df_rel.drop_duplicates(subset=['slope1', 'bath_slope1'], keep='first')\n",
    "\n",
    "# Sorted arrays for CDFs\n",
    "bias_sorted = np.sort(df_rel_unique['rel_error'].values)\n",
    "cdf_bias = np.arange(1, len(bias_sorted) + 1) / len(bias_sorted)\n",
    "\n",
    "abs_sorted = np.sort(df_rel_unique['abs_rel_error'].values)\n",
    "cdf_abs = np.arange(1, len(abs_sorted) + 1) / len(abs_sorted)\n",
    "\n",
    "# Metrics\n",
    "mean_bias = df_rel_unique['rel_error'].mean()\n",
    "mean_abs_rel_error = df_rel_unique['abs_rel_error'].mean()\n",
    "spearman_rho, spearman_p = spearmanr(df_rel_unique['slope1'], df_rel_unique['bath_slope1'])\n",
    "p68 = np.percentile(df_rel_unique['abs_rel_error'], 68)\n",
    "\n",
    "# Colors (customize as desired)\n",
    "color_bias = '#ff5722'\n",
    "color_abs = '#fbc02d'\n",
    "color_p68 = 'dimgrey'\n",
    "color_zero = '#ffa726'\n",
    "color_mean = '#2979ff'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Plotting ---\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "\n",
    "# Plot Relative Error CDF\n",
    "ax.scatter(\n",
    "    bias_sorted, cdf_bias,\n",
    "    color=color_bias,\n",
    "    label='Relative Error CDF (bias, %)',\n",
    "    s=70,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='o',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Plot Absolute Relative Error CDF\n",
    "ax.scatter(\n",
    "    abs_sorted, cdf_abs,\n",
    "    color=color_abs,\n",
    "    label='Absolute Relative Error CDF (%)',\n",
    "    s=25,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='D',\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Zero-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=0,\n",
    "    color=color_zero,\n",
    "    linestyle='--',\n",
    "    linewidth=1.8,\n",
    "    label='Zero Bias'\n",
    ")\n",
    "\n",
    "# Mean-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=mean_bias,\n",
    "    color=color_mean,\n",
    "    linestyle='-.',\n",
    "    linewidth=2,\n",
    "    label=f'Mean Bias = {mean_bias:.2f} %'\n",
    ")\n",
    "\n",
    "# 68th-percentile vertical/horizontal lines\n",
    "ax.axvline(\n",
    "    x=p68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5,\n",
    "    label=f'|68%ile|: {p68:.2f} %'\n",
    ")\n",
    "ax.axhline(\n",
    "    y=0.68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# Axis limits and ticks\n",
    "xmin = np.floor(min(bias_sorted.min(), abs_sorted.min())) - 1\n",
    "xmax = np.ceil(max(bias_sorted.max(), abs_sorted.max(), p68)) + 1\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_xticks(np.arange(xmin, xmax + 1, 20))\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Relative Error (%)', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=14)\n",
    "ax.set_title(\n",
    "    'CDF of Slope Relative Error, Mean Bias and 68%tile\\n',\n",
    "    fontsize=16,\n",
    "    weight='bold'\n",
    ")\n",
    "\n",
    "# Grid and legend\n",
    "ax.grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=12, loc='lower right', frameon=True, fancybox=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Marginal KDE plot for bias (inset at top) ---\n",
    "# ------------------------------------------------------------------------------\n",
    "ax_kde = inset_axes(\n",
    "    ax,\n",
    "    width=\"100%\", height=\"20%\",\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0, 1.05, 1, 0.3),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=df_rel_unique['rel_error'],\n",
    "    ax=ax_kde,\n",
    "    fill=True,\n",
    "    linewidth=2,\n",
    "    alpha=0.3\n",
    ")\n",
    "ax_kde.set_xlim(xmin, xmax)\n",
    "ax_kde.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print metrics summary ---\n",
    "print(f\"Spearman correlation: {spearman_rho:.3f} (p={spearman_p:.2g})\")\n",
    "print(f\"Mean Bias (rel error): {mean_bias:.2f} %\")\n",
    "print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f} %\")\n",
    "print(f\"68th percentile abs rel error: {p68:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['bath_slope1']\n",
    "x  = reg_slope_bath_sort['p_dist_out']\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ figure setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "fig = plt.figure(figsize=(15,6), facecolor='white')\n",
    "gs  = GridSpec(1, 2, width_ratios=[5, 1.5], wspace=0.15)\n",
    "\n",
    "# vibrant palette\n",
    "c1, c2 = '#1f77b4', '#ff7f0e'\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ left: line vs. distance with thinner lines ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "\n",
    "ax_main.plot(\n",
    "    x, s1,\n",
    "    color=c1,\n",
    "    linestyle='-',\n",
    "    linewidth=1.5,          # ‚Üê thinner line\n",
    "    marker='o',\n",
    "    markersize=5,           # ‚Üê small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c1,\n",
    "    markeredgewidth=1.0,\n",
    "    label='SWOT slope'\n",
    ")\n",
    "ax_main.plot(\n",
    "    x, s2,\n",
    "    color=c2,\n",
    "    linestyle='--',\n",
    "    linewidth=1.5,          # ‚Üê thinner line\n",
    "    marker='D',\n",
    "    markersize=5,           # ‚Üê small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c2,\n",
    "    markeredgewidth=1.0,\n",
    "    label='Bathymetry slope'\n",
    ")\n",
    "\n",
    "# gentle fill under curves\n",
    "ax_main.fill_between(x, s1, alpha=0.1, color=c1)\n",
    "ax_main.fill_between(x, s2, alpha=0.1, color=c2)\n",
    "\n",
    "ax_main.set_xlabel('Distance to outlet (m)', fontsize=12, weight='bold')\n",
    "ax_main.set_ylabel('Slope value', fontsize=12, weight='bold')\n",
    "ax_main.set_title('Comparison between SWOT and Bathymetry Slopes', fontsize=16, weight='bold')\n",
    "ax_main.grid(alpha=0.3, linestyle='--')\n",
    "ax_main.invert_xaxis()\n",
    "ax_main.legend(loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ right: side boxplot sharing the same y-axis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax_box = fig.add_subplot(gs[1], sharey=ax_main)\n",
    "\n",
    "bp = ax_box.boxplot(\n",
    "    [s1, s2],\n",
    "    positions=[1,2],\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    showfliers=False,\n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='black'),\n",
    "    capprops=dict(color='black'),\n",
    ")\n",
    "\n",
    "# color the boxes with semi-transparent fill\n",
    "for patch, color in zip(bp['boxes'], [c1, c2]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.3)\n",
    "\n",
    "# overlay the raw points on top\n",
    "for i, (y, color) in enumerate(zip([s1, s2], [c1, c2]), start=1):\n",
    "    xi = np.random.normal(i, 0.04, size=len(y))\n",
    "    ax_box.scatter(\n",
    "        xi, y,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        s=12,\n",
    "        edgecolors='none',\n",
    "        zorder=10\n",
    "    )\n",
    "\n",
    "ax_box.set_xticks([1,2])\n",
    "ax_box.set_xticklabels(['SWOT', 'Bathymetry'], fontsize=10, weight='bold')\n",
    "ax_box.margins(x=0.3)\n",
    "ax_box.yaxis.tick_right()\n",
    "ax_box.yaxis.set_label_position(\"right\")\n",
    "ax_box.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6817c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['bath_slope1']\n",
    "\n",
    "# set up the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(s1, s2, \n",
    "            color='red',      # match your SWOT color\n",
    "            alpha=0.7, \n",
    "            edgecolors='none', \n",
    "            s=30)\n",
    "\n",
    "# 1:1 line\n",
    "lims = [\n",
    "    np.min([s1.min(), s2.min()]), \n",
    "    np.max([s1.max(), s2.max()])\n",
    "]\n",
    "plt.plot(lims, lims, \n",
    "        ls='--', \n",
    "        lw=1.5, \n",
    "        color='green', \n",
    "        label='1:1 line')\n",
    "\n",
    "margin = 0.001\n",
    "low, high = lims\n",
    "\n",
    "plt.xlim(low - margin, high + margin)\n",
    "plt.ylim(low - margin, high + margin)\n",
    "\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('SWOT vs. Bathymetry Slopes', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']           # SWOT slope\n",
    "s2 = reg_slope_bath_sort['bath_slope1']      # Bathymetry slope\n",
    "\n",
    "# 1) Hexbin + 1:1 line\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.hexbin(s1, s2, gridsize=100, mincnt=1, cmap='Blues')  # no color specified ‚Üí default cmap\n",
    "lims = [min(s1.min(), s2.min()), max(s1.max(), s2.max())]\n",
    "plt.plot(lims, lims, ls='--', lw=1.5, color='green', label='1:1 line')\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('Hexbin: SWOT vs Bath Slopes', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 2) Bland‚ÄìAltman (Difference vs Average)\n",
    "diff = s1 - s2\n",
    "avg  = 0.5 * (s1 + s2)\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(avg, diff, s=20, alpha=0.5)\n",
    "plt.axhline(mean_diff, color='red', linestyle='--', label='Mean diff')\n",
    "plt.axhline(mean_diff + 1.96*std_diff, color='gray', linestyle=':', label='¬±1.96œÉ')\n",
    "plt.axhline(mean_diff - 1.96*std_diff, color='gray', linestyle=':')\n",
    "plt.xlabel('Average slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Difference (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Bland‚ÄìAltman: Difference vs. Average', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 3) Residuals vs SWOT slope\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(s1, diff, s=20, alpha=0.5)\n",
    "plt.axhline(0, color='black', linestyle='--', lw=1)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Residuals vs. SWOT Slope', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 4) CDF of Absolute Residuals (|SWOT ‚Äì Bath|)\n",
    "abs_diff = np.abs(diff)\n",
    "abs_diff = abs_diff[np.isfinite(abs_diff)]\n",
    "xs = np.sort(abs_diff)\n",
    "cdf = np.arange(1, len(xs) + 1) / len(xs)\n",
    "\n",
    "p50 = np.percentile(abs_diff, 50)\n",
    "p68 = np.percentile(abs_diff, 68)\n",
    "p95 = np.percentile(abs_diff, 95)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, cdf, lw=2)\n",
    "plt.axvline(p68, linestyle='--', label=f'68%tile = {p68:.4f}')\n",
    "plt.axvline(p50, linestyle=':',  label=f'Median = {p50:.4f}')\n",
    "plt.axvline(p95, linestyle=':',  label=f'95%tile = {p95:.4f}')\n",
    "plt.xlabel('|Residual| (|SWOT ‚Äì Bath|)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Cumulative Probability', fontsize=12, weight='bold')\n",
    "plt.title('CDF of Absolute Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "# (Alternative) Histogram of signed residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=30, density=True, alpha=0.7)\n",
    "plt.axvline(mean_diff, linestyle='--', label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Density', fontsize=12, weight='bold')\n",
    "plt.title('Residual Distribution', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Histogram of Residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(0, color='black', linestyle='--', lw=1, label='Zero line')\n",
    "plt.axvline(mean_diff, color='red', linestyle='--', lw=1, label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Count', fontsize=12, weight='bold')\n",
    "plt.title('Histogram of Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Clean residuals\n",
    "diff_clean = np.asarray(diff[np.isfinite(diff)])\n",
    "\n",
    "# Stats\n",
    "q1, med, q3 = np.percentile(diff_clean, [25, 50, 75])\n",
    "mean_val    = diff_clean.mean()\n",
    "p68         = np.percentile(diff_clean, 68)\n",
    "\n",
    "# Keep comparable limits\n",
    "ymin = np.percentile(diff_clean, 1)\n",
    "ymax = np.percentile(diff_clean, 99)\n",
    "pad  = 0.02 * (ymax - ymin)\n",
    "ylim = (ymin - pad, ymax + pad)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# Violin plot with same proportional look\n",
    "ax = sns.violinplot(\n",
    "    y=diff_clean,\n",
    "    inner='box',\n",
    "    color='lightblue',\n",
    "    cut=0,\n",
    "    scale='width',\n",
    "    bw=0.2,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "# Markers for stats\n",
    "ax.scatter(0, mean_val, s=60, marker='D', color='#d62728', zorder=5, label=f\"Mean = {mean_val:.4f}\")\n",
    "ax.scatter(0, med,      s=60, marker='o', facecolors='white', edgecolors='black', zorder=5, label=f\"Median = {med:.4f}\")\n",
    "ax.scatter(0, q1,       s=80, marker='_', color='#2ca02c', zorder=5, label=f\"Q1 = {q1:.4f}\")\n",
    "ax.scatter(0, q3,       s=80, marker='_', color='#1f77b4', zorder=5, label=f\"Q3 = {q3:.4f}\")\n",
    "ax.axhline(p68, color='purple', linestyle=':', lw=1.2, label=f\"68%tile = {p68:.4f}\")\n",
    "\n",
    "# Labels, limits, legend\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_ylabel('Residual (SWOT - Bath)', fontsize=12, weight='bold')\n",
    "ax.set_title('Residual Distribution Around Zero', fontsize=14, weight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.legend(loc='lower left', fontsize=9, frameon=True)  # legend in lower-left\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea4057",
   "metadata": {},
   "source": [
    "## By reach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b333d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stats_report_for_group(g: pd.DataFrame):\n",
    "    \"\"\"Compute metrics for one consecutive TRUE group (one segment_id).\"\"\"\n",
    "    # Clean copy\n",
    "    g = g.copy()\n",
    "\n",
    "    # Core vectors\n",
    "    x_dist = g['p_dist_out'].values\n",
    "    s1 = g['slope1'].values        # SWOT\n",
    "    s2 = g['bath_slope1'].values   # Bath\n",
    "\n",
    "    # Metrics (use non-NaNs only)\n",
    "    m = np.isfinite(s1) & np.isfinite(s2)\n",
    "    s1c, s2c = s1[m], s2[m]\n",
    "\n",
    "    # If too few points, return minimal info\n",
    "    if len(s1c) < 2:\n",
    "        return {\n",
    "            \"pearson\": np.nan, \"pearson_p\": np.nan,\n",
    "            \"spearman\": np.nan, \"spearman_p\": np.nan,\n",
    "            \"mae\": np.nan, \"rmse\": np.nan, \"r2\": np.nan,\n",
    "            \"mean_rel\": np.nan, \"mean_abs_rel\": np.nan, \"p68_abs_rel\": np.nan\n",
    "        }\n",
    "\n",
    "    pr, prp = pearsonr(s1c, s2c)\n",
    "    sr, srp = spearmanr(s1c, s2c)\n",
    "    mae     = mean_absolute_error(s2c, s1c)\n",
    "    rmse    = np.sqrt(mean_squared_error(s2c, s1c))\n",
    "    r2      = r2_score(s2c, s1c)\n",
    "\n",
    "    # Relative errors (drop dup pairs as in your logic)\n",
    "    df_rel = pd.DataFrame({\"slope1\": s1c, \"bath_slope1\": s2c})\n",
    "    df_rel[\"rel_error\"] = 100.0 * (df_rel[\"slope1\"] - df_rel[\"bath_slope1\"]) / df_rel[\"bath_slope1\"]\n",
    "    df_rel[\"abs_rel_error\"] = df_rel[\"rel_error\"].abs()\n",
    "    df_rel_u = df_rel.drop_duplicates(subset=[\"slope1\", \"bath_slope1\"], keep=\"first\")\n",
    "\n",
    "    mean_rel = float(df_rel_u[\"rel_error\"].mean())\n",
    "    mean_abs = float(df_rel_u[\"abs_rel_error\"].mean())\n",
    "    p68      = float(np.percentile(df_rel_u[\"abs_rel_error\"].values, 68))\n",
    "\n",
    "    return {\n",
    "        \"pearson\": pr, \"pearson_p\": prp,\n",
    "        \"spearman\": sr, \"spearman_p\": srp,\n",
    "        \"mae\": mae, \"rmse\": rmse, \"r2\": r2,\n",
    "        \"mean_rel\": mean_rel, \"mean_abs_rel\": mean_abs, \"p68_abs_rel\": p68\n",
    "    }\n",
    "\n",
    "def _print_group_header(seg_id, g_sorted):\n",
    "    \"\"\"Header with first/last node_id and dist_to_out (p_dist_out).\"\"\"\n",
    "    first_row = g_sorted.iloc[0]\n",
    "    last_row  = g_sorted.iloc[-1]\n",
    "    # assumes columns exist:\n",
    "    first_node = first_row.get('node_id', 'NA')\n",
    "    last_node  = last_row.get('node_id', 'NA')\n",
    "    first_dist = float(first_row['p_dist_out'])\n",
    "    last_dist  = float(last_row['p_dist_out'])\n",
    "    print(f\"\\n=== Segment {int(seg_id)} ===\")\n",
    "    print(f\"First node_id: {first_node}  |  Last node_id: {last_node}\")\n",
    "    print(f\"First p_dist_out: {first_dist:.3f}  |  Last p_dist_out: {last_dist:.3f}\")\n",
    "\n",
    "def _plots_for_group(g_sorted, seg_id, save_prefix=None):\n",
    "    \"\"\"Your full plotting suite, scoped to the current group.\"\"\"\n",
    "    s1 = g_sorted['slope1'].values\n",
    "    s2 = g_sorted['bath_slope1'].values\n",
    "    x  = g_sorted['p_dist_out'].values\n",
    "\n",
    "    # ---------- Relative error CDF (+ KDE inset) ----------\n",
    "    rel_error = 100.0 * (s1 - s2) / s2\n",
    "    abs_rel_error = np.abs(rel_error)\n",
    "    df_rel = pd.DataFrame({\"slope1\": s1, \"bath_slope1\": s2,\n",
    "                           \"rel_error\": rel_error, \"abs_rel_error\": abs_rel_error})\n",
    "    df_rel_unique = df_rel.drop_duplicates(subset=['slope1','bath_slope1'], keep='first')\n",
    "\n",
    "    bias_sorted = np.sort(df_rel_unique['rel_error'].values)\n",
    "    cdf_bias = np.arange(1, len(bias_sorted)+1) / len(bias_sorted)\n",
    "    abs_sorted = np.sort(df_rel_unique['abs_rel_error'].values)\n",
    "    cdf_abs = np.arange(1, len(abs_sorted)+1) / len(abs_sorted)\n",
    "\n",
    "    mean_bias = df_rel_unique['rel_error'].mean()\n",
    "    mean_abs_rel_error = df_rel_unique['abs_rel_error'].mean()\n",
    "    _, spearman_p = spearmanr(df_rel_unique['slope1'], df_rel_unique['bath_slope1'])\n",
    "    p68 = np.percentile(df_rel_unique['abs_rel_error'], 68)\n",
    "\n",
    "    color_bias = '#ff5722'\n",
    "    color_abs  = '#fbc02d'\n",
    "    color_p68  = 'dimgrey'\n",
    "    color_zero = '#ffa726'\n",
    "    color_mean = '#2979ff'\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.set_facecolor('#f5f5f5')\n",
    "    ax.scatter(bias_sorted, cdf_bias, color=color_bias, label='Relative Error CDF (bias, %)',\n",
    "               s=70, edgecolors='black', linewidth=0.5, marker='o', alpha=0.7)\n",
    "    ax.scatter(abs_sorted, cdf_abs, color=color_abs, label='Absolute Relative Error CDF (%)',\n",
    "               s=25, edgecolors='black', linewidth=0.5, marker='D', alpha=0.9)\n",
    "    ax.axvline(x=0, color=color_zero, linestyle='--', linewidth=1.8, label='Zero Bias')\n",
    "    ax.axvline(x=mean_bias, color=color_mean, linestyle='-.', linewidth=2, label=f'Mean Bias = {mean_bias:.2f} %')\n",
    "    ax.axvline(x=p68, color=color_p68, linestyle=':', linewidth=2.5, label=f'|68%ile|: {p68:.2f} %')\n",
    "    ax.axhline(y=0.68, color=color_p68, linestyle=':', linewidth=2.5)\n",
    "    xmin = np.floor(min(bias_sorted.min(), abs_sorted.min())) - 1\n",
    "    xmax = np.ceil(max(bias_sorted.max(), abs_sorted.max(), p68)) + 1\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_xticks(np.arange(xmin, xmax + 1, 20))\n",
    "    ax.set_xlabel('Relative Error (%)', fontsize=14)\n",
    "    ax.set_ylabel('Cumulative Probability', fontsize=14)\n",
    "    ax.set_title(f'Segment {int(seg_id)} ‚Äì CDF of Slope Relative Error\\n', fontsize=16, weight='bold')\n",
    "    ax.grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "    ax.legend(fontsize=12, loc='lower right', frameon=True, fancybox=True)\n",
    "\n",
    "    ax_kde = inset_axes(ax, width=\"100%\", height=\"20%\", loc='upper center',\n",
    "                        bbox_to_anchor=(0, 1.05, 1, 0.3), bbox_transform=ax.transAxes, borderpad=0)\n",
    "    sns.kdeplot(x=df_rel_unique['rel_error'], ax=ax_kde, fill=True, linewidth=2, alpha=0.3)\n",
    "    ax_kde.set_xlim(xmin, xmax); ax_kde.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_cdf_rel_error.png\", dpi=200)\n",
    "\n",
    "    # ---------- Distance plot + boxplot ----------\n",
    "    fig = plt.figure(figsize=(15,6), facecolor='white')\n",
    "    gs  = GridSpec(1, 2, width_ratios=[5, 1.5], wspace=0.15)\n",
    "    c1, c2 = '#1f77b4', '#ff7f0e'\n",
    "    ax_main = fig.add_subplot(gs[0])\n",
    "    ax_main.plot(x, s1, color=c1, linestyle='-', linewidth=1.5, marker='o', markersize=5,\n",
    "                 markerfacecolor='white', markeredgecolor=c1, markeredgewidth=1.0, label='SWOT slope')\n",
    "    ax_main.plot(x, s2, color=c2, linestyle='--', linewidth=1.5, marker='D', markersize=5,\n",
    "                 markerfacecolor='white', markeredgecolor=c2, markeredgewidth=1.0, label='Bathymetry slope')\n",
    "    ax_main.fill_between(x, s1, alpha=0.1, color=c1)\n",
    "    ax_main.fill_between(x, s2, alpha=0.1, color=c2)\n",
    "    ax_main.set_xlabel('Distance to outlet (m)', fontsize=12, weight='bold')\n",
    "    ax_main.set_ylabel('Slope value', fontsize=12, weight='bold')\n",
    "    ax_main.set_title(f'Segment {int(seg_id)} ‚Äì Slopes vs Distance', fontsize=16, weight='bold')\n",
    "    ax_main.grid(alpha=0.3, linestyle='--')\n",
    "    ax_main.invert_xaxis()\n",
    "    ax_main.legend(loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "    ax_box = fig.add_subplot(gs[1], sharey=ax_main)\n",
    "    bp = ax_box.boxplot([s1, s2], positions=[1,2], widths=0.6, patch_artist=True, showfliers=False,\n",
    "                        medianprops=dict(color='black'), whiskerprops=dict(color='black'), capprops=dict(color='black'))\n",
    "    for patch, color in zip(bp['boxes'], [c1, c2]):\n",
    "        patch.set_facecolor(color); patch.set_alpha(0.3)\n",
    "    for i,(y,color) in enumerate([(s1,c1),(s2,c2)], start=1):\n",
    "        xi = np.random.normal(i, 0.04, size=len(y))\n",
    "        ax_box.scatter(xi, y, color=color, alpha=0.6, s=12, edgecolors='none', zorder=10)\n",
    "    ax_box.set_xticks([1,2]); ax_box.set_xticklabels(['SWOT','Bathymetry'], fontsize=10, weight='bold')\n",
    "    ax_box.margins(x=0.3); ax_box.yaxis.tick_right(); ax_box.yaxis.set_label_position(\"right\")\n",
    "    ax_box.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_slopes_vs_dist_box.png\", dpi=200)\n",
    "\n",
    "    # ---------- Scatter 1:1 ----------\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(s1, s2, color='red', alpha=0.7, edgecolors='none', s=30)\n",
    "    lims = [min(s1.min(), s2.min()), max(s1.max(), s2.max())]\n",
    "    plt.plot(lims, lims, ls='--', lw=1.5, color='green', label='1:1 line')\n",
    "    margin = 0.001\n",
    "    low, high = lims\n",
    "    plt.xlim(low - margin, high + margin); plt.ylim(low - margin, high + margin)\n",
    "    plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì SWOT vs Bath Slopes', fontsize=14, weight='bold')\n",
    "    plt.grid(alpha=0.3, linestyle='--'); plt.legend(); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_scatter_1to1.png\", dpi=200)\n",
    "\n",
    "    # ---------- Hexbin + BA + Residuals + CDF of |res| + Histograms ----------\n",
    "    # hexbin\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.hexbin(s1, s2, gridsize=100, mincnt=1, cmap='Blues')\n",
    "    lims = [min(s1.min(), s2.min()), max(s1.max(), s2.max())]\n",
    "    plt.plot(lims, lims, ls='--', lw=1.5, color='green', label='1:1 line')\n",
    "    plt.xlim(lims); plt.ylim(lims)\n",
    "    plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì Hexbin SWOT vs Bath', fontsize=14, weight='bold')\n",
    "    plt.legend(); plt.grid(alpha=0.3, linestyle='--'); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_hexbin.png\", dpi=200)\n",
    "\n",
    "    # differences\n",
    "    diff = s1 - s2\n",
    "    avg  = 0.5 * (s1 + s2)\n",
    "    mean_diff = diff.mean(); std_diff = diff.std()\n",
    "\n",
    "    # Bland‚ÄìAltman\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(avg, diff, s=20, alpha=0.5)\n",
    "    plt.axhline(mean_diff, color='red', linestyle='--', label='Mean diff')\n",
    "    plt.axhline(mean_diff + 1.96*std_diff, color='gray', linestyle=':', label='¬±1.96œÉ')\n",
    "    plt.axhline(mean_diff - 1.96*std_diff, color='gray', linestyle=':')\n",
    "    plt.xlabel('Average slope', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Difference (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì Bland‚ÄìAltman', fontsize=14, weight='bold')\n",
    "    plt.legend(); plt.grid(alpha=0.3, linestyle='--'); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_bland_altman.png\", dpi=200)\n",
    "\n",
    "    # Residuals vs SWOT slope\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(s1, diff, s=20, alpha=0.5)\n",
    "    plt.axhline(0, color='black', linestyle='--', lw=1)\n",
    "    plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì Residuals vs SWOT', fontsize=14, weight='bold')\n",
    "    plt.grid(alpha=0.3, linestyle='--'); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_residuals_vs_swot.png\", dpi=200)\n",
    "\n",
    "    # CDF of |res|\n",
    "    abs_diff = np.abs(diff[np.isfinite(diff)])\n",
    "    xs = np.sort(abs_diff); cdf = np.arange(1, len(xs)+1) / len(xs)\n",
    "    p50 = np.percentile(abs_diff, 50); p68 = np.percentile(abs_diff, 68); p95 = np.percentile(abs_diff, 95)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(xs, cdf, lw=2)\n",
    "    plt.axvline(p68, linestyle='--', label=f'68%tile = {p68:.4f}')\n",
    "    plt.axvline(p50, linestyle=':',  label=f'Median = {p50:.4f}')\n",
    "    plt.axvline(p95, linestyle=':',  label=f'95%tile = {p95:.4f}')\n",
    "    plt.xlabel('|Residual| (|SWOT ‚Äì Bath|)', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Cumulative Probability', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì CDF of |Residuals|', fontsize=14, weight='bold')\n",
    "    plt.legend(); plt.grid(alpha=0.3, linestyle='--'); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_cdf_abs_residuals.png\", dpi=200)\n",
    "\n",
    "    # Histogram + Violin\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(diff[np.isfinite(diff)], bins=30, density=True, alpha=0.7)\n",
    "    plt.axvline(mean_diff, linestyle='--', label=f'Mean = {mean_diff:.4f}')\n",
    "    plt.xlabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "    plt.ylabel('Density', fontsize=12, weight='bold')\n",
    "    plt.title(f'Segment {int(seg_id)} ‚Äì Residual Distribution', fontsize=14, weight='bold')\n",
    "    plt.legend(); plt.grid(alpha=0.3, linestyle='--'); plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_hist_density.png\", dpi=200)\n",
    "\n",
    "    diff_clean = diff[np.isfinite(diff)]\n",
    "    q1, med, q3 = np.percentile(diff_clean, [25, 50, 75])\n",
    "    mean_val    = diff_clean.mean()\n",
    "    p68v        = np.percentile(diff_clean, 68)\n",
    "    ymin = np.percentile(diff_clean, 1); ymax = np.percentile(diff_clean, 99)\n",
    "    pad  = 0.02 * (ymax - ymin); ylim = (ymin - pad, ymax + pad)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    ax = sns.violinplot(y=diff_clean, inner='box', color='lightblue', cut=0, scale='width', bw=0.2, linewidth=1.2)\n",
    "    ax.axhline(0, color='black', linestyle='--', lw=1)\n",
    "    ax.scatter(0, mean_val, s=60, marker='D', color='#d62728', zorder=5, label=f\"Mean = {mean_val:.4f}\")\n",
    "    ax.scatter(0, med,      s=60, marker='o', facecolors='white', edgecolors='black', zorder=5, label=f\"Median = {med:.4f}\")\n",
    "    ax.scatter(0, q1,       s=80, marker='_', color='#2ca02c', zorder=5, label=f\"Q1 = {q1:.4f}\")\n",
    "    ax.scatter(0, q3,       s=80, marker='_', color='#1f77b4', zorder=5, label=f\"Q3 = {q3:.4f}\")\n",
    "    ax.axhline(p68v, color='purple', linestyle=':', lw=1.2, label=f\"68%tile = {p68v:.4f}\")\n",
    "    ax.set_ylim(ylim); ax.set_ylabel('Residual (SWOT - Bath)', fontsize=12, weight='bold')\n",
    "    ax.set_title(f'Segment {int(seg_id)} ‚Äì Residual Distribution Around Zero', fontsize=14, weight='bold')\n",
    "    ax.set_xticks([]); ax.legend(loc='lower left', fontsize=9, frameon=True)\n",
    "    plt.tight_layout()\n",
    "    if save_prefix: plt.savefig(f\"{save_prefix}_seg{int(seg_id)}_violin_residuals.png\", dpi=200)\n",
    "\n",
    "def report_and_plot_consecutive_true_groups(annotated_df: pd.DataFrame, save_prefix=None):\n",
    "    \"\"\"\n",
    "    For each consecutive TRUE group (segment_id), print the detailed metrics report\n",
    "    and generate all plots for that group. Assumes annotated_df has:\n",
    "    ['segment_id','flagged','node_id','p_dist_out','slope1','bath_slope1'].\n",
    "    \"\"\"\n",
    "    # Ensure groups are contiguous by distance (your segmentation walked farthest‚Üínearest)\n",
    "    gdf = annotated_df.copy().sort_values('p_dist_out', ascending=False)\n",
    "    # Iterate only over TRUE groups (segment_id not NaN)\n",
    "    for seg_id in sorted(gdf['segment_id'].dropna().unique()):\n",
    "        g = gdf[gdf['segment_id'] == seg_id].copy().sort_values('p_dist_out', ascending=False)\n",
    "        _print_group_header(seg_id, g)\n",
    "\n",
    "        stats = _stats_report_for_group(g)\n",
    "        print(f\"Pearson correlation: {stats['pearson']:.3f} (p={stats['pearson_p']:.2g})\")\n",
    "        print(f\"Spearman correlation: {stats['spearman']:.3f} (p={stats['spearman_p']:.2g})\")\n",
    "        print(f\"Mean Absolute Error (MAE): {stats['mae']:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {stats['rmse']:.4f}\")\n",
    "        print(f\"R\\u00b2: {stats['r2']:.3f}\")\n",
    "        print(f\"Mean Relative Error (Bias): {stats['mean_rel']:.2f}%\")\n",
    "        print(f\"Mean Absolute Relative Error: {stats['mean_abs_rel']:.2f}%\")\n",
    "        print(f\"68th percentile Absolute Relative Error: {stats['p68_abs_rel']:.2f}%\")\n",
    "\n",
    "        # Plots for this group\n",
    "        _plots_for_group(g, seg_id, save_prefix=save_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87991af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_plot_consecutive_true_groups(annotated_df, save_prefix=\"seg_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ce331",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Po', 'Garonne'],\n",
    "    'r_40':  [0.44, 0.62, 0.64, 0.54, 0.21, 0.18],\n",
    "    'œÅ_40':  [0.42, 0.65, 0.67, 0.52, 0.30, 0.08],\n",
    "    'r_80':  [0.41, 0.83, 0.45, 0.1, 0.34, 0.24],\n",
    "    'œÅ_80':  [0.37, 0.70, 0.51, -0.08, 0.36, 0.06],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Reshape to long format\n",
    "df_long = pd.DataFrame({\n",
    "    'River': df['River'].tolist() * 2,\n",
    "    'Width': ['40m'] * len(df) + ['80m'] * len(df),\n",
    "    'r': df['r_40'].tolist() + df['r_80'].tolist(),\n",
    "    'œÅ': df['œÅ_40'].tolist() + df['œÅ_80'].tolist()\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter points by width\n",
    "colors = {'40m': 'tab:blue', '80m': 'tab:orange'}\n",
    "markers = {'40m': 'o', '80m': 's'}\n",
    "\n",
    "for width in df_long['Width'].unique():\n",
    "    subset = df_long[df_long['Width'] == width]\n",
    "    ax.scatter(\n",
    "        subset['r'],\n",
    "        subset['œÅ'],\n",
    "        label=width,\n",
    "        c=colors[width],\n",
    "        marker=markers[width],\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "# Annotate basins\n",
    "for _, row in df_long.iterrows():\n",
    "    ax.text(row['r'] + 0.01, row['œÅ'] + 0.01, row['River'], fontsize=8)\n",
    "\n",
    "# Labels and style\n",
    "ax.set_xlabel('r', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('œÅ', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Scatter Plot of r vs œÅ by Segment Width', fontsize=14, fontweight='bold')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.legend(title='Segment Width')\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08988ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "threshold = 0.40\n",
    "outline_color = 'limegreen'\n",
    "dash_on, dash_off = 1.5, 2.0  # dotted look\n",
    "lw = 2\n",
    "\n",
    "# =========================\n",
    "# Data\n",
    "# =========================\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Po', 'Garonne'],\n",
    "    'r_40':  [0.44, 0.62, 0.64, 0.54, 0.21, 0.18],\n",
    "    'œÅ_40':  [0.42, 0.65, 0.67, 0.52, 0.30, 0.08],\n",
    "    'r_80':  [0.41, 0.83, 0.45, 0.1, 0.34, 0.24],\n",
    "    'œÅ_80':  [0.37, 0.70, 0.51, -0.08, 0.36, 0.06],\n",
    "}\n",
    "df = pd.DataFrame(data).set_index('River')\n",
    "df = df[['r_40', 'œÅ_40', 'r_80', 'œÅ_80']]\n",
    "\n",
    "# =========================\n",
    "# Heatmap\n",
    "# =========================\n",
    "vals = df.values\n",
    "vmax = np.nanmax(np.abs(vals))\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.8, 5.0))\n",
    "im = ax.imshow(vals, aspect='auto', cmap='RdBu_r', norm=norm, origin='upper')\n",
    "\n",
    "ax.set_xticks(np.arange(df.shape[1]))\n",
    "ax.set_xticklabels(df.columns, fontsize=11, fontweight='bold')\n",
    "ax.set_yticks(np.arange(df.shape[0]))\n",
    "ax.set_yticklabels(df.index, fontsize=11)\n",
    "\n",
    "# annotate values\n",
    "for i in range(vals.shape[0]):\n",
    "    for j in range(vals.shape[1]):\n",
    "        ax.text(j, i, f\"{vals[i, j]:.2f}\",\n",
    "                ha='center', va='center', fontsize=9,\n",
    "                color='black' if abs(vals[i, j]) < vmax*0.55 else 'white')\n",
    "\n",
    "ax.set_title('Correlation Heatmap (centered at 0)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('metric@Segment Width', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('River', fontsize=11, fontweight='bold')\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Correlation', fontsize=11, fontweight='bold')\n",
    "\n",
    "# =========================\n",
    "# Boxy perimeter for r > threshold OR œÅ > threshold\n",
    "# =========================\n",
    "mask = vals > threshold  # includes both Pearson and Spearman\n",
    "\n",
    "rows, cols = mask.shape\n",
    "edges = set()\n",
    "\n",
    "def add_edge(p1, p2):\n",
    "    edges.add((p1, p2) if p1 < p2 else (p2, p1))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if not mask[i, j]:\n",
    "            continue\n",
    "        x0, x1 = j - 0.5, j + 0.5\n",
    "        y0, y1 = i - 0.5, i + 0.5\n",
    "        if i == 0 or not mask[i-1, j]:      # top\n",
    "            add_edge((x0, y0), (x1, y0))\n",
    "        if i == rows-1 or not mask[i+1, j]: # bottom\n",
    "            add_edge((x0, y1), (x1, y1))\n",
    "        if j == 0 or not mask[i, j-1]:      # left\n",
    "            add_edge((x0, y0), (x0, y1))\n",
    "        if j == cols-1 or not mask[i, j+1]: # right\n",
    "            add_edge((x1, y0), (x1, y1))\n",
    "\n",
    "segments = [list(edge) for edge in edges]\n",
    "\n",
    "if segments:\n",
    "    lc = LineCollection(\n",
    "        segments,\n",
    "        colors=outline_color,\n",
    "        linewidths=3,\n",
    "        linestyles=(0, (dash_on, dash_off)),\n",
    "        capstyle='butt',\n",
    "        joinstyle='miter',\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "# keep full pixel grid visible\n",
    "ax.set_xlim(-0.5, cols - 0.5)\n",
    "ax.set_ylim(rows - 0.5, -0.5)\n",
    "\n",
    "# =========================\n",
    "# Legend outside the axes\n",
    "# =========================\n",
    "outline_handle = Line2D([0], [0], color=outline_color, lw=lw, linestyle=(0, (dash_on, dash_off)))\n",
    "fig.legend([outline_handle], [f'Pearson or Spearman > {threshold:.2f}'],\n",
    "           loc='lower right', bbox_to_anchor=(0.98, -0.05), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4021d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== INPUT DATA (replace with your real numbers) ====\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Po', 'Garonne'],\n",
    "    # 40 m\n",
    "    'bias_40':  [-77.2,  53.4, -29.9,  -41.4, -13.6, -77.1],   # %\n",
    "    'p68_40':   [87.7, 99.6, 55.9, 66.3,  66.3,  86.2],  # %\n",
    "    # 80 m\n",
    "    'bias_80':  [ -78.3, 19.1, -18.6, -16.3,  -4.14,  -78.6],  # %\n",
    "    'p68_80':   [88.0, 51.27, 50.5, 77.7,  56.3,  86.46],  # %\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# ==== PLOT ====\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "\n",
    "rivers = df['River'].values\n",
    "x = np.arange(len(rivers))              # river positions\n",
    "group_w = 0.36                          # width of each width-group (40m vs 80m)\n",
    "bar_w   = group_w / 2.0                 # two bars inside each group (Bias, P68)\n",
    "\n",
    "# Offsets: 40m group to the left, 80m group to the right\n",
    "x40 = x - group_w/2\n",
    "x80 = x + group_w/2\n",
    "\n",
    "# Colors per metric (consistent across widths)\n",
    "colors = {'Bias': '#4361ee', '68%ile': '#f59e0b'}\n",
    "\n",
    "# 40 m bars\n",
    "b1 = ax.bar(x40 - bar_w/2, df['bias_40'], width=bar_w, label='Bias (40 m)', color=colors['Bias'])\n",
    "b2 = ax.bar(x40 + bar_w/2, df['p68_40'],  width=bar_w, label='68%ile (40 m)', color=colors['68%ile'])\n",
    "\n",
    "# 80 m bars\n",
    "b3 = ax.bar(x80 - bar_w/2, df['bias_80'], width=bar_w, label='Bias (80 m)', color=colors['Bias'], alpha=0.55)\n",
    "b4 = ax.bar(x80 + bar_w/2, df['p68_80'],  width=bar_w, label='68%ile (80 m)', color=colors['68%ile'], alpha=0.55)\n",
    "\n",
    "# Zero line (helps read positive/negative bias)\n",
    "ax.axhline(0, color='gray', linewidth=1, linestyle='--', zorder=0)\n",
    "\n",
    "# X labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rivers, rotation=30, ha='right')\n",
    "\n",
    "ax.set_ylabel('Percent (%)', fontweight='bold')\n",
    "ax.set_title('Bias and 68th-Percentile Error by River and Segment Width', fontweight='bold')\n",
    "\n",
    "# Legend (metric color + width opacity cue)\n",
    "leg = ax.legend(ncol=2, frameon=False, loc='upper center')\n",
    "\n",
    "# Value labels on bars\n",
    "def label_bars(containers):\n",
    "    for c in containers:\n",
    "        for rect in c:\n",
    "            h = rect.get_height()\n",
    "            ax.annotate(f'{h:.0f}%',\n",
    "                        xy=(rect.get_x() + rect.get_width()/2, h),\n",
    "                        xytext=(0, 4 if h>=0 else -12),\n",
    "                        textcoords='offset points',\n",
    "                        ha='center', va='bottom' if h>=0 else 'top',\n",
    "                        fontsize=8)\n",
    "label_bars([b1, b2, b3, b4])\n",
    "\n",
    "# Tidy layout\n",
    "ymin = min(df[['bias_40','bias_80']].min().min() - 10, -10)\n",
    "ymax = max(df[['p68_40','p68_80']].max().max() + 10,  10)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "ax.grid(axis='y', linestyle=':', alpha=0.35)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2948ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== DATA (your example) =====\n",
    "data = {\n",
    "    'River':  ['Cape Fear','Atrato','Sacramento','Pee Dee','Po','Garonne'],\n",
    "    'bias_40':[-77.2, 53.4, -29.9, -41.4, -13.6, -77.1],   # %\n",
    "    'p68_40': [ 87.7, 99.6,  55.9,  66.3,  66.3,  86.2],   # %\n",
    "    'bias_80':[-78.3, 19.1, -18.6, -16.3,  -4.14, -78.6],  # %\n",
    "    'p68_80': [ 88.0, 51.27, 50.5,  77.7,  56.3,  86.46],  # %\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# OPTIONAL: sort rivers for a tidy visual (pick one metric to sort on)\n",
    "# df = df.sort_values('bias_40').reset_index(drop=True)\n",
    "\n",
    "# Prepare matrices\n",
    "rivers = df['River'].tolist()\n",
    "bias = df[['bias_40','bias_80']].to_numpy()\n",
    "p68  = df[['p68_40','p68_80']].to_numpy()\n",
    "\n",
    "# ===== PLOT =====\n",
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    2, 1, figsize=(8.8, 7.2),\n",
    "    sharex=True, sharey=True,\n",
    "    gridspec_kw={'height_ratios': [1, 1], 'hspace': 0.15}\n",
    ")\n",
    "\n",
    "# ---- Heatmap 1: Bias (centered at 0) ----\n",
    "bmax = np.nanmax(np.abs(bias))\n",
    "b_norm = TwoSlopeNorm(vmin=-bmax, vcenter=0.0, vmax=bmax)\n",
    "im1 = ax1.imshow(bias, cmap='RdBu_r', norm=b_norm, aspect='auto', origin='upper')\n",
    "\n",
    "# annotate bias\n",
    "for i in range(bias.shape[0]):\n",
    "    for j in range(bias.shape[1]):\n",
    "        ax1.text(j, i, f\"{bias[i, j]:.1f}\",\n",
    "                 ha='center', va='center',\n",
    "                 color='black' if abs(bias[i, j]) < 0.55*bmax else 'white',\n",
    "                 fontsize=9)\n",
    "\n",
    "ax1.set_title('Bias (%)', fontweight='bold', pad=8)\n",
    "ax1.set_yticks(np.arange(len(rivers)))\n",
    "ax1.set_yticklabels(rivers)\n",
    "ax1.set_xticks([0, 1])\n",
    "ax1.set_xticklabels(['40 m', '80 m'], fontweight='bold')\n",
    "ax1.set_ylabel('River', fontweight='bold')\n",
    "cbar1 = fig.colorbar(im1, ax=ax1, fraction=0.046, pad=0.02)\n",
    "cbar1.set_label('Bias (%)', fontweight='bold')\n",
    "\n",
    "# ---- Heatmap 2: 68%ile error ----\n",
    "e_min, e_max = np.nanmin(p68), np.nanmax(p68)\n",
    "im2 = ax2.imshow(p68, cmap='Oranges', vmin=e_min, vmax=e_max, aspect='auto', origin='upper')\n",
    "\n",
    "# annotate p68\n",
    "for i in range(p68.shape[0]):\n",
    "    for j in range(p68.shape[1]):\n",
    "        ax2.text(j, i, f\"{p68[i, j]:.1f}\",\n",
    "                 ha='center', va='center',\n",
    "                 color='black' if p68[i, j] < 0.6*e_max else 'white',\n",
    "                 fontsize=9)\n",
    "\n",
    "ax2.set_title('68th Percentile Error (%)', fontweight='bold', pad=8)\n",
    "ax2.set_yticks(np.arange(len(rivers)))\n",
    "ax2.set_yticklabels(rivers)  # sharey keeps alignment\n",
    "ax2.set_xticks([0, 1])\n",
    "ax2.set_xticklabels(['40 m', '80 m'], fontweight='bold')\n",
    "ax2.set_xlabel('Segment Width', fontweight='bold')\n",
    "ax2.set_ylabel('River', fontweight='bold')\n",
    "cbar2 = fig.colorbar(im2, ax=ax2, fraction=0.046, pad=0.02)\n",
    "cbar2.set_label('Absolute Relative Error (%)', fontweight='bold')\n",
    "\n",
    "# cosmetic gridlines around cells (optional)\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xticks(np.arange(-0.5, 2, 1), minor=True)\n",
    "    ax.set_yticks(np.arange(-0.5, len(rivers), 1), minor=True)\n",
    "    ax.grid(which='minor', color='white', linestyle='-', linewidth=1.0)\n",
    "    ax.tick_params(which='minor', bottom=False, left=False)\n",
    "\n",
    "fig.suptitle('Dual Heatmaps: Bias and 68%ile Error by River & Width', fontweight='bold', y=0.98)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68919f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- DATA -----\n",
    "data = {\n",
    "    'River':  ['Cape Fear','Atrato','Sacramento','Pee Dee','Po','Garonne'],\n",
    "    'bias_40':[-77.2, 53.4, -29.9, -41.4, -13.6, -77.1],\n",
    "    'p68_40': [ 87.7, 99.6,  55.9,  66.3,  66.3,  86.2],\n",
    "    'bias_80':[-78.3, 19.1, -18.6, -16.3,  -4.14, -78.6],\n",
    "    'p68_80': [ 88.0, 51.27, 50.5,  77.7,  56.3,  86.46],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Long format\n",
    "long = pd.DataFrame({\n",
    "    'River': df['River'].tolist()*2,\n",
    "    'Width': ['40 m']*len(df) + ['80 m']*len(df),\n",
    "    'Bias':  df['bias_40'].tolist() + df['bias_80'].tolist(),\n",
    "    'P68':   df['p68_40'].tolist()  + df['p68_80'].tolist()\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8.6, 6.4))\n",
    "\n",
    "for width, subset in long.groupby('Width'):\n",
    "    ax.scatter(subset['Bias'], subset['P68'], s=120, alpha=0.9, label=width, edgecolor='black')\n",
    "    # label each point with the river (optional)\n",
    "    for _, r in subset.iterrows():\n",
    "        ax.text(r['Bias']+1.5, r['P68']+1.0, r['River'], fontsize=8)\n",
    "\n",
    "ax.axvline(0, linestyle='--', linewidth=0.8)\n",
    "ax.set_xlabel('Bias (%)', fontweight='bold')\n",
    "ax.set_ylabel('68th Percentile Error (%)', fontweight='bold')\n",
    "ax.set_title('Bias vs 68%ile Error ‚Äî Bubble Plot', fontweight='bold')\n",
    "ax.grid(True, linestyle=':', alpha=0.35)\n",
    "ax.legend(frameon=False, title='Segment Width')\n",
    "plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304275ab",
   "metadata": {},
   "source": [
    "## Width variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a538f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path2w_wse)\n",
    "cv_width = df.groupby('node_id')['width'].agg(lambda x: x.std()/x.mean()).rename(\"CV_width\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bcb3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv = cv_width.mean()\n",
    "print(mean_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rivers ={\n",
    "    'River':  [\"Cape Fear\", \"Atrato\", \"Sacramento\", \"Pee Dee\", \"Po\", \"Garonne\"],\n",
    "    'cv_val':      [0.16, 0.39, 0.30, 0.29, 0.2, 0.08],\n",
    "    'œÅ_10%':       [0.47, 0.56, 0.35, 0.70, 0.29, 0.18],\n",
    "    'œÅ_15%':       [0.38, 0.59, 0.44, 0.63,0.32, 0.2],\n",
    "    'œÅ_20%':       [0.35, 0.54, 0.57, 0.57, 0.36, 0.19],\n",
    "    'œÅ_23%':       [0.44, 0.60 ,0.53, 0.33, 0.38, 0.19],\n",
    "    'œÅ_25%':       [0.44, 0.59, 0.57, 0.29, 0.39, 0.19],\n",
    "    'œÅ_30%':       [0.44, 0.63, 0.69, 0.17, 0.39,0.19],\n",
    "    'bias(%) 10%': [-66.51, 64.85, -22.37, -58.53, -10.94, -81.62],\n",
    "    'bias(%) 15%': [-61.56, 59.45, -23.35, -44.77, -3.23, -81.95],\n",
    "    'bias(%) 20%': [-70.95, 54.26, -26.67, -38.62, 0.12, -83.11],\n",
    "    'bias(%) 23%': [-70.95, 32.19, -31.52, -25.26, 1.77, -82.62],\n",
    "    'bias(%) 25%': [-70.95, 43.42, -30.00, -13.78, 1.60, -82.62],\n",
    "    'bias(%) 30%': [-70.95, 27.68, -28.89, -3.96, 14.07, -82.62],\n",
    "    '68\"%\"ile 10%': [81.11, 103.96, 66.13, 71.83, 67.65, 89.78],\n",
    "    '68\"%\"ile 15%': [81.11, 99.08, 67.36, 64.00, 61.75, 89.12],\n",
    "    '68\"%\"ile 20%': [81.91, 95.11, 69.54, 56.02, 59.94, 89.63],\n",
    "    '68\"%\"ile 23%': [81.91, 73.08, 58.52, 58.14, 56.04, 89.63],\n",
    "    '68\"%\"ile 25%': [81.91, 91.57, 56.98, 73.22, 52.98, 89.63],\n",
    "    '68\"%\"ile 30%': [81.91, 81.39, 53.38, 76.25, 58.63, 89.63],\n",
    "}\n",
    "\n",
    "summary_cv = pd.DataFrame(CV_rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator, PercentFormatter\n",
    "# Columns for panels\n",
    "rho_cols = summary_cv.filter(regex=r'^68\"%\\\"?ile\\s*\\d+%$').columns.tolist()\n",
    "\n",
    "# ---- Per-river label offsets (pixels) to prevent overlap ----\n",
    "LABEL_OFFSET = {\n",
    "    \"Cape Fear\": (-10, 10),\n",
    "    \"Po\":        (10, 10),   # push down so it won't collide with Cape Fear\n",
    "    \"Sacramento\":(10, 10),\n",
    "    \"Atrato\":    (10, 10),\n",
    "    \"Pee Dee\":   (-10, 10),\n",
    "    \"Garonne\":   (10, -4),\n",
    "}\n",
    "\n",
    "# ---- Plot ----\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, rho_cols):\n",
    "    x = summary_cv['cv_val']\n",
    "    y = summary_cv[col]\n",
    "\n",
    "    # Scatter (zorder so points under labels)\n",
    "    ax.scatter(x, y, s=100, zorder=2)\n",
    "\n",
    "    # River labels with per-river offsets + leader lines\n",
    "    for xi, yi, name in zip(x, y, summary_cv['River']):\n",
    "        dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "        ax.annotate(\n",
    "            name,\n",
    "            xy=(xi, yi),\n",
    "            xytext=(dx, dy),\n",
    "            textcoords='offset points',\n",
    "            ha='left' if dx >= 0 else 'right',\n",
    "            va='center',\n",
    "            fontsize=12,\n",
    "            bbox=None,\n",
    "            arrowprops=dict(arrowstyle='-', lw=0.7, alpha=0.7),\n",
    "            zorder=0,\n",
    "            clip_on=False\n",
    "        )\n",
    "\n",
    "    # Fixed scales\n",
    "    ax.set_xlim(0, 0.5)\n",
    "    ax.set_ylim(0, 105)\n",
    "\n",
    "    # Force y-axis ticks every 0.1\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter())\n",
    "\n",
    "    # Bigger tick labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "    # Axis labels\n",
    "    ax.set_xlabel('CV', fontsize=14)\n",
    "    ax.set_ylabel(col, fontsize=14)\n",
    "\n",
    "    # Inner dashed grid (major + minor)\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "    # Spearman œÅ inside plot (bottom-right)\n",
    "    r, _ = spearmanr(x, y)\n",
    "    ax.text(0.95, 0.05, f\"œÅ = {r:.2f}\",\n",
    "            transform=ax.transAxes, ha='right', va='bottom',\n",
    "            fontsize=10, fontweight='bold')\n",
    "\n",
    "# Hide unused panels if any (not needed here but safe)\n",
    "for ax in axes[len(rho_cols):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(summary[\"CV_width\"], summary[\"median_rho\"], s=100, color=\"steelblue\")\n",
    "\n",
    "# Add labels for each point\n",
    "for i, row in summary.iterrows():\n",
    "    plt.text(row[\"CV_width\"]+0.005, row[\"median_rho\"], row[\"riverID\"], fontsize=9)\n",
    "\n",
    "plt.xlabel(\"Width Variability (CV)\", fontsize=12)\n",
    "plt.ylabel(\"Median Spearman's œÅ\", fontsize=12)\n",
    "plt.title(\"Relationship between Width Variability and Spearman Correlation\", fontsize=13)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04540af",
   "metadata": {},
   "source": [
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_river_grid(df, river_order=None, suptitle=\"r vs œÅ across rivers\"):\n",
    "    required = {\"river\", \"setting\", \"r\", \"rho\", \"r_p\", \"rho_p\"}\n",
    "    if missing := required - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in pd.unique(df[\"river\"]) if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Color map per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global axis limits\n",
    "    xmin = min(0.0, df[\"r\"].min()) - 0.02\n",
    "    xmax = max(1.0, df[\"r\"].max()) + 0.02\n",
    "    ymin = min(0.0, df[\"rho\"].min()) - 0.02\n",
    "    ymax = max(1.0, df[\"rho\"].max()) + 0.02\n",
    "\n",
    "    # Grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            for _, row in chunk.iterrows():\n",
    "                nonsig = (row[\"r_p\"] > 0.05) or (row[\"rho_p\"] > 0.05)\n",
    "                marker = \"x\" if nonsig else \"o\"\n",
    "                kwargs = dict(s=70, marker=marker)\n",
    "                if nonsig:\n",
    "                    kwargs.update(color=col, linewidth=1.0, alpha=0.95)\n",
    "                else:\n",
    "                    kwargs.update(facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95)\n",
    "                ax.scatter(row[\"r\"], row[\"rho\"], **kwargs)\n",
    "\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=0.6, alpha=0.4)\n",
    "        ax.axhline(0, lw=0.6, alpha=0.4)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"œÅ (Spearman)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"r (Pearson)\")\n",
    "\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Legends outside\n",
    "    settings_legend = fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Segment width\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(5, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)   # OUTSIDE bottom\n",
    "    )\n",
    "\n",
    "    signif_handles = [\n",
    "        Line2D([], [], marker='o', linestyle='None',\n",
    "               markerfacecolor='white', markeredgecolor='black',\n",
    "               markersize=7, markeredgewidth=0.8, label='p ‚â§ 0.05 (significant)'),\n",
    "        Line2D([], [], marker='x', linestyle='None',\n",
    "               color='black', markersize=7, label='p > 0.05 (not significant)')\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=signif_handles,\n",
    "        loc=\"center left\",\n",
    "        frameon=True,\n",
    "        title=\"Significance\",\n",
    "        bbox_to_anchor=(1.01, 0.5)   # OUTSIDE right\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.9, 0.95])  # leave room\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"r\":0.44,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.05},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"r\":0.41,\"r_p\":0.06,\"rho\":0.37,\"rho_p\":0.09},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.02,\"rho\":0.47,\"rho_p\":0.04},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"r\":0.44,\"r_p\":0.06,\"rho\":0.38,\"rho_p\":0.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"r\":0.62,\"r_p\":2.5e-4,\"rho\":0.65,\"rho_p\":9.3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"r\":0.83,\"r_p\":4.3e-8,\"rho\":0.70,\"rho_p\":3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"r\":0.35,\"r_p\":0.07,\"rho\":0.56,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"r\":0.60,\"r_p\":0.002,\"rho\":0.59,\"rho_p\":0.002},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"r\":0.49,\"r_p\":0.009,\"rho\":0.54,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"r\":0.46,\"r_p\":0.013,\"rho\":0.59,\"rho_p\":0.001},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"r\":0.55,\"r_p\":0.003,\"rho\":0.63,\"rho_p\":4.7e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"r\":0.64,\"r_p\":4.6e-6,\"rho\":0.67,\"rho_p\":1e-6},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"r\":0.45,\"r_p\":0.003,\"rho\":0.51,\"rho_p\":5e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"r\":0.37,\"r_p\":0.02,\"rho\":0.35,\"rho_p\":0.02},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"r\":0.46,\"r_p\":0.002,\"rho\":0.44,\"rho_p\":0.004},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"r\":0.56,\"r_p\":1.2e-4,\"rho\":0.57,\"rho_p\":7.1e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"r\":0.54,\"r_p\":2.6e-4,\"rho\":0.53,\"rho_p\":4.2e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"r\":0.58,\"r_p\":4e-5,\"rho\":0.57,\"rho_p\":7.5e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"r\":0.65,\"r_p\":2.5e-6,\"rho\":0.69,\"rho_p\":3.8e-7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"r\":0.54,\"r_p\":1.7e-4,\"rho\":0.52,\"rho_p\":3.4e-4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"r\":0.10,\"r_p\":0.52,\"rho\":-0.08,\"rho_p\":0.59},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"r\":0.63,\"r_p\":3.3e-6,\"rho\":0.7,\"rho_p\":9.7e-8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"r\":0.69,\"r_p\":4.1e-7,\"rho\":0.63,\"rho_p\":6.6e-6},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"r\":0.65,\"r_p\":2.4e-6,\"rho\":0.57,\"rho_p\":6.8e-5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"r\":0.41,\"r_p\":0.006,\"rho\":0.33,\"rho_p\":0.03},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"r\":0.32,\"r_p\":0.03,\"rho\":0.29,\"rho_p\":0.056},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"r\":0.14,\"r_p\":0.4,\"rho\":0.17,\"rho_p\":0.25},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"r\":0.21,\"r_p\":0.004,\"rho\":0.3,\"rho_p\":2.8e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"r\":0.34,\"r_p\":1.6e-6,\"rho\":0.36,\"rho_p\":3.6e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"r\":0.2,\"r_p\":0.005,\"rho\":0.29,\"rho_p\":4e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"r\":0.28,\"r_p\":6.4e-5,\"rho\":0.32,\"rho_p\":4.8e-6},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"r\":0.34,\"r_p\":1.3e-6,\"rho\":0.36,\"rho_p\":3.2e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"r\":0.36,\"r_p\":5e-7,\"rho\":0.38,\"rho_p\":9.4e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"r\":0.36,\"r_p\":2.7e-7,\"rho\":0.39,\"rho_p\":2.3e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"r\":0.34,\"r_p\":2.1e-6,\"rho\":0.39,\"rho_p\":3.6e-8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"r\":0.18,\"r_p\":0.46,\"rho\":0.08,\"rho_p\":0.74},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"r\":0.24,\"r_p\":0.34,\"rho\":0.06,\"rho_p\":0.81},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"r\":0.4,\"r_p\":0.14,\"rho\":0.41,\"rho_p\":0.12},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.16,\"rho\":0.26,\"rho_p\":0.32},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"r\":0.26,\"r_p\":0.3,\"rho\":0.23,\"rho_p\":0.37},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"r\":0.26,\"r_p\":0.31,\"rho\":0.16,\"rho_p\":0.54},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"r\":0.18,\"r_p\":0.47,\"rho\":0.09,\"rho_p\":0.73},\n",
    "      \n",
    "])\n",
    "plot_river_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_p68_grid(\n",
    "    df,\n",
    "    river_order=None,\n",
    "    suptitle=\"Bias vs 68%ile Error across rivers\",\n",
    "    bias_col=\"bias_pct\",     # x-axis (e.g., -78.3 means -78.3%)\n",
    "    p68_col=\"p68_pct\",       # y-axis (e.g., 88.0 means 88%)\n",
    "):\n",
    "    \"\"\"\n",
    "    Faceted 2x3 scatter: one subplot per river with shared axes.\n",
    "    Expects tidy df columns: river, setting, bias_col, p68_col.\n",
    "\n",
    "    - Color encodes 'setting'\n",
    "    - Vertical dashed line at Bias = 0\n",
    "    - Legends placed OUTSIDE the grid (no overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    required = {\"river\", \"setting\", bias_col, p68_col}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # Stable river order\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in rivers if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Palette per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global limits (include 0 on x for reference)\n",
    "    xmin = min(0.0, df[bias_col].min()) - 2\n",
    "    xmax = max(0.0, df[bias_col].max()) + 2\n",
    "    ymin = max(0.0, min(df[p68_col].min(), 0)) - 2\n",
    "    ymax = df[p68_col].max() + 2\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # One proxy per setting for legend\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            ax.scatter(\n",
    "                chunk[bias_col], chunk[p68_col],\n",
    "                s=70, marker=\"o\", facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95\n",
    "            )\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        # Cosmetics\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=1.0, alpha=0.5, linestyle=\"--\")  # Bias = 0 reference\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Label only left/bottom axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"68th Percentile Error (%)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"Bias (%)\")\n",
    "\n",
    "    # Hide unused axes if <6 rivers\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Settings legend OUTSIDE bottom\n",
    "    fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Setting\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(6, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.98, 0.95])  # room for legend & title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"bias_pct\":-77.2,\"p68_pct\": 87.7},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"bias_pct\":-78.3,\"p68_pct\": 88.0},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"bias_pct\":-66.5,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"bias_pct\":-61.6,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"bias_pct\":53.4,\"p68_pct\": 99.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"bias_pct\":19.1,\"p68_pct\": 51.3},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"bias_pct\":64.9,\"p68_pct\": 103.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"bias_pct\":59.5,\"p68_pct\": 99.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"bias_pct\":54.3,\"p68_pct\": 95.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"bias_pct\":43.4,\"p68_pct\": 91.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"bias_pct\":27.7,\"p68_pct\": 81.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"bias_pct\":-29.9,\"p68_pct\": 55.9},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"bias_pct\":-18.6,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"bias_pct\":-22.4,\"p68_pct\": 66.13},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"bias_pct\":-23.4,\"p68_pct\": 67.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"bias_pct\":-26.7,\"p68_pct\": 69.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"bias_pct\":-31.5,\"p68_pct\": 58.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"bias_pct\":-30.0,\"p68_pct\": 57.0},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"bias_pct\":-28.9,\"p68_pct\": 53.4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"bias_pct\":-41.4,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"bias_pct\":-16.3,\"p68_pct\": 77.7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"bias_pct\":-58.5,\"p68_pct\": 71.8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"bias_pct\":-44.8,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"bias_pct\":-38.6,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"bias_pct\":-25.3,\"p68_pct\": 58.1},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"bias_pct\":-13.8,\"p68_pct\": 73.2},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"bias_pct\":-3.96,\"p68_pct\": 76.6},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"bias_pct\":-13.6,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"bias_pct\":-4.14,\"p68_pct\": 56.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"bias_pct\":-10.9,\"p68_pct\": 67.7},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"bias_pct\":-3.23,\"p68_pct\": 61.8},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"bias_pct\":0.12,\"p68_pct\": 59.9},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"bias_pct\":1.8,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"bias_pct\":1.6,\"p68_pct\": 53.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"bias_pct\":14.1,\"p68_pct\": 58.6},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"bias_pct\":-77.9,\"p68_pct\": 86.2},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"bias_pct\":-78.6,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"bias_pct\":-85.1,\"p68_pct\": 89.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"bias_pct\":-84.2,\"p68_pct\": 87.4},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"bias_pct\":-82.0,\"p68_pct\": 86.8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"bias_pct\":-80.5,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"bias_pct\":-79.9,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"bias_pct\":-78.1,\"p68_pct\": 86.2},\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_p68_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd166f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

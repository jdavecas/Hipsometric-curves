{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a915bdc",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85550f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, mapping, LineString, MultiLineString, shape\n",
    "import rasterio\n",
    "from rasterio.mask import mask as rio_mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyproj import Transformer\n",
    "import glob\n",
    "from shapely.ops import unary_union\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import random\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import math\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from config_paths import DATA, OUTPUT, INTERMEDIATE\n",
    "from ast import literal_eval\n",
    "import fnmatch, yaml \n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ad655",
   "metadata": {},
   "source": [
    "This code uses bathymetry models, together with SWOT data and and cross-section shapefiles, to create raster maks of WSE. Intercepting cross-sections with the masks river widths are estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77d0f2",
   "metadata": {},
   "source": [
    "#### 0. PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressions\n",
    "regress_path = pd.read_csv(OUTPUT/\"Global_22_07_25/simple/B774_huber_reg.csv\")#/dark_fr_030/8_bits/S3_15Obs_100W/jul_3_25/Po_River_huber_reg_3.csv\")) #No_Norm/dark_fr_030/8_bits/S3_15Obs_100W/jun_6_25/\n",
    "                                        #\"Po_River_huber_reg.csv\"))\n",
    "poly_path = gpd.read_file(INTERMEDIATE/\"Store/Binary_Masks/Sacramento/Shps/Sacramento_cx.shp\")\n",
    "tif_path = DATA/\"External/Bathymetries/namerica/Sacramento_River/Sacramento_EGM08.tif\"\n",
    "base_folder = INTERMEDIATE/\"Store/Binary_Masks/Atrato/\"\n",
    "#swot_simple = INTERMEDIATE/\"Store/Validation/csv/Atrato/Atrato_reg_bottom_error_simple.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871c76",
   "metadata": {},
   "source": [
    "#### 1. Subsetting only the nodes lying inside the raster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d505ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    nod = src.nodata\n",
    "    # pull lon/lat from your DataFrame\n",
    "    xs = regress_path[\"lon\"].values\n",
    "    ys = regress_path[\"lat\"].values\n",
    "\n",
    "    # ─── NEW: reproject point coords into the raster’s CRS ───\n",
    "    xs, ys = transform(\n",
    "        \"EPSG:4326\",    # source CRS of your lon/lat\n",
    "        src.crs,        # target CRS (raster.crs)\n",
    "        xs.tolist(),\n",
    "        ys.tolist(),\n",
    "    )\n",
    "    xs = np.array(xs)\n",
    "    ys = np.array(ys)\n",
    "\n",
    "    # Check raster bounds (now in raster CRS)\n",
    "    left, bottom, right, top = src.bounds\n",
    "    inside_bounds = (xs >= left) & (xs <= right) & (ys >= bottom) & (ys <= top)\n",
    "    \n",
    "    # Preallocate array for sampled values (nan for out-of-bounds)\n",
    "    sampled = np.full(xs.shape, np.nan)\n",
    "    xs_in = xs[inside_bounds]\n",
    "    ys_in = ys[inside_bounds]\n",
    "    idxs_in = np.where(inside_bounds)[0]\n",
    "    if len(xs_in) > 0:\n",
    "        vals = list(src.sample(zip(xs_in, ys_in)))\n",
    "        vals = np.array([v[0] for v in vals])\n",
    "        sampled[idxs_in] = vals\n",
    "\n",
    "    # Mask: inside bounds AND not nodata AND not nan\n",
    "    mask = inside_bounds & (~np.isnan(sampled)) & ((nod is None) | (sampled != nod))\n",
    "\n",
    "# subset and export\n",
    "regress_path_inside = regress_path[mask]\n",
    "print(f\"{regress_path_inside.shape[0]} nodes inside raster, {regress_path.shape[0]} total nodes\")\n",
    "\n",
    "regress_path_inside.to_csv(\n",
    "    os.path.join(INTERMEDIATE, \"Store/Validation/csv/Sacramento/Sacramento_validate_nodes.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde52126",
   "metadata": {},
   "source": [
    "#### 2. Extract from bathymetry the bottom of the river channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    # 1) show me the raster’s CRS\n",
    "    print(\"Raster CRS is:\", src.crs)\n",
    "\n",
    "    # 2) if your poly_path is still EPSG:4326, move it into the raster’s CRS\n",
    "    if poly_path.crs != src.crs:\n",
    "        poly_path = poly_path.to_crs(src.crs)\n",
    "\n",
    "    results = []\n",
    "    for _, row in poly_path.iterrows():\n",
    "        node = row[\"node_id\"]\n",
    "        geom = [mapping(row.geometry)]\n",
    "        out_image, _ = rio_mask(\n",
    "            src,\n",
    "            geom,\n",
    "            crop=True,         # keeps memory use tiny\n",
    "            all_touched=True,\n",
    "            nodata=src.nodata,\n",
    "            filled=False\n",
    "        )\n",
    "        arr = out_image[0].astype(\"float32\")\n",
    "        arr = np.ma.masked_equal(arr, src.nodata)\n",
    "        arr = np.ma.masked_where(np.isnan(arr), arr)\n",
    "        min_val = arr.min() if arr.count() > 0 else np.nan\n",
    "        results.append({\"node_id\": node, \"min_value\": float(min_val)})\n",
    "\n",
    "lprb = pd.DataFrame(results)\n",
    "lprb.to_csv(\n",
    "    os.path.join(\n",
    "        INTERMEDIATE,\n",
    "        \"Store/Validation/csv/Sacramento/Sacramento_channel_min_values.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprb = pd.read_csv(INTERMEDIATE/\"Store/Validation/csv/Garonne/Garonne_reg_bottom_error_simple.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc401",
   "metadata": {},
   "source": [
    "Merge the two csv files, the one with the regression values and the one with the minimum values in the river bed, and then  \n",
    "estimate the difference and errors between the intercepts and the real bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77007700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure node_id is string in both DataFrames for a valid merge\n",
    "regress_path[\"node_id\"] = regress_path[\"node_id\"].astype(str)\n",
    "lprb[\"node_id\"] = lprb[\"node_id\"].astype(str)\n",
    "\n",
    "merged = regress_path.merge(\n",
    "    lprb[[\"node_id\", \"min_value\"]],\n",
    "    on=\"node_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "intercept = np.where(\n",
    "    merged[\"intercept2\"].isnull(),\n",
    "    merged[\"intercept1\"],\n",
    "    merged[\"intercept2\"]\n",
    ")\n",
    "\n",
    "merged[\"error\"] = intercept - merged[\"min_value\"]\n",
    "merged[\"abs_error\"] = merged[\"error\"].abs()\n",
    "merged[\"rel_error_%\"] = merged[\"error\"] / merged[\"min_value\"] * 100\n",
    "\n",
    "merged.to_csv(os.path.join(INTERMEDIATE, \"Store/Validation/csv/Sacramento/Sacramento_reg_bottom_error.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b64e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_csv_path = INTERMEDIATE/\"Store/Validation/csv/Atrato/Less_filters/Atrato_reg_bottom_error.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13409f",
   "metadata": {},
   "source": [
    "Create folders for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Base directory where you want to create the folders\n",
    "#    For example: \"/path/to/output_folders\"\n",
    "\n",
    "\n",
    "# 3) Read the master CSV into a DataFrame\n",
    "master_df = pd.read_csv(master_csv_path)\n",
    "\n",
    "# 4) Extract unique node_id values (as strings)\n",
    "unique_nodes = master_df[\"node_id\"].astype(str).unique()\n",
    "\n",
    "# 5) Write the list of node_id codes to a CSV (no header, one per line)\n",
    "list_csv_path = os.path.join(base_folder, \"node_ids_list.csv\")\n",
    "pd.Series(unique_nodes).to_csv(list_csv_path, index=False, header=False)\n",
    "\n",
    "# 6) Loop over each unique node_id and create a folder if it doesn't already exist\n",
    "for node_id in unique_nodes:\n",
    "    node_folder = os.path.join(base_folder, node_id)\n",
    "    # exist_ok=True means “do nothing if the folder already exists”\n",
    "    os.makedirs(node_folder, exist_ok=True)\n",
    "    print(f\"Ensured folder exists: {node_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0bcba",
   "metadata": {},
   "source": [
    "Create in each node_id folder, a csv file with a list of WSE extracted from the regression csv for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_wse_csvs(\n",
    "    master_csv_path: str,\n",
    "    base_folder: str,\n",
    "    subtract_value: float = 2.04,\n",
    "    apply_subtraction: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each node_id in master_df, write:\n",
    "        1) raw WSEs → base_folder/node_id/node_id.csv\n",
    "        2) (optionally) adjusted WSEs (WSE - subtract_value)\n",
    "            → base_folder/node_id/node_id_adjusted.csv\n",
    "\n",
    "    Args:\n",
    "        master_csv_path: path to CSV with columns 'node_id' and 'wse'\n",
    "        base_folder: directory that will contain subfolders named by node_id\n",
    "        subtract_value: the value to subtract from each WSE when apply_subtraction=True\n",
    "        apply_subtraction: if True, also create the adjusted CSV\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(master_csv_path):\n",
    "        print(f\"Error: File '{master_csv_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the master table\n",
    "    df = pd.read_csv(master_csv_path)\n",
    "\n",
    "    # Loop through each distinct node_id\n",
    "    for node_id in df[\"node_id\"].astype(str).unique():\n",
    "        # Grab all the WSE values for this node\n",
    "        wse_values = df.loc[df[\"node_id\"].astype(str) == node_id, \"wse\"]\n",
    "\n",
    "        # Ensure the node's folder exists\n",
    "        node_folder = os.path.join(base_folder, node_id)\n",
    "        os.makedirs(node_folder, exist_ok=True)\n",
    "\n",
    "        # 1) Write the raw WSEs\n",
    "        raw_path = os.path.join(node_folder, f\"{node_id}.csv\")\n",
    "        wse_values.to_csv(raw_path, index=False, header=False)\n",
    "        print(f\"✔ Saved {len(wse_values)} raw WSEs to {raw_path}\")\n",
    "\n",
    "        # 2) If requested, write the adjusted WSEs\n",
    "        if apply_subtraction:\n",
    "            adjusted = wse_values - subtract_value\n",
    "            adj_path = os.path.join(node_folder, f\"{node_id}_adjusted.csv\")\n",
    "            adjusted.to_csv(adj_path, index=False, header=False)\n",
    "            print(f\"✔ Saved {len(adjusted)} adjusted WSEs to {adj_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just export raw WSE lists:\n",
    "export_wse_csvs(master_csv_path, base_folder)\n",
    "\n",
    "# Export raw WSE lists *and* create adjusted CSVs (subtract 2.04):\n",
    "#export_wse_csvs(master_csv_path, base_folder, apply_subtraction=True)\n",
    "\n",
    "# Export with a different subtraction value, say 1.5:\n",
    "#export_wse_csvs(master_csv_path, base_folder, subtract_value=1.5, apply_subtraction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3e509",
   "metadata": {},
   "source": [
    "#### 3. Create a mask for each WSE in each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_to    = None                     # or e.g. [\"21406100080691\", \"21406100080702\"]\n",
    "\n",
    "# Tunable: number of worker threads for writing masks\n",
    "_max_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "def _write_mask_single(out_path, profile, dem, valid, nodata_mask, nodata_value, t):\n",
    "    \"\"\"Build and write one mask TIFF for threshold t.\"\"\"\n",
    "    mask = np.zeros_like(dem, dtype=np.uint8)\n",
    "    np.less_equal(dem, t, where=valid, out=mask, casting=\"unsafe\")\n",
    "    mask[nodata_mask] = 255\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(mask, 1)\n",
    "    return out_path\n",
    "\n",
    "with rasterio.Env(GDAL_NUM_THREADS=\"ALL_CPUS\", VSI_CACHE=\"TRUE\", VSI_CACHE_SIZE=str(256*1024*1024)):\n",
    "    # === STEP 1: Read the DEM ONCE ===\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        dem = src.read(1, resampling=Resampling.nearest)\n",
    "        profile = src.profile.copy()\n",
    "        nodata_value = src.nodata if src.nodata is not None else -9999\n",
    "\n",
    "    profile.update({\n",
    "        \"dtype\": rasterio.uint8,\n",
    "        \"count\": 1,\n",
    "        \"nodata\": 255,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    nodata_mask = (dem == nodata_value)\n",
    "    valid = ~nodata_mask\n",
    "\n",
    "    # === STEP 2: Locate all node_id subfolders ===\n",
    "    node_folders = [d for d in glob.glob(os.path.join(base_folder, \"*\")) if os.path.isdir(d)]\n",
    "    if limit_to is not None:\n",
    "        node_folders = [d for d in node_folders if os.path.basename(d) in limit_to]\n",
    "\n",
    "    print(f\"🔍 Found {len(node_folders)} node‐folders to process.\\n\")\n",
    "\n",
    "    # === STEP 3: Process each folder ===\n",
    "    for node_folder in node_folders:\n",
    "        node_id = os.path.basename(node_folder)\n",
    "        print(f\" Processing node_id = {node_id}\")\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(node_folder, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            print(f\" No CSVs found inside {node_folder}. Skipping.\\n\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_files:\n",
    "            try:\n",
    "                df_wse = pd.read_csv(csv_path, header=None, names=[\"wse\"], dtype=float)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {csv_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            wse_values = df_wse[\"wse\"].dropna().values\n",
    "            if wse_values.size == 0:\n",
    "                print(f\"No valid WSE values in {csv_path}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Found {len(wse_values)} thresholds in {os.path.basename(csv_path)}\")\n",
    "\n",
    "            masks_folder = os.path.join(node_folder, \"masks\")\n",
    "            os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "            futures = []\n",
    "            with ThreadPoolExecutor(max_workers=_max_workers) as ex:\n",
    "                for t in wse_values:\n",
    "                    t_str = str(t).replace(\".\", \"_\").replace(\"-\", \"m\")\n",
    "                    out_name = f\"mask_{t_str}.tif\"\n",
    "                    out_path = os.path.join(masks_folder, out_name)\n",
    "\n",
    "                    if os.path.exists(out_path):\n",
    "                        print(f\"      • {out_name} already exists. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    futures.append(\n",
    "                        ex.submit(_write_mask_single, out_path, profile, dem, valid, nodata_mask, nodata_value, t)\n",
    "                    )\n",
    "\n",
    "                for f in as_completed(futures):\n",
    "                    try:\n",
    "                        done_path = f.result()\n",
    "                        print(f\"Saved {os.path.basename(done_path)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      • Failed to write a mask: {e}\")\n",
    "\n",
    "        print(\"\")  # blank line\n",
    "\n",
    "    print(\"All node‐folders processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454f46b",
   "metadata": {},
   "source": [
    "#### 4. Measuring river widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "\n",
    "def ensure_mask_georeferenced(mask_path, ref_crs, ref_transform):\n",
    "    with rasterio.open(mask_path, 'r+') as dst:\n",
    "        needs_update = False\n",
    "        if dst.crs is None:\n",
    "            dst.crs = ref_crs\n",
    "            needs_update = True\n",
    "        if dst.transform.is_identity:\n",
    "            dst.transform = ref_transform\n",
    "            needs_update = True\n",
    "        if needs_update:\n",
    "            print(f\"Updated georeferencing for: {os.path.basename(mask_path)}\")\n",
    "\n",
    "def fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform):\n",
    "    for node_folder in os.listdir(base_folder):\n",
    "        masks_dir = os.path.join(base_folder, node_folder, \"masks\")\n",
    "        if os.path.isdir(masks_dir):\n",
    "            for mask_path in glob.glob(os.path.join(masks_dir, \"mask_*.tif\")):\n",
    "                ensure_mask_georeferenced(mask_path, ref_crs, ref_transform)\n",
    "\n",
    "def extract_wse(mask_filename):\n",
    "    # strip extension and split on underscores\n",
    "    name = os.path.splitext(os.path.basename(mask_filename))[0]\n",
    "    parts = name.split('_')   # [\"mask\", \"50\", \"535\"]\n",
    "    if len(parts) == 3:\n",
    "        return float(f\"{parts[1]}.{parts[2]}\")  # \"50\" + \".\" + \"535\" → 50.535\n",
    "    return None\n",
    "\n",
    "def possible_node_id_strings(node_id):\n",
    "    candidates = set()\n",
    "    candidates.add(str(node_id))\n",
    "    try:\n",
    "        as_float = float(node_id)\n",
    "        candidates.add(str(int(as_float)))\n",
    "        candidates.add(f\"{as_float:.3f}\")\n",
    "        candidates.add(f\"{as_float:.1f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return list(candidates)\n",
    "\n",
    "def measure_mask_segments_by_sampling(polyline, mask_path, gdf_crs, spacing=1):\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        raster_crs = src.crs\n",
    "\n",
    "        # ─── REPLACED: use the mask/raster CRS (e.g. EPSG:6339) instead of Italy UTM ───\n",
    "        utm_crs = raster_crs\n",
    "\n",
    "        if gdf_crs and gdf_crs != utm_crs:\n",
    "            polyline_proj = gpd.GeoSeries([polyline], crs=gdf_crs).to_crs(utm_crs).iloc[0]\n",
    "        else:\n",
    "            polyline_proj = polyline\n",
    "\n",
    "        length = polyline_proj.length\n",
    "        if length < spacing:\n",
    "            return []\n",
    "\n",
    "        num_points = int(length / spacing)\n",
    "        distances = np.linspace(0, length, num_points)\n",
    "        points = [polyline_proj.interpolate(d) for d in distances]\n",
    "\n",
    "        coords = gpd.GeoSeries(points, crs=utm_crs).to_crs(raster_crs)\n",
    "        coords = [(pt.x, pt.y) for pt in coords]\n",
    "\n",
    "        vals = [v[0] for v in src.sample(coords)]\n",
    "        vals = [int(round(v)) if v in [0, 1] else 0 for v in vals]\n",
    "\n",
    "        segments = []\n",
    "        in_segment = False\n",
    "        start_dist = None\n",
    "\n",
    "        for i in range(1, len(vals)):\n",
    "            prev, curr = vals[i - 1], vals[i]\n",
    "            if not in_segment and prev == 0 and curr == 1:\n",
    "                in_segment = True\n",
    "                start_dist = distances[i]\n",
    "            elif in_segment and prev == 1 and curr == 0:\n",
    "                end_dist = distances[i]\n",
    "                seg_len = end_dist - start_dist\n",
    "                if seg_len >= 20:\n",
    "                    segments.append(seg_len)\n",
    "                in_segment = False\n",
    "\n",
    "        return segments\n",
    "\n",
    "def process_single_polyline(args):\n",
    "    idx, row, base_folder, node_id_field, gdf_crs = args\n",
    "    node_id_raw = row[node_id_field]\n",
    "    possible_names = possible_node_id_strings(node_id_raw)\n",
    "    folder_path = None\n",
    "    used_node_id = None\n",
    "    for node_id in possible_names:\n",
    "        test_path = os.path.join(base_folder, node_id, 'masks')\n",
    "        if os.path.exists(test_path):\n",
    "            folder_path = test_path\n",
    "            used_node_id = node_id\n",
    "            break\n",
    "    if folder_path is None:\n",
    "        return []\n",
    "\n",
    "    polyline = row.geometry\n",
    "    mask_files = sorted(glob.glob(os.path.join(folder_path, 'mask_*.tif')))\n",
    "    if not mask_files:\n",
    "        return []\n",
    "\n",
    "    row_results = []\n",
    "    max_segments = 0\n",
    "    for mask_file in mask_files:\n",
    "        wse_val = extract_wse(os.path.basename(mask_file))\n",
    "        if wse_val is None:\n",
    "            continue\n",
    "        segment_lengths = measure_mask_segments_by_sampling(polyline, mask_file, gdf_crs)\n",
    "        if segment_lengths:\n",
    "            width = max(segment_lengths)\n",
    "            n_segments = len(segment_lengths)\n",
    "            others = sorted([s for s in segment_lengths if s != width], reverse=True)\n",
    "        else:\n",
    "            width = 0.0\n",
    "            n_segments = 0\n",
    "            others = []\n",
    "        max_segments = max(max_segments, len(others))\n",
    "        row_results.append([used_node_id, wse_val, width, n_segments] + others)\n",
    "    return row_results, max_segments\n",
    "\n",
    "def process_all_parallel(base_folder, shapefile_path, output_csv_path, reference_raster_path, node_id_field='node_id', n_workers=None):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_results = []\n",
    "    max_segments_overall = 0\n",
    "\n",
    "    with rasterio.open(reference_raster_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "\n",
    "    print(\"Checking and fixing georeferencing for all masks...\")\n",
    "    fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform)\n",
    "    print(\"Georeference check complete.\\n\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        args_iterable = [\n",
    "            (idx, row, base_folder, node_id_field, gdf.crs)\n",
    "            for idx, row in gdf.iterrows()\n",
    "        ]\n",
    "        futures = [executor.submit(process_single_polyline, args) for args in args_iterable]\n",
    "        for i, future in enumerate(as_completed(futures), 1):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                row_results, max_segs = res\n",
    "                all_results.extend(row_results)\n",
    "                max_segments_overall = max(max_segments_overall, max_segs)\n",
    "            if i % 5 == 0 or i == len(futures):\n",
    "                print(f\"Processed {i}/{len(futures)} polylines...\")\n",
    "\n",
    "    columns = ['node_id', 'wse', 'width', 'n_segments'] + [f'seg_{i+1}' for i in range(max_segments_overall)]\n",
    "    padded_results = []\n",
    "    for row in all_results:\n",
    "        base = row[:4]\n",
    "        segs = row[4:]\n",
    "        segs = segs + [float('nan')] * (max_segments_overall - len(segs))\n",
    "        padded_results.append(base + segs)\n",
    "\n",
    "    df = pd.DataFrame(padded_results, columns=columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "# Example usage (edit paths as needed):\n",
    "path2poly = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Sacramento/Shps/Sacramento_cx.shp\")\n",
    "process_all_parallel(base_folder, path2poly, os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Sacramento/Bath_widths.csv\"), \n",
    "                    tif_path, n_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5c451",
   "metadata": {},
   "source": [
    "### 5. Hubber regression for dynamic threshold: ###\n",
    "\n",
    "This means that piecewise models can include either two segments (one breakpoint) or three segments (two breakpoints), provided that each segment spans more than a fixed percentage of the river width. This constraint prevents unrealistically short segments of only 2–5 m, which would be impossible for SWOT to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_aic(rss, n, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "def fit_huber(X_input, y_input):\n",
    "    model = HuberRegressor(epsilon=1.345, max_iter=500)\n",
    "    model.fit(X_input, y_input)\n",
    "    y_pred = model.predict(X_input)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(\n",
    "    X, y,\n",
    "    min_width_range=None,   # if None: dynamic span per group = alpha * {median|mean}(width)\n",
    "    alpha=0.30,             # fraction for dynamic span (default 30%)\n",
    "    basis='median'          # 'median' (default) or 'mean'\n",
    "):\n",
    "    \"\"\"\n",
    "    Fits Huber regressions (simple / 1-BP / 2-BP).\n",
    "    A breakpoint is accepted only if EACH resulting segment's width span (np.ptp)\n",
    "    is >= threshold. Threshold is:\n",
    "        - min_width_range (absolute, if provided), otherwise\n",
    "        - alpha * {median|mean}(width) computed per group (default: 30% of median).\n",
    "\n",
    "    NOTE: Function name, keys, and overall behavior remain the same as your original.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    results = {\n",
    "        'best_model_type': 'simple',\n",
    "        'bath_slope1': None,\n",
    "        'bath_slope2': None,\n",
    "        'bath_slope3': None,\n",
    "        'r2': None,\n",
    "        'intercepts': (None, None, None),\n",
    "        'bp': None\n",
    "    }\n",
    "\n",
    "    # --- dynamic per-group span (default) ---\n",
    "    if min_width_range is None:\n",
    "        if basis == 'mean':\n",
    "            min_width_range = alpha * float(np.nanmean(X))\n",
    "        else:\n",
    "            min_width_range = alpha * float(np.nanmedian(X))\n",
    "\n",
    "    def is_invalid_slope(s):\n",
    "        # invalid if None/NaN/inf or < 0.001 or negative\n",
    "        return (s is None) or np.isnan(s) or np.isinf(s) or (s < 0.001)\n",
    "\n",
    "    # === Simple model ===\n",
    "    model_s, y_pred_s, res_s = fit_huber(X.reshape(-1, 1), y)\n",
    "    rss_s = np.sum(res_s ** 2)\n",
    "    aic_s = compute_aic(rss_s, n, 2)\n",
    "    slope_simple = model_s.coef_[0]\n",
    "    intercept_simple = model_s.intercept_\n",
    "    best_model_type = 'simple'\n",
    "    best_aic = aic_s\n",
    "\n",
    "    slope1 = slope2 = slope3 = None\n",
    "    intercept1 = intercept2 = intercept3 = None\n",
    "    best_bp = None\n",
    "    best_bps = None\n",
    "\n",
    "    unique_x = np.unique(X)\n",
    "\n",
    "    # === Try 2 breakpoints (three segments) ===\n",
    "    if len(unique_x) >= 4:\n",
    "        for i in range(1, len(unique_x) - 2):\n",
    "            for j in range(i + 1, len(unique_x) - 1):\n",
    "                bp1 = unique_x[i]\n",
    "                bp2 = unique_x[j]\n",
    "                if bp1 >= bp2:\n",
    "                    continue\n",
    "\n",
    "                seg1 = X[X < bp1]\n",
    "                seg2 = X[(X >= bp1) & (X < bp2)]\n",
    "                seg3 = X[X >= bp2]\n",
    "\n",
    "                # need at least 2 points per side and sufficient width span (dynamic per group)\n",
    "                if any(seg.size < 2 for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "                if any(np.ptp(seg) < min_width_range for seg in (seg1, seg2, seg3)):\n",
    "                    continue\n",
    "\n",
    "                X_pw2 = np.column_stack([X, np.maximum(0, X - bp1), np.maximum(0, X - bp2)])\n",
    "                try:\n",
    "                    model_pw2, y_pred_pw2, res_pw2 = fit_huber(X_pw2, y)\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                h0 = model_pw2.intercept_\n",
    "                h1, d1, d2 = model_pw2.coef_\n",
    "\n",
    "                slope_left  = h1\n",
    "                slope_mid   = h1 + d1\n",
    "                slope_right = h1 + d1 + d2\n",
    "\n",
    "                # reject invalid / negative / too-flat segments\n",
    "                if any(is_invalid_slope(s) for s in (slope_left, slope_mid, slope_right)):\n",
    "                    continue\n",
    "\n",
    "                intercept_left  = h0\n",
    "                intercept_mid   = h0 - d1 * bp1\n",
    "                intercept_right = h0 - d1 * bp1 - d2 * bp2\n",
    "\n",
    "                rss_pw2 = np.sum(res_pw2 ** 2)\n",
    "                aic_pw2 = compute_aic(rss_pw2, n, 4)  # 4 params: h0, h1, d1, d2\n",
    "\n",
    "                if aic_pw2 < best_aic:\n",
    "                    best_model_type = 'piecewise_2bp'\n",
    "                    best_aic = aic_pw2\n",
    "                    # keep naming convention: slope1 = rightmost (higher widths)\n",
    "                    slope1, slope2, slope3 = slope_right, slope_mid, slope_left\n",
    "                    intercept1, intercept2, intercept3 = intercept_right, intercept_mid, intercept_left\n",
    "                    best_bps = (bp1, bp2)\n",
    "\n",
    "    # === Try 1 breakpoint (two segments) ===\n",
    "    if best_model_type == 'simple' and len(unique_x) >= 3:\n",
    "        for bp in unique_x[1:-1]:\n",
    "            seg_left = X[X < bp]\n",
    "            seg_right = X[X >= bp]\n",
    "\n",
    "            if seg_left.size < 2 or seg_right.size < 2:\n",
    "                continue\n",
    "            if (np.ptp(seg_left) < min_width_range) or (np.ptp(seg_right) < min_width_range):\n",
    "                continue\n",
    "\n",
    "            X_pw1 = np.column_stack([X, np.maximum(0, X - bp)])\n",
    "            try:\n",
    "                model_pw1, y_pred_pw1, res_pw1 = fit_huber(X_pw1, y)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            h0 = model_pw1.intercept_\n",
    "            h1, d1 = model_pw1.coef_\n",
    "\n",
    "            slope_left  = h1\n",
    "            slope_right = h1 + d1\n",
    "\n",
    "            if any(is_invalid_slope(s) for s in (slope_left, slope_right)):\n",
    "                continue\n",
    "\n",
    "            intercept_left  = h0\n",
    "            intercept_right = h0 - d1 * bp\n",
    "\n",
    "            rss_pw1 = np.sum(res_pw1 ** 2)\n",
    "            aic_pw1 = compute_aic(rss_pw1, n, 3)  # 3 params: h0, h1, d1\n",
    "\n",
    "            if aic_pw1 < best_aic:\n",
    "                best_model_type = 'piecewise'\n",
    "                best_aic = aic_pw1\n",
    "                # naming convention: slope1 = right (higher widths), slope2 = left\n",
    "                slope1, slope2 = slope_right, slope_left\n",
    "                intercept1, intercept2 = intercept_right, intercept_left\n",
    "                best_bp = bp\n",
    "\n",
    "    # === Store results (match original keys) ===\n",
    "    if best_model_type == 'piecewise_2bp':\n",
    "        results['type'] = 'piecewise_2bp'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['bath_slope3'] = slope3\n",
    "        results['intercepts'] = (intercept1, intercept2, intercept3)\n",
    "        results['bp'] = best_bps\n",
    "        y_pred = np.empty_like(y)\n",
    "        y_pred[:] = slope3 * X + intercept3\n",
    "        y_pred[X >= best_bps[0]] = slope2 * X[X >= best_bps[0]] + intercept2\n",
    "        y_pred[X >= best_bps[1]] = slope1 * X[X >= best_bps[1]] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    elif best_model_type == 'piecewise':\n",
    "        results['type'] = 'piecewise'\n",
    "        results['bath_slope1'] = slope1\n",
    "        results['bath_slope2'] = slope2\n",
    "        results['intercepts'] = (intercept1, intercept2, None)\n",
    "        results['bp'] = best_bp\n",
    "        y_pred = slope2 * X + intercept2\n",
    "        y_pred[X >= best_bp] = slope1 * X[X >= best_bp] + intercept1\n",
    "        results['r2'] = r2_score(y, y_pred)\n",
    "\n",
    "    else:\n",
    "        results['type'] = 'simple'\n",
    "        results['bath_slope1'] = slope_simple\n",
    "        results['intercepts'] = (intercept_simple, None, None)\n",
    "        results['r2'] = r2_score(y, model_s.predict(X.reshape(-1, 1)))\n",
    "\n",
    "    return results\n",
    "\n",
    "def groupwise_regression(\n",
    "    csv_path,\n",
    "    min_width_range=None,  # if provided: fixed absolute span (meters). If None: dynamic span.\n",
    "    alpha=0.30,            # fraction for dynamic span (default 30%)\n",
    "    basis='median',        # 'median' (default) or 'mean'\n",
    "    global_stat=False      # if True: use one global statistic for all groups\n",
    "):\n",
    "    \"\"\"\n",
    "    min_width_range:\n",
    "        - If provided (number), use a fixed minimum width span (meters) for all groups (original behavior).\n",
    "        - If None (default), use dynamic span = alpha * {median|mean}(width):\n",
    "            * per group (default), OR\n",
    "            * global across the whole dataset (if global_stat=True).\n",
    "\n",
    "    Prints, for transparency:\n",
    "      - Global median/mean (if global_stat=True)\n",
    "      - Per-group median/mean and the min_width_span_used\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "\n",
    "    # --- compute global statistic if needed ---\n",
    "    if global_stat:\n",
    "        if basis == 'mean':\n",
    "            global_val = float(np.nanmean(df['width']))\n",
    "        else:\n",
    "            global_val = float(np.nanmedian(df['width']))\n",
    "        print(f\"Global {basis} width = {global_val:.2f}\")\n",
    "    else:\n",
    "        global_val = None\n",
    "\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 3:\n",
    "            continue\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "\n",
    "        # compute width statistic used for this group\n",
    "        if global_stat:\n",
    "            group_stat = global_val\n",
    "        else:\n",
    "            if basis == 'mean':\n",
    "                group_stat = float(np.nanmean(X))\n",
    "            else:\n",
    "                group_stat = float(np.nanmedian(X))\n",
    "\n",
    "        # decide dynamic or fixed span\n",
    "        if min_width_range is None:\n",
    "            dyn_span = alpha * group_stat\n",
    "        else:\n",
    "            dyn_span = float(min_width_range)\n",
    "\n",
    "        # print per-group info\n",
    "        print(f\"node_id={node_id}: {basis} width = {group_stat:.2f}, min_width_span_used = {dyn_span:.2f}\")\n",
    "\n",
    "        res = fit_regressions_for_group(\n",
    "            X, y,\n",
    "            min_width_range=dyn_span,\n",
    "            alpha=alpha,\n",
    "            basis=basis\n",
    "        )\n",
    "        result_rows.append({\n",
    "            'node_id': node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_slope2': res['bath_slope2'],\n",
    "            'bath_intercept': res['intercepts'][0],\n",
    "            'r2': res['r2'],\n",
    "            'type': res['type'],\n",
    "            'width_stat': group_stat,\n",
    "            'min_width_span_used': dyn_span\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    fig, axs = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    axs = axs.flatten()\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "        if res['type'] == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.2f}\")\n",
    "        elif res['type'] == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', linewidth=2, label=f\"Left: slope={slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linewidth=2, linestyle='--', label=f\"Right: slope={slope1:.2f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "        else:  # 'piecewise_2bp'\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', linewidth=2, label=f\"Left: {slope3:.2f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linewidth=2, linestyle='--', label=f\"Mid: {slope2:.2f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linewidth=2, linestyle=':', label=f\"Right: {slope1:.2f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR2={res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2w_wse = INTERMEDIATE/\"Store/Binary_Masks/Po/Bath_widths.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1e865",
   "metadata": {},
   "source": [
    "#### 5.1. calling the function and plotting:\n",
    "csv_path = path2w_wse\n",
    "Path to a CSV with at least the columns node_id, width, and wse. The function groups rows by node_id and fits regressions per group.\n",
    "\n",
    "min_width_range = float(\"inf\")\n",
    "Sets a fixed minimum width span each segment must cover to accept a breakpoint. Using ∞ means no finite segment can satisfy it → all breakpoint models are rejected, so every group falls back to the simple (single-slope) Huber regression.\n",
    "\n",
    "global_stat = False\n",
    "If you were using a dynamic threshold (i.e., min_width_range=None), this would decide whether to compute the width statistic globally or per group. Since you set a fixed threshold, this flag is ignored.\n",
    "\n",
    "alpha = 0.20 and basis = 'mean'\n",
    "These only matter when min_width_range=None (dynamic span = alpha * {median|mean}(width)). With a fixed threshold, both are ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, group_results = groupwise_regression(\n",
    "    path2w_wse,\n",
    "    min_width_range=float(\"inf\"),   # disables all break-point models\n",
    "    global_stat=False,\n",
    "    alpha=0.20,                     # ignored now\n",
    "    basis='mean'                    # ignored now\n",
    ")\n",
    "# Optionally, save to CSV\n",
    "result_df.to_csv(INTERMEDIATE/\"Store/Binary_Masks/Po/Bath_simple.csv\", index=False)\n",
    "# Plot 8 random node_id fits\n",
    "plot_random_regressions(group_results, n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_regressions(group_results):\n",
    "    import math\n",
    "\n",
    "    n = len(group_results)\n",
    "    if n == 0:\n",
    "        print(\"No groups to plot.\")\n",
    "        return\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(n / ncols)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    # Ensure axs is always a flat iterable\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        axs = axs.flatten()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (node_id, (group, res)) in enumerate(group_results.items()):\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "\n",
    "        x_plot = np.linspace(X.min(), X.max(), 300)\n",
    "        model_type = res['type']\n",
    "\n",
    "        if model_type == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.4f}\")\n",
    "\n",
    "        elif model_type == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', label=f\"Left: slope={slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linestyle='--', label=f\"Right: slope={slope1:.4f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "\n",
    "        elif model_type == 'piecewise_2bp':\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', label=f\"Left: {slope3:.4f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linestyle='--', label=f\"Mid: {slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linestyle=':', label=f\"Right: {slope1:.4f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR² = {res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any extra unused subplots\n",
    "    total_axes = nrows * ncols\n",
    "    for j in range(n, total_axes):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_regressions(group_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aec3ba",
   "metadata": {},
   "source": [
    "#### * Merge csv file with Bathymetry slopes with the Regression master csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(master_csv, complementary_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Merge two CSV files on 'node_id' and save the result.\n",
    "    \"\"\"\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "    comp_df = pd.read_csv(complementary_csv)\n",
    "\n",
    "    # Merge on 'node_id'\n",
    "    merged_df = master_df.merge(comp_df, on='node_id', how='inner')\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged CSV saved to {output_csv}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "#### Names of files to use\n",
    "# Bath_slopes_simple.csv\n",
    "# Bath_slopes3.csv #Dynamically fitted\n",
    "\n",
    "path2slope = INTERMEDIATE/\"Store/Binary_Masks/Atrato/Less_filters/Bath_simple.csv\"\n",
    "outpath = INTERMEDIATE/\"Store/Validation/csv/Atrato/Less_filters/reg_slope_simple.csv\"\n",
    "# output dynamically fitted slopes: reg_slope_bath3.csv\n",
    "# output simple fitted slopes: reg_slope_simple.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26992158",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath = merge_csv(master_csv_path, path2slope, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort = reg_slope_bath.sort_values('p_dist_out', ascending=False).groupby('node_id', as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier removal in SWOT and bathymetry\n",
    "Q1, Q3 = reg_slope_bath_sort['slope1'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "l1, u1 = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "\n",
    "# bath_slope1 fences\n",
    "Q1b, Q3b = reg_slope_bath_sort['bath_slope1'].quantile([0.25, 0.75])\n",
    "IQRb   = Q3b - Q1b\n",
    "l2, u2 = Q1b - 1.5*IQRb, Q3b + 1.5*IQRb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b444e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (reg_slope_bath_sort['slope1']      >= l1) & (reg_slope_bath_sort['slope1']      <= u1) &\n",
    "    (reg_slope_bath_sort['bath_slope1'] >= l2) & (reg_slope_bath_sort['bath_slope1'] <= u2)\n",
    ")\n",
    "removed_iqr = reg_slope_bath_sort.loc[~mask].copy()\n",
    "removed_iqr['removal_stage'] = 'IQR_fences'\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    _rel = 100.0 * (removed_iqr['slope1'] - removed_iqr['bath_slope1']) / removed_iqr['bath_slope1']\n",
    "removed_iqr['rel_error'] = _rel\n",
    "removed_iqr['abs_rel_error'] = removed_iqr['rel_error'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27523505",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort = reg_slope_bath_sort[mask]\n",
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Work on a copy so you can compare before/after if needed\n",
    "df = reg_slope_bath_sort.copy()\n",
    "\n",
    "# --- 1) Compute rel error (percent) in-place-friendly columns\n",
    "# (safe divide: ignore zero/NaN denominators in the trimming stats)\n",
    "rel = 100 * (df['slope1'] - df['bath_slope1']) / df['bath_slope1']\n",
    "df['rel_error'] = rel\n",
    "df['abs_rel_error'] = rel.abs()\n",
    "\n",
    "# --- 2) Build a UNIQUE view (like your CDF step) for threshold estimation only\n",
    "uniq = df[['slope1','bath_slope1','rel_error']].dropna().drop_duplicates(subset=['slope1','bath_slope1'])\n",
    "\n",
    "# --- 3) MAD right-tail rule (robust, no small-slope dropping)\n",
    "med  = uniq['rel_error'].median()\n",
    "mad0 = (uniq['rel_error'] - med).abs().median()\n",
    "mad  = 1.4826 * mad0                # #1.4826 makes MAD behave like std deviation\n",
    "k    = 3.0                          # tweakable: 2.5–4. tweak: bigger = keep more, smaller = remove more. it’s analogous to saying “flag anything more than 3σ above the median.”\n",
    "thr_mad = med + k * mad\n",
    "\n",
    "mask_out_mad = uniq['rel_error'] > thr_mad\n",
    "\n",
    "# --- 4) Optional percentile tail shave (set q=None to skip)\n",
    "q = 0.99                            # top 1% cutoff; set to None to disable\n",
    "if q is not None:\n",
    "    thr_q = uniq['rel_error'].quantile(q)\n",
    "    mask_out_pct = uniq['rel_error'] > thr_q\n",
    "    mask_out_all = mask_out_mad | mask_out_pct\n",
    "else:\n",
    "    mask_out_all = mask_out_mad\n",
    "\n",
    "# --- 5) Map the unique outliers back to the full df (keep structure)\n",
    "out_pairs = uniq.loc[mask_out_all, ['slope1','bath_slope1']]\n",
    "out_pairs['__key__'] = 1\n",
    "df_key = df[['slope1','bath_slope1']].copy()\n",
    "df_key['__key__'] = 1\n",
    "\n",
    "# mark matches\n",
    "to_drop = df_key.merge(out_pairs, on=['__key__','slope1','bath_slope1'], how='left', indicator=True)\n",
    "is_rel_outlier = (to_drop['_merge'] == 'both').values\n",
    "\n",
    "# (optional) keep a flag and a log before dropping\n",
    "df['is_rel_outlier'] = is_rel_outlier\n",
    "removed = df[df['is_rel_outlier']].copy()\n",
    "kept    = df[~df['is_rel_outlier']].copy()\n",
    "\n",
    "removed = removed.copy()\n",
    "removed['removal_stage'] = 'MAD_or_pct'\n",
    "\n",
    "_deleted_cols = ['node_id', 'slope1', 'slope_bathy', 'rel_error', 'abs_rel_error', 'removal_stage']\n",
    "deleted_nodes = pd.concat(\n",
    "    [\n",
    "        removed_iqr.reindex(columns=_deleted_cols, fill_value=np.nan),\n",
    "        removed.reindex(columns=_deleted_cols, fill_value=np.nan)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "# Keep a single record per node_id (remove this line if you prefer all occurrences)\n",
    "deleted_nodes = deleted_nodes.drop_duplicates(subset=['node_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 6) Replace reg_slope_bath_sort with the cleaned version (same columns as before)\n",
    "# If you want the exact original schema, drop the helper columns:\n",
    "reg_slope_bath_sort = kept.drop(columns=['rel_error','abs_rel_error','is_rel_outlier'])\n",
    "\n",
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9a546",
   "metadata": {},
   "source": [
    "Error metrics and coefficient of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f84571",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = reg_slope_bath_sort['slope1'].values\n",
    "y = reg_slope_bath_sort['bath_slope1'].values\n",
    "\n",
    "# Classic metrics\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "mae = mean_absolute_error(y, x)\n",
    "rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "# Relative errors\n",
    "rel_error = 100 * (x - y) / y\n",
    "mean_bias = np.mean(rel_error)\n",
    "mean_abs_rel_error = np.mean(np.abs(rel_error))\n",
    "percentile_68 = np.percentile(np.abs(rel_error), 68)\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f} (p={pearson_p:.2g})\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f} (p={spearman_p:.2g})\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"Mean Relative Error (Bias): {mean_bias:.2f}%\")\n",
    "print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f}%\")\n",
    "print(f\"68th percentile Absolute Relative Error: {percentile_68:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal deleted nodes: {len(deleted_nodes)}\")\n",
    "print(\"Deleted node_ids:\", deleted_nodes['node_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2789e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Bootstrap correlations + append one river to a CSV (now with 68th percentiles) ======\n",
    "import numpy as np, csv, os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# where to accumulate all rivers\n",
    "OUT_CSV = \"/Volumes/Science_SSD/Dissertation/2_intermediate/Store/Validation/csv/Per_river_stats_accumulated_Vok.csv\"\n",
    "\n",
    "BOOTSTRAP_B = 2000   # resamples per river\n",
    "MIN_SAMPLES = 10     # skip if too few pairs\n",
    "RNG_SEED    = 42\n",
    "\n",
    "def _clean_xy(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    return x[m], y[m]\n",
    "\n",
    "def _abs_rel_error_percent(x, y):\n",
    "    \"\"\"Return (signed) relative error (%) and absolute relative error (%), guarding y==0.\"\"\"\n",
    "    m = (y != 0) & np.isfinite(x) & np.isfinite(y)\n",
    "    rel = np.full_like(x, np.nan, dtype=float)\n",
    "    rel[m] = 100.0 * (x[m] - y[m]) / y[m]   # signed %\n",
    "    abs_rel = np.abs(rel[np.isfinite(rel)])  # magnitude %\n",
    "    return rel, abs_rel\n",
    "\n",
    "def _bootstrap_corrs(x, y, B=1000, seed=None):\n",
    "    \"\"\"Bootstrap distributions of Pearson r and Spearman ρ.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(x)\n",
    "    pr = np.empty(B); sr = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n, endpoint=False)\n",
    "        xb, yb = x[idx], y[idx]\n",
    "        # robust to constant resamples\n",
    "        try:   pr[b] = pearsonr(xb, yb)[0]\n",
    "        except Exception: pr[b] = np.nan\n",
    "        try:   sr[b] = spearmanr(xb, yb)[0]\n",
    "        except Exception: sr[b] = np.nan\n",
    "    pr = pr[np.isfinite(pr)]\n",
    "    sr = sr[np.isfinite(sr)]\n",
    "    return pr, sr\n",
    "\n",
    "def _qstats(a):\n",
    "    if a.size == 0: return np.nan, np.nan, np.nan, np.nan\n",
    "    q1, q2, q3 = np.nanpercentile(a, [25, 50, 75])\n",
    "    return q1, q2, q3, (q3 - q1)\n",
    "\n",
    "def append_river_with_bootstrap(label, x, y,\n",
    "                                out_csv=OUT_CSV,\n",
    "                                B=BOOTSTRAP_B,\n",
    "                                min_samples=MIN_SAMPLES,\n",
    "                                seed=RNG_SEED):\n",
    "    \"\"\"\n",
    "    Compute metrics + bootstrap IQRs of Pearson/Spearman and append one row to CSV.\n",
    "    Also stores:\n",
    "      - bias_mean_% (mean signed relative error, %)\n",
    "      - rel_p68_% (68th percentile of signed relative error, %)\n",
    "      - abs_rel_p68_% (68th percentile of absolute relative error, %)\n",
    "      - abs_rel_q1_%, abs_rel_q3_%, abs_rel_iqr_% (magnitude)\n",
    "    \"\"\"\n",
    "    x, y = _clean_xy(x, y)\n",
    "    n = len(x)\n",
    "    if n < min_samples:\n",
    "        print(f\"⚠️  Skipped '{label}' (n={n} < {min_samples})\")\n",
    "        return\n",
    "\n",
    "    # point estimates (originals)\n",
    "    pearson_r, pearson_p     = pearsonr(x, y)\n",
    "    spearman_rho, spearman_p = spearmanr(x, y)\n",
    "    mae  = mean_absolute_error(y, x)\n",
    "    rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "    # relative errors\n",
    "    rel, abs_rel = _abs_rel_error_percent(x, y)\n",
    "    bias_mean   = np.nanmean(rel) if np.any(np.isfinite(rel)) else np.nan          # %\n",
    "    mare        = np.nanmean(abs_rel) if abs_rel.size else np.nan                 # %\n",
    "    abs_q1      = np.nanpercentile(abs_rel, 25) if abs_rel.size else np.nan\n",
    "    abs_q3      = np.nanpercentile(abs_rel, 75) if abs_rel.size else np.nan\n",
    "    abs_iqr     = (abs_q3 - abs_q1) if abs_rel.size else np.nan\n",
    "    # 68th percentiles\n",
    "    rel_p68     = np.nanpercentile(rel, 68) if np.any(np.isfinite(rel)) else np.nan        # signed %\n",
    "    abs_rel_p68 = np.nanpercentile(abs_rel, 68) if abs_rel.size else np.nan                # magnitude %\n",
    "\n",
    "    # bootstrap coefficient variability\n",
    "    pr_dist, sr_dist = _bootstrap_corrs(x, y, B=B, seed=seed)\n",
    "    p_q1, p_med, p_q3, p_iqr = _qstats(pr_dist)\n",
    "    s_q1, s_med, s_q3, s_iqr = _qstats(sr_dist)\n",
    "\n",
    "    print(f\"River: {label} (n={n})\")\n",
    "    print(f\"  Pearson r = {pearson_r:.3f}  | Bootstrap Q1/med/Q3 = {p_q1:.3f}/{p_med:.3f}/{p_q3:.3f}\")\n",
    "    print(f\"  Spearman ρ = {spearman_rho:.3f} | Bootstrap Q1/med/Q3 = {s_q1:.3f}/{s_med:.3f}/{s_q3:.3f}\")\n",
    "    print(f\"  MAE={mae:.4f}  RMSE={rmse:.4f}  Bias={bias_mean:.2f}%\")\n",
    "    print(f\"  |Rel error|: MARE={mare:.2f}%, Q1={abs_q1:.2f}%, Q3={abs_q3:.2f}%, IQR={abs_iqr:.2f}%\")\n",
    "    print(f\"  Rel 68th=% {rel_p68:.2f}   |Rel| 68th=% {abs_rel_p68:.2f}\")\n",
    "\n",
    "    row = {\n",
    "        \"river\": str(label), \"n\": int(n),\n",
    "        \"pearson_r\": pearson_r, \"pearson_p\": pearson_p,\n",
    "        \"spearman_rho\": spearman_rho, \"spearman_p\": spearman_p,\n",
    "\n",
    "        # bootstrap whiskers for coefficients (to plot)\n",
    "        \"pearson_q1\": p_q1, \"pearson_q2\": p_med, \"pearson_q3\": p_q3, \"pearson_iqr\": p_iqr,\n",
    "        \"spearman_q1\": s_q1, \"spearman_q2\": s_med, \"spearman_q3\": s_q3, \"spearman_iqr\": s_iqr,\n",
    "\n",
    "        # error stats (percent)\n",
    "        \"mae\": mae, \"rmse\": rmse,\n",
    "        \"bias_mean_%\": bias_mean,           # signed mean bias (%)\n",
    "        \"rel_p68_%\": rel_p68,               # 68th percentile of signed relative error (%)\n",
    "        \"abs_rel_p68_%\": abs_rel_p68,       # 68th percentile of absolute relative error (%)\n",
    "        \"mare_%\": mare,                     # mean absolute relative error (%)\n",
    "        \"abs_rel_q1_%\": abs_q1, \"abs_rel_q3_%\": abs_q3, \"abs_rel_iqr_%\": abs_iqr,\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(out_csv)\n",
    "    with open(out_csv, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "    print(f\"✔ Appended → {out_csv}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your arrays:\n",
    "x = reg_slope_bath_sort['slope1'].values\n",
    "y = reg_slope_bath_sort['bath_slope1'].values  # <- your bath column\n",
    "\n",
    "# Give this run a label (river name/id)\n",
    "append_river_with_bootstrap(\"Atrato\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cbf64",
   "metadata": {},
   "source": [
    "#### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error = 100 * (reg_slope_bath_sort['slope1'] - reg_slope_bath_sort['slope_bathy']) / reg_slope_bath_sort['slope_bathy']\n",
    "abs_rel_error = np.abs(rel_error)\n",
    "\n",
    "# Drop duplicates for (slope1, bath_slope1) pairs (optional but matches your logic)\n",
    "df_rel = pd.DataFrame({\n",
    "    'slope1': reg_slope_bath_sort['slope1'],\n",
    "    'slope_bathy': reg_slope_bath_sort['slope_bathy'],\n",
    "    'rel_error': rel_error,\n",
    "    'abs_rel_error': abs_rel_error\n",
    "})\n",
    "df_rel_unique = df_rel.drop_duplicates(subset=['slope1', 'slope_bathy'], keep='first')\n",
    "\n",
    "# Sorted arrays for CDFs\n",
    "bias_sorted = np.sort(df_rel_unique['rel_error'].values)\n",
    "cdf_bias = np.arange(1, len(bias_sorted) + 1) / len(bias_sorted)\n",
    "\n",
    "abs_sorted = np.sort(df_rel_unique['abs_rel_error'].values)\n",
    "cdf_abs = np.arange(1, len(abs_sorted) + 1) / len(abs_sorted)\n",
    "\n",
    "# Metrics\n",
    "mean_bias = df_rel_unique['rel_error'].mean()\n",
    "mean_abs_rel_error = df_rel_unique['abs_rel_error'].mean()\n",
    "spearman_rho, spearman_p = spearmanr(df_rel_unique['slope1'], df_rel_unique['slope_bathy'])\n",
    "p68 = np.percentile(df_rel_unique['abs_rel_error'], 68)\n",
    "\n",
    "# Colors (customize as desired)\n",
    "color_bias = '#ff5722'\n",
    "color_abs = '#fbc02d'\n",
    "color_p68 = 'dimgrey'\n",
    "color_zero = '#ffa726'\n",
    "color_mean = '#2979ff'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Plotting ---\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "\n",
    "# Plot Relative Error CDF\n",
    "ax.scatter(\n",
    "    bias_sorted, cdf_bias,\n",
    "    color=color_bias,\n",
    "    label='Relative Error CDF (bias, %)',\n",
    "    s=70,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='o',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Plot Absolute Relative Error CDF\n",
    "ax.scatter(\n",
    "    abs_sorted, cdf_abs,\n",
    "    color=color_abs,\n",
    "    label='Absolute Relative Error CDF (%)',\n",
    "    s=25,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='D',\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Zero-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=0,\n",
    "    color=color_zero,\n",
    "    linestyle='--',\n",
    "    linewidth=1.8,\n",
    "    label='Zero Bias'\n",
    ")\n",
    "\n",
    "# Mean-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=mean_bias,\n",
    "    color=color_mean,\n",
    "    linestyle='-.',\n",
    "    linewidth=2,\n",
    "    label=f'Mean Bias = {mean_bias:.2f} %'\n",
    ")\n",
    "\n",
    "# 68th-percentile vertical/horizontal lines\n",
    "ax.axvline(\n",
    "    x=p68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5,\n",
    "    label=f'|68%ile|: {p68:.2f} %'\n",
    ")\n",
    "ax.axhline(\n",
    "    y=0.68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# Axis limits and ticks\n",
    "xmin = np.floor(min(bias_sorted.min(), abs_sorted.min())) - 1\n",
    "xmax = np.ceil(max(bias_sorted.max(), abs_sorted.max(), p68)) + 1\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_xticks(np.arange(xmin, xmax + 1, 20))\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Relative Error (%)', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=14)\n",
    "ax.set_title(\n",
    "    'CDF of Slope Relative Error, Mean Bias and 68%tile\\n',\n",
    "    fontsize=16,\n",
    "    weight='bold'\n",
    ")\n",
    "\n",
    "# Grid and legend\n",
    "ax.grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=12, loc='lower right', frameon=True, fancybox=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Marginal KDE plot for bias (inset at top) ---\n",
    "# ------------------------------------------------------------------------------\n",
    "ax_kde = inset_axes(\n",
    "    ax,\n",
    "    width=\"100%\", height=\"20%\",\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0, 1.05, 1, 0.3),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=df_rel_unique['rel_error'],\n",
    "    ax=ax_kde,\n",
    "    fill=True,\n",
    "    linewidth=2,\n",
    "    alpha=0.3\n",
    ")\n",
    "ax_kde.set_xlim(xmin, xmax)\n",
    "ax_kde.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print metrics summary ---\n",
    "print(f\"Spearman correlation: {spearman_rho:.3f} (p={spearman_p:.2g})\")\n",
    "print(f\"Mean Bias (rel error): {mean_bias:.2f} %\")\n",
    "print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f} %\")\n",
    "print(f\"68th percentile abs rel error: {p68:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['slope_bathy']\n",
    "x  = reg_slope_bath_sort['p_dist_out']\n",
    "\n",
    "# ─── figure setup ───────────────────────────────────────────────\n",
    "fig = plt.figure(figsize=(15,6), facecolor='white')\n",
    "gs  = GridSpec(1, 2, width_ratios=[5, 1.5], wspace=0.15)\n",
    "\n",
    "# vibrant palette\n",
    "c1, c2 = '#1f77b4', '#ff7f0e'\n",
    "\n",
    "# ─── left: line vs. distance with thinner lines ────────────────\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "\n",
    "ax_main.plot(\n",
    "    x, s1,\n",
    "    color=c1,\n",
    "    linestyle='-',\n",
    "    linewidth=1.5,          # ← thinner line\n",
    "    marker='o',\n",
    "    markersize=5,           # ← small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c1,\n",
    "    markeredgewidth=1.0,\n",
    "    label='SWOT slope'\n",
    ")\n",
    "ax_main.plot(\n",
    "    x, s2,\n",
    "    color=c2,\n",
    "    linestyle='--',\n",
    "    linewidth=1.5,          # ← thinner line\n",
    "    marker='D',\n",
    "    markersize=5,           # ← small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c2,\n",
    "    markeredgewidth=1.0,\n",
    "    label='Bathymetry slope'\n",
    ")\n",
    "\n",
    "# gentle fill under curves\n",
    "ax_main.fill_between(x, s1, alpha=0.1, color=c1)\n",
    "ax_main.fill_between(x, s2, alpha=0.1, color=c2)\n",
    "\n",
    "ax_main.set_xlabel('Distance to outlet (m)', fontsize=12, weight='bold')\n",
    "ax_main.set_ylabel('Slope value', fontsize=12, weight='bold')\n",
    "ax_main.set_title('Comparison between SWOT and Bathymetry Slopes', fontsize=16, weight='bold')\n",
    "ax_main.grid(alpha=0.3, linestyle='--')\n",
    "ax_main.invert_xaxis()\n",
    "ax_main.legend(loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "# ─── right: side boxplot sharing the same y-axis ───────────────\n",
    "ax_box = fig.add_subplot(gs[1], sharey=ax_main)\n",
    "\n",
    "bp = ax_box.boxplot(\n",
    "    [s1, s2],\n",
    "    positions=[1,2],\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    showfliers=False,\n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='black'),\n",
    "    capprops=dict(color='black'),\n",
    ")\n",
    "\n",
    "# color the boxes with semi-transparent fill\n",
    "for patch, color in zip(bp['boxes'], [c1, c2]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.3)\n",
    "\n",
    "# overlay the raw points on top\n",
    "for i, (y, color) in enumerate(zip([s1, s2], [c1, c2]), start=1):\n",
    "    xi = np.random.normal(i, 0.04, size=len(y))\n",
    "    ax_box.scatter(\n",
    "        xi, y,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        s=12,\n",
    "        edgecolors='none',\n",
    "        zorder=10\n",
    "    )\n",
    "\n",
    "ax_box.set_xticks([1,2])\n",
    "ax_box.set_xticklabels(['SWOT', 'Bathymetry'], fontsize=10, weight='bold')\n",
    "ax_box.margins(x=0.3)\n",
    "ax_box.yaxis.tick_right()\n",
    "ax_box.yaxis.set_label_position(\"right\")\n",
    "ax_box.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6817c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['slope_bathy']\n",
    "\n",
    "# set up the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(s1, s2, \n",
    "            color='red',      # match your SWOT color\n",
    "            alpha=0.7, \n",
    "            edgecolors='none', \n",
    "            s=30)\n",
    "\n",
    "# 1:1 line\n",
    "lims = [\n",
    "    np.min([s1.min(), s2.min()]), \n",
    "    np.max([s1.max(), s2.max()])\n",
    "]\n",
    "plt.plot(lims, lims, \n",
    "        ls='--', \n",
    "        lw=1.5, \n",
    "        color='green', \n",
    "        label='1:1 line')\n",
    "\n",
    "margin = 0.001\n",
    "low, high = lims\n",
    "\n",
    "plt.xlim(low - margin, high + margin)\n",
    "plt.ylim(low - margin, high + margin)\n",
    "\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('SWOT vs. Bathymetry Slopes', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']           # SWOT slope\n",
    "s2 = reg_slope_bath_sort['slope_bathy']      # Bathymetry slope\n",
    "\n",
    "# 1) Hexbin + 1:1 line\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.hexbin(s1, s2, gridsize=100, mincnt=1, cmap='Blues')  # no color specified → default cmap\n",
    "lims = [min(s1.min(), s2.min()), max(s1.max(), s2.max())]\n",
    "plt.plot(lims, lims, ls='--', lw=1.5, color='green', label='1:1 line')\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('Hexbin: SWOT vs Bath Slopes', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 2) Bland–Altman (Difference vs Average)\n",
    "diff = s1 - s2\n",
    "avg  = 0.5 * (s1 + s2)\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(avg, diff, s=20, alpha=0.5)\n",
    "plt.axhline(mean_diff, color='red', linestyle='--', label='Mean diff')\n",
    "plt.axhline(mean_diff + 1.96*std_diff, color='gray', linestyle=':', label='±1.96σ')\n",
    "plt.axhline(mean_diff - 1.96*std_diff, color='gray', linestyle=':')\n",
    "plt.xlabel('Average slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Difference (SWOT – Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Bland–Altman: Difference vs. Average', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 3) Residuals vs SWOT slope\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(s1, diff, s=20, alpha=0.5)\n",
    "plt.axhline(0, color='black', linestyle='--', lw=1)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Residual (SWOT – Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Residuals vs. SWOT Slope', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 4) CDF of Absolute Residuals (|SWOT – Bath|)\n",
    "abs_diff = np.abs(diff)\n",
    "abs_diff = abs_diff[np.isfinite(abs_diff)]\n",
    "xs = np.sort(abs_diff)\n",
    "cdf = np.arange(1, len(xs) + 1) / len(xs)\n",
    "\n",
    "p50 = np.percentile(abs_diff, 50)\n",
    "p68 = np.percentile(abs_diff, 68)\n",
    "p95 = np.percentile(abs_diff, 95)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, cdf, lw=2)\n",
    "plt.axvline(p68, linestyle='--', label=f'68%tile = {p68:.4f}')\n",
    "plt.axvline(p50, linestyle=':',  label=f'Median = {p50:.4f}')\n",
    "plt.axvline(p95, linestyle=':',  label=f'95%tile = {p95:.4f}')\n",
    "plt.xlabel('|Residual| (|SWOT – Bath|)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Cumulative Probability', fontsize=12, weight='bold')\n",
    "plt.title('CDF of Absolute Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "# (Alternative) Histogram of signed residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=30, density=True, alpha=0.7)\n",
    "plt.axvline(mean_diff, linestyle='--', label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT – Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Density', fontsize=12, weight='bold')\n",
    "plt.title('Residual Distribution', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Histogram of Residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(0, color='black', linestyle='--', lw=1, label='Zero line')\n",
    "plt.axvline(mean_diff, color='red', linestyle='--', lw=1, label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT – Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Count', fontsize=12, weight='bold')\n",
    "plt.title('Histogram of Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Clean residuals\n",
    "diff_clean = np.asarray(diff[np.isfinite(diff)])\n",
    "\n",
    "# Stats\n",
    "q1, med, q3 = np.percentile(diff_clean, [25, 50, 75])\n",
    "mean_val    = diff_clean.mean()\n",
    "p68         = np.percentile(diff_clean, 68)\n",
    "\n",
    "# Keep comparable limits\n",
    "ymin = np.percentile(diff_clean, 1)\n",
    "ymax = np.percentile(diff_clean, 99)\n",
    "pad  = 0.02 * (ymax - ymin)\n",
    "ylim = (ymin - pad, ymax + pad)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# Violin plot with same proportional look\n",
    "ax = sns.violinplot(\n",
    "    y=diff_clean,\n",
    "    inner='box',\n",
    "    color='lightblue',\n",
    "    cut=0,\n",
    "    scale='width',\n",
    "    bw=0.2,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "# Markers for stats\n",
    "ax.scatter(0, mean_val, s=60, marker='D', color='#d62728', zorder=5, label=f\"Mean = {mean_val:.4f}\")\n",
    "ax.scatter(0, med,      s=60, marker='o', facecolors='white', edgecolors='black', zorder=5, label=f\"Median = {med:.4f}\")\n",
    "ax.scatter(0, q1,       s=80, marker='_', color='#2ca02c', zorder=5, label=f\"Q1 = {q1:.4f}\")\n",
    "ax.scatter(0, q3,       s=80, marker='_', color='#1f77b4', zorder=5, label=f\"Q3 = {q3:.4f}\")\n",
    "ax.axhline(p68, color='purple', linestyle=':', lw=1.2, label=f\"68%tile = {p68:.4f}\")\n",
    "\n",
    "# Labels, limits, legend\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_ylabel('Residual (SWOT - Bath)', fontsize=12, weight='bold')\n",
    "ax.set_title('Residual Distribution Around Zero', fontsize=14, weight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.legend(loc='lower left', fontsize=9, frameon=True)  # legend in lower-left\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ce331",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from matplotlib.lines import Line2D\n",
    "# -------------------------\n",
    "# Data\n",
    "# -------------------------\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Pee Dee up', 'Pee Dee down', 'Po', 'Garonne'],\n",
    "    'r_simple':  [0.31, 0.61, 0.44, -0.11, -0.25, 0.37, 0.39, 0.24],\n",
    "    'ρ_simple':  [0.42, 0.62, 0.49, -0.25, -0.15, 0.63, 0.47, 0.34],\n",
    "    #'r_20':      [0.35, 0.49, 0.56, 0.55, np.nan, np.nan, 0.34, 0.11],\n",
    "    #'ρ_20':      [0.44, 0.65, 0.57, 0.49, np.nan, np.nan, 0.36, 0.19],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# -------------------------\n",
    "# Control which approaches to include\n",
    "# -------------------------\n",
    "approaches_to_plot = [\"simple\", \"20\"]\n",
    "\n",
    "# -------------------------\n",
    "# Reshape into long format\n",
    "# -------------------------\n",
    "suffixes = []\n",
    "for c in df.columns:\n",
    "    if c.startswith(\"r_\"):\n",
    "        suf = c[2:]\n",
    "        if f\"ρ_{suf}\" in df.columns and suf in approaches_to_plot:\n",
    "            suffixes.append(suf)\n",
    "\n",
    "long_rows = []\n",
    "for s in suffixes:\n",
    "    tmp = pd.DataFrame({\n",
    "        'River': df['River'],\n",
    "        'Approach': s.replace('avrg', 'average'),\n",
    "        'r': df[f'r_{s}'],\n",
    "        'ρ': df[f'ρ_{s}'],\n",
    "    })\n",
    "    long_rows.append(tmp)\n",
    "\n",
    "df_long = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# Assign unique color+marker per river\n",
    "# -------------------------\n",
    "colors = cycle(plt.cm.tab10.colors)   # use tab10 palette cycling\n",
    "markers = cycle(['o', 's', '^', 'D', 'P', 'X', 'v', '<', '>'])\n",
    "\n",
    "rivers = df['River'].unique()\n",
    "river_styles = {}\n",
    "for river, color, marker in zip(rivers, colors, markers):\n",
    "    river_styles[river] = {\"color\": color, \"marker\": marker}\n",
    "\n",
    "# -------------------------\n",
    "# Plot\n",
    "# -------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "for _, row in df_long.iterrows():\n",
    "    style = river_styles[row['River']]\n",
    "    ax.scatter(\n",
    "        row['r'],\n",
    "        row['ρ'],\n",
    "        color=style[\"color\"],\n",
    "        marker=style[\"marker\"],\n",
    "        s=100,\n",
    "        edgecolor='black',\n",
    "        linewidth=0.7\n",
    "    )\n",
    "\n",
    "# Axes, style\n",
    "ax.set_xlabel('r', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('ρ', fontsize=12, fontweight='bold')\n",
    "ax.set_title('r vs ρ by River', fontsize=14, fontweight='bold')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlim(-0.5, 1.05)\n",
    "ax.set_ylim(-0.5, 1.05)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Legend (only rivers)\n",
    "river_handles = [\n",
    "    Line2D([0], [0], marker=style[\"marker\"], color='w',\n",
    "           label=river, markerfacecolor=style[\"color\"],\n",
    "           markeredgecolor='black', markersize=10)\n",
    "    for river, style in river_styles.items()\n",
    "]\n",
    "ax.legend(handles=river_handles, title=\"Rivers\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08988ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "threshold = 0.40\n",
    "outline_color = 'limegreen'\n",
    "dash_on, dash_off = 1.5, 2.0  # dotted look\n",
    "lw = 2\n",
    "\n",
    "# =========================\n",
    "# Data\n",
    "# =========================\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Po', 'Garonne'],\n",
    "    'r_40':  [0.44, 0.62, 0.64, 0.54, 0.21, 0.18],\n",
    "    'ρ_40':  [0.42, 0.65, 0.67, 0.52, 0.30, 0.08],\n",
    "    'r_80':  [0.41, 0.83, 0.45, 0.1, 0.34, 0.24],\n",
    "    'ρ_80':  [0.37, 0.70, 0.51, -0.08, 0.36, 0.06],\n",
    "}\n",
    "df = pd.DataFrame(data).set_index('River')\n",
    "df = df[['r_40', 'ρ_40', 'r_80', 'ρ_80']]\n",
    "\n",
    "# =========================\n",
    "# Heatmap\n",
    "# =========================\n",
    "vals = df.values\n",
    "vmax = np.nanmax(np.abs(vals))\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.8, 5.0))\n",
    "im = ax.imshow(vals, aspect='auto', cmap='RdBu_r', norm=norm, origin='upper')\n",
    "\n",
    "ax.set_xticks(np.arange(df.shape[1]))\n",
    "ax.set_xticklabels(df.columns, fontsize=11, fontweight='bold')\n",
    "ax.set_yticks(np.arange(df.shape[0]))\n",
    "ax.set_yticklabels(df.index, fontsize=11)\n",
    "\n",
    "# annotate values\n",
    "for i in range(vals.shape[0]):\n",
    "    for j in range(vals.shape[1]):\n",
    "        ax.text(j, i, f\"{vals[i, j]:.2f}\",\n",
    "                ha='center', va='center', fontsize=9,\n",
    "                color='black' if abs(vals[i, j]) < vmax*0.55 else 'white')\n",
    "\n",
    "ax.set_title('Correlation Heatmap (centered at 0)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('metric@Segment Width', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('River', fontsize=11, fontweight='bold')\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Correlation', fontsize=11, fontweight='bold')\n",
    "\n",
    "# =========================\n",
    "# Boxy perimeter for r > threshold OR ρ > threshold\n",
    "# =========================\n",
    "mask = vals > threshold  # includes both Pearson and Spearman\n",
    "\n",
    "rows, cols = mask.shape\n",
    "edges = set()\n",
    "\n",
    "def add_edge(p1, p2):\n",
    "    edges.add((p1, p2) if p1 < p2 else (p2, p1))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if not mask[i, j]:\n",
    "            continue\n",
    "        x0, x1 = j - 0.5, j + 0.5\n",
    "        y0, y1 = i - 0.5, i + 0.5\n",
    "        if i == 0 or not mask[i-1, j]:      # top\n",
    "            add_edge((x0, y0), (x1, y0))\n",
    "        if i == rows-1 or not mask[i+1, j]: # bottom\n",
    "            add_edge((x0, y1), (x1, y1))\n",
    "        if j == 0 or not mask[i, j-1]:      # left\n",
    "            add_edge((x0, y0), (x0, y1))\n",
    "        if j == cols-1 or not mask[i, j+1]: # right\n",
    "            add_edge((x1, y0), (x1, y1))\n",
    "\n",
    "segments = [list(edge) for edge in edges]\n",
    "\n",
    "if segments:\n",
    "    lc = LineCollection(\n",
    "        segments,\n",
    "        colors=outline_color,\n",
    "        linewidths=3,\n",
    "        linestyles=(0, (dash_on, dash_off)),\n",
    "        capstyle='butt',\n",
    "        joinstyle='miter',\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "# keep full pixel grid visible\n",
    "ax.set_xlim(-0.5, cols - 0.5)\n",
    "ax.set_ylim(rows - 0.5, -0.5)\n",
    "\n",
    "# =========================\n",
    "# Legend outside the axes\n",
    "# =========================\n",
    "outline_handle = Line2D([0], [0], color=outline_color, lw=lw, linestyle=(0, (dash_on, dash_off)))\n",
    "fig.legend([outline_handle], [f'Pearson or Spearman > {threshold:.2f}'],\n",
    "           loc='lower right', bbox_to_anchor=(0.98, -0.05), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68919f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Data (keep all approaches; lengths may differ)\n",
    "# -------------------------\n",
    "data = {\n",
    "    'River':  ['Cape Fear','Atrato','Sacramento','Pee Dee', 'Pee Dee up', 'Pee Dee down', 'Po','Garonne'],\n",
    "    'bias_simple':[-44.4, 20.4, -16.3, -11.9, 18.7, -72.9, 12.0, -80.0],\n",
    "    'p68_simple': [75.6, 51.3,  53.8,  76.5, 74.4, 77.3, 49.0,  88.7],\n",
    "    #'bias_avrg': [-54.6, 19.6, -26.8, -26.2,  13.7, -81.2],          # shorter\n",
    "    #'p68_avrg':  [78.4, 47.9,  41.5,  70.6,  58.5,  89.2],           # shorter\n",
    "    #'bias_20':   [-71.0, 54.3, -26.7, -38.6, 0.12, -83.1, np.nan, np.nan],\n",
    "    #'p68_20':    [81.9, 95.1,  69.5,  56.0,  59.9, 89.6, np.nan, np.nan],\n",
    "}\n",
    "\n",
    "# Build base df and safely align any shorter columns\n",
    "df = pd.DataFrame({'River': data['River']})\n",
    "for k, v in data.items():\n",
    "    if k == 'River': \n",
    "        continue\n",
    "    s = pd.Series(v)\n",
    "    df[k] = s.reindex(range(len(df))).values  # pad with NaN to match length\n",
    "\n",
    "# -------------------------\n",
    "# Choose which approaches to include (ignore 'avrg')\n",
    "# -------------------------\n",
    "INCLUDE = ['simple', '20']   # <- change here to toggle approaches\n",
    "# INCLUDE = ['simple']       # e.g., only simple\n",
    "# INCLUDE = ['simple', 'avrg', '20']  # to include all\n",
    "\n",
    "# -------------------------\n",
    "# Auto-detect suffixes, then filter by INCLUDE\n",
    "# -------------------------\n",
    "suffixes = []\n",
    "for c in df.columns:\n",
    "    if c.startswith('bias_'):\n",
    "        suf = c[5:]\n",
    "        if f'p68_{suf}' in df.columns:\n",
    "            suffixes.append(suf)\n",
    "\n",
    "if INCLUDE is not None:\n",
    "    suffixes = [s for s in suffixes if s in INCLUDE]\n",
    "\n",
    "# -------------------------\n",
    "# Long format for plotting\n",
    "# -------------------------\n",
    "long_rows = []\n",
    "for s in suffixes:\n",
    "    tmp = pd.DataFrame({\n",
    "        'River': df['River'],\n",
    "        'Approach': s.replace('avrg', 'average'),\n",
    "        'Bias': df[f'bias_{s}'],\n",
    "        'P68':  df[f'p68_{s}'],\n",
    "    })\n",
    "    long_rows.append(tmp)\n",
    "\n",
    "long = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "# Drop rows that are all-NaN for the selected approach\n",
    "long = long.dropna(subset=['Bias', 'P68'], how='all')\n",
    "\n",
    "# -------------------------\n",
    "# Assign unique color + marker per river (no approach legend)\n",
    "# -------------------------\n",
    "rivers = df['River'].unique()\n",
    "colors = cycle(plt.cm.tab10.colors)\n",
    "markers = cycle(['o','s','^','D','P','X','v','<','>'])\n",
    "\n",
    "river_styles = {}\n",
    "for river, color, marker in zip(rivers, colors, markers):\n",
    "    river_styles[river] = {'color': color, 'marker': marker}\n",
    "\n",
    "# -------------------------\n",
    "# Plot\n",
    "# -------------------------\n",
    "fig, ax = plt.subplots(figsize=(8.6, 6.4))\n",
    "\n",
    "for _, row in long.iterrows():\n",
    "    st = river_styles[row['River']]\n",
    "    ax.scatter(\n",
    "        row['Bias'], row['P68'],\n",
    "        s=120, alpha=0.9,\n",
    "        color=st['color'],\n",
    "        marker=st['marker'],\n",
    "        edgecolor='black', linewidth=0.7\n",
    "    )\n",
    "\n",
    "# Axes & style\n",
    "ax.axvline(0, linestyle='--', linewidth=0.8, color='gray')\n",
    "ax.set_xlabel('Bias (%)', fontweight='bold')\n",
    "ax.set_ylabel('68th Percentile Error (%)', fontweight='bold')\n",
    "ax.set_title('Bias vs 68%ile Error — by River', fontweight='bold')\n",
    "ax.grid(True, linestyle=':', alpha=0.35)\n",
    "\n",
    "# Legend: only rivers\n",
    "river_handles = [\n",
    "    Line2D([0], [0], marker=st['marker'], color='w', label=river,\n",
    "           markerfacecolor=st['color'], markeredgecolor='black', markersize=10)\n",
    "    for river, st in river_styles.items()\n",
    "]\n",
    "ax.legend(handles=river_handles, title='Rivers',\n",
    "          bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304275ab",
   "metadata": {},
   "source": [
    "## Width variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e40ed6",
   "metadata": {},
   "source": [
    "Testing how strongly width variabilty affects errors, bias and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48386aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swot = pd.read_csv(swot_simple)\n",
    "to_exclude = deleted_nodes['node_id'].unique().tolist()\n",
    "df_swot = df_swot[~df_swot['node_id'].isin(to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a538f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bath = pd.read_csv(path2w_wse)\n",
    "to_exclude_bath = deleted_nodes['node_id'].unique().tolist()\n",
    "df_bath = df_bath[~df_bath['node_id'].isin(to_exclude_bath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefvar(dframe):\n",
    "    cv_width = dframe.groupby('node_id')['width'].agg(lambda x: x.std()/x.mean())\n",
    "    return cv_width.reset_index().rename(columns={\"width\": \"CV_width\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = coefvar(df_bath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a658d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv = cv['CV_width'].mean()\n",
    "mean_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rivers ={\n",
    "    'River':  [\"Cape Fear\", \"Atrato\", \"Sacramento\", \"Pee Dee\",\"Pee Dee up\", \"Pee Dee down\", \"Po\", \"Garonne\"],\n",
    "    'cv_Bath_group':      [0.16, 0.42, 0.30, 0.29, 0.09, 0.95, 0.20, 0.07],\n",
    "    #'cv_Bath_all':        [0.33, 0.55, 0.39, 0.42, 0.31, 0.11],\n",
    "    'cv_swot_group':      [0.22, 0.26, 0.20, 0.42, 0.09, 0.87, 0.17, 0.27],\n",
    "    #'cv_swot_all':      [0.33, 0.37, 0.35, 0.86, 0.31, 0.33],\n",
    "    'ρ_simple':       [0.42, 0.62, 0.49, -0.25, -0.15, 0.63, 0.47, 0.34],\n",
    "    #'ρ_avrg':       [0.31, 0.74, 0.67, 0.06,0.39, 0.23],\n",
    "    #'ρ_20%':       [0.44, 0.54, 0.57, 0.49, 0.36, 0.19],\n",
    "    #'ρ_23%':       [0.44, 0.60 ,0.53, 0.33, 0.38, 0.19],\n",
    "    #'ρ_25%':       [0.44, 0.59, 0.57, 0.29, 0.39, 0.19],\n",
    "    #'ρ_30%':       [0.44, 0.63, 0.69, 0.17, 0.39,0.19],\n",
    "    'bias(%) simple': [-44.4, 20.4, -16.3, -12.6, 18.7, -72.9, 12.0, -80.0],\n",
    "    #'bias(%) avrg': [-54.6, 19.6, -26.8, -26.2, 13.7, -81.2],\n",
    "    #'bias(%) 20%': [-71.0, 54.3, -26.7, -34.9, 0.12, -83.11],\n",
    "    #'bias(%) 23%': [-70.95, 32.19, -31.52, -25.26, 1.77, -82.62],\n",
    "    #'bias(%) 25%': [-70.95, 43.42, -30.00, -13.78, 1.60, -82.62],\n",
    "    #'bias(%) 30%': [-70.95, 27.68, -28.89, -3.96, 14.07, -82.62],\n",
    "    '68\"%\"ile simple': [75.6, 51.3, 53.8, 76.8, 74.4, 77.3, 49.0, 88.7],\n",
    "    #'68\"%\"ile avrg': [78.4, 47.9, 41.5, 70.6, 58.5, 89.2],\n",
    "    #'68\"%\"ile 20%': [81.9, 95.1, 69.5, 61.0, 60.0, 89.6],\n",
    "    #'68\"%\"ile 23%': [81.91, 73.08, 58.52, 58.14, 56.04, 89.63],\n",
    "    #'68\"%\"ile 25%': [81.91, 91.57, 56.98, 73.22, 52.98, 89.63],\n",
    "    #'68\"%\"ile 30%': [81.91, 81.39, 53.38, 76.25, 58.63, 89.63],\n",
    "}\n",
    "\n",
    "summary_cv = pd.DataFrame(CV_rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "import yaml\n",
    "import re, fnmatch\n",
    "\n",
    "# --- column normalization (accept your original headers) ---\n",
    "RENAME_MAP = {\n",
    "    'ρ_simple': 'rho_simple',\n",
    "    'ρ_20%': 'rho_20pct',\n",
    "    'bias(%) simple': 'bias_simple_pct',\n",
    "    'bias(%) 20%': 'bias_20_pct',\n",
    "    '68\"%\"ile simple': 'p68_simple_pct',\n",
    "    '68\"%\"ile 20%': 'p68_20_pct',\n",
    "}\n",
    "\n",
    "def load_data(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    # keep everything; just sanity-check key CV columns exist\n",
    "    expected_any = {'cv_swot_group'}\n",
    "    missing = [c for c in expected_any if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "def load_config(yaml_path: str|None=None, fallback: dict|None=None) -> dict:\n",
    "    if yaml_path:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "    else:\n",
    "        cfg = fallback or {}\n",
    "    # Defaults (unchanged behavior unless you set them in YAML)\n",
    "    cfg.setdefault('x_col', 'cv_swot_group')\n",
    "    cfg.setdefault('y_cols', ['rho_simple','rho_20pct'])\n",
    "    cfg.setdefault('percent_axis', True)\n",
    "    cfg.setdefault('label_offset', {})\n",
    "    cfg.setdefault('x_max', 1.8)\n",
    "    cfg.setdefault('y_mode', 'auto')                     # 'auto' | 'percent' | 'fraction'\n",
    "    cfg.setdefault('fraction_as_percent_labels', False)\n",
    "    # NEW: optional filter for plotting only specific rivers (keeps the rest of the data)\n",
    "    cfg.setdefault('include_rivers', None)               # e.g., ['Pee Dee up','Pee Dee down']\n",
    "    return cfg\n",
    "\n",
    "def expand_y_cols(df: pd.DataFrame, patterns) -> list[str]:\n",
    "    cols = set()\n",
    "    for pat in patterns:\n",
    "        if isinstance(pat, str) and pat.startswith('/') and pat.endswith('/'):\n",
    "            rx = re.compile(pat[1:-1])\n",
    "            cols |= {c for c in df.columns if rx.search(c)}\n",
    "        else:\n",
    "            cols |= set(fnmatch.filter(df.columns, pat))\n",
    "    return [c for c in df.columns if c in cols]\n",
    "\n",
    "def is_bias(col: str) -> bool:\n",
    "    return 'bias' in col\n",
    "\n",
    "def _classify(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Return one of: 'bias_pct' (−100..100), 'p68_pct' (0..100), 'fraction' (−0.3..1)\n",
    "    \"\"\"\n",
    "    if col.startswith('bias_') and col.endswith('_pct'):\n",
    "        return 'bias_pct'\n",
    "    if col.startswith('p68_') and col.endswith('_pct'):\n",
    "        return 'p68_pct'\n",
    "    if col.startswith('rho_'):\n",
    "        return 'fraction'\n",
    "    return 'bias_pct' if (is_bias(col) or col.endswith('_pct')) else 'fraction'\n",
    "\n",
    "def plot_panels(df: pd.DataFrame, cfg: dict):\n",
    "    X_COL = cfg['x_col']\n",
    "    y_cols = expand_y_cols(df, cfg['y_cols'])\n",
    "    PERCENT = bool(cfg['percent_axis'])\n",
    "    LABEL_OFFSET = cfg['label_offset']\n",
    "    x_max = float(cfg.get('x_max', 1.8))\n",
    "    Y_MODE = cfg.get('y_mode', 'auto')\n",
    "    FRAC_AS_PCT = bool(cfg.get('fraction_as_percent_labels', False))\n",
    "    INCLUDE_RIVERS = cfg.get('include_rivers', None)\n",
    "\n",
    "    # --- NEW: filter only for plotting (does not modify the original df) ---\n",
    "    df_plot = df if not INCLUDE_RIVERS else df[df['River'].isin(INCLUDE_RIVERS)].copy()\n",
    "    if df_plot.empty:\n",
    "        raise ValueError(\"No rows to plot after applying include_rivers filter.\")\n",
    "    for c in [X_COL, 'River', *y_cols]:\n",
    "        if c not in df_plot.columns:\n",
    "            raise KeyError(f\"Column '{c}' not found after normalization.\")\n",
    "\n",
    "    # prepare figure with up to 2 panels (as your current code)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, col in zip(axes, y_cols):\n",
    "        x = df_plot[X_COL]\n",
    "        y = df_plot[col]\n",
    "\n",
    "        ax.scatter(x, y, s=100, zorder=2)\n",
    "\n",
    "        # River labels\n",
    "        for xi, yi, name in zip(x, y, df_plot['River']):\n",
    "            dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "            ax.annotate(\n",
    "                name, xy=(xi, yi), xytext=(dx, dy), textcoords='offset points',\n",
    "                ha='left' if dx >= 0 else 'right', va='center', fontsize=12,\n",
    "                arrowprops=dict(arrowstyle='-', lw=0.7, alpha=0.7),\n",
    "                zorder=0, clip_on=False\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(0, x_max)\n",
    "\n",
    "        # ---- Y axis behavior (unchanged) ----\n",
    "        if Y_MODE == 'percent':\n",
    "            ax.set_ylim(-100, 100)\n",
    "            ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "            ax.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
    "\n",
    "        elif Y_MODE == 'fraction':\n",
    "            ax.set_ylim(-0.3, 1.0)\n",
    "            ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "            if FRAC_AS_PCT:\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0))\n",
    "            else:\n",
    "                ax.yaxis.set_major_formatter('{x:.2f}')\n",
    "\n",
    "        else:\n",
    "            kind = _classify(col)\n",
    "            if kind == 'bias_pct':\n",
    "                ax.set_ylim(-100, 100)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=100) if PERCENT else '{x:.0f}')\n",
    "            elif kind == 'p68_pct':\n",
    "                ax.set_ylim(0, 100)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(10))\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=100) if PERCENT else '{x:.0f}')\n",
    "            else:\n",
    "                ax.set_ylim(-0.3, 1.0)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "                if FRAC_AS_PCT or PERCENT:\n",
    "                    ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0))\n",
    "                else:\n",
    "                    ax.yaxis.set_major_formatter('{x:.2f}')\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.set_xlabel('CV', fontsize=14)\n",
    "        ax.set_ylabel(col, fontsize=14)\n",
    "\n",
    "        ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "        ax.minorticks_on()\n",
    "        ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "        # Stats (on visible points)\n",
    "        try:\n",
    "            r_s, _ = spearmanr(x, y)\n",
    "            r_p, _ = pearsonr(x, y)\n",
    "            ax.text(0.95, 0.05, f\"ρ = {r_s:.2f}\",\n",
    "                    transform=ax.transAxes, ha='right', va='bottom',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "            ax.text(0.95, 0.05, f\"r = {r_p:.2f}\",\n",
    "                    transform=ax.transAxes, ha='right', va='top',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Hide unused panels if fewer y_cols\n",
    "    for ax in axes[len(y_cols):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rivers = load_data(INTERMEDIATE/\"Store/Figures/cv_rivers.csv\")\n",
    "cfg = load_config(INTERMEDIATE/\"STORE/Figures/plot_config.yaml\")  # or load_config(fallback={\"y_cols\":[\"rho_*\"]})\n",
    "plot_panels(df_rivers, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'River':  [\"Cape Fear\", \"Atrato\", \"Sacramento\", \"Pee Dee Down\", \"Po\", \"Garonne\"],\n",
    "    'cv_swot_group':      [0.22, 0.26, 0.20, 0.87, 0.17, 0.27],\n",
    "    'cv_Bath_group':      [0.16, 0.42, 0.30, 0.95, 0.20, 0.07],\n",
    "    'cv_swot_all':      [0.33, 0.37, 0.35, 0.86, 0.31, 0.33],\n",
    "    'cv_Bath_all':        [0.33, 0.55, 0.39, 0.42, 0.31, 0.11]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Scatter plot ---\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(df['cv_swot_group'], df['cv_Bath_group'], s=100, zorder=2)\n",
    "\n",
    "# Add river names as labels\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(row['cv_swot_group'] + 0.01,  # small x-offset\n",
    "            row['cv_Bath_group'] + 0.01,  # use Bath_group to match y-axis\n",
    "            row['River'],\n",
    "            fontsize=10, va='center', ha='left')\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"CV SWOT group\", fontsize=14)\n",
    "ax.set_ylabel(\"CV Bath group\", fontsize=14)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Correlation stats (between same x and y you plotted)\n",
    "rho_s, p_s = spearmanr(df['cv_swot_group'], df['cv_Bath_group'])\n",
    "rho_p, p_p = pearsonr(df['cv_swot_group'], df['cv_Bath_group'])\n",
    "\n",
    "ax.text(0.95, 0.05, f\"ρ = {rho_s:.2f}\\nr = {rho_p:.2f}\",\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "x = summary_cv['cv_Bath_group']\n",
    "y = summary_cv['cv_swot_group']\n",
    "\n",
    "# Scatter\n",
    "ax.scatter(x, y, s=100, zorder=2)\n",
    "\n",
    "# River labels\n",
    "for xi, yi, name in zip(x, y, summary_cv['River']):\n",
    "    dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "    ax.annotate(\n",
    "        name, xy=(xi, yi), xytext=(dx, dy),\n",
    "        textcoords='offset points',\n",
    "        ha='left' if dx >= 0 else 'right',\n",
    "        va='center', fontsize=12,\n",
    "        arrowprops=dict(arrowstyle='-', lw=0.7, alpha=0.7),\n",
    "        zorder=0, clip_on=False\n",
    "    )\n",
    "\n",
    "# Axes\n",
    "ax.set_xlim(0, 0.5)\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.set_xlabel(\"CV Bathymetry Group\", fontsize=14)\n",
    "ax.set_ylabel(\"CV SWOT Group\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "ax.minorticks_on()\n",
    "ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "# Correlation between them\n",
    "r, _ = spearmanr(x, y)\n",
    "ax.text(0.95, 0.05, f\"ρ = {r:.2f}\",\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Column selection & label offsets\n",
    "# -----------------------------\n",
    "bias_cols = summary_cv.filter(regex=r'^bias\\(%\\)\\s*\\d+%$').columns.tolist()\n",
    "\n",
    "LABEL_OFFSET = {\n",
    "    \"Cape Fear\": (-10, 10),\n",
    "    \"Po\":        ( 10, 10),\n",
    "    \"Sacramento\":( 10, 10),\n",
    "    \"Atrato\":    ( 10, 10),\n",
    "    \"Pee Dee\":   (-10, 10),\n",
    "    \"Garonne\":   ( 10, -4),\n",
    "}\n",
    "\n",
    "# X series to compare (name, legend label, marker)\n",
    "x_cols = [\n",
    "    ('cv_swot_all',   'SWOT all',   'o'),\n",
    "    ('cv_swot_group', 'SWOT group', 's'),\n",
    "    ('cv_Bath_all',   'Bath all',   '^'),\n",
    "    ('cv_Bath_group', 'Bath group', 'D'),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, bias_cols):\n",
    "    y = summary_cv[col]\n",
    "\n",
    "    ann_lines = []  # lines for correlation textbox\n",
    "\n",
    "    # Plot each CV series\n",
    "    for j, (xname, label, marker) in enumerate(x_cols):\n",
    "        x = summary_cv[xname]\n",
    "        ax.scatter(x, y, s=90, marker=marker, label=label, zorder=2)\n",
    "\n",
    "        # Correlations for this series\n",
    "        rs, _ = spearmanr(x, y)\n",
    "        rp, _ = pearsonr(x, y)\n",
    "        ann_lines.append(f\"{label}: ρ={rs:.2f}, r={rp:.2f}\")\n",
    "\n",
    "        # Add river labels only once (first series) to avoid 4x duplicates\n",
    "        if j == 0:\n",
    "            for xi, yi, name in zip(x, y, summary_cv['River']):\n",
    "                dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "                ax.annotate(\n",
    "                    name, xy=(xi, yi), xytext=(dx, dy),\n",
    "                    textcoords='offset points',\n",
    "                    ha='left' if dx >= 0 else 'right',\n",
    "                    va='center', fontsize=10,\n",
    "                    arrowprops=dict(arrowstyle='-', lw=0.6, alpha=0.6),\n",
    "                    zorder=0, clip_on=False\n",
    "                )\n",
    "\n",
    "    # Axes styling\n",
    "    ax.set_xlim(0, 1.2)  # up to 1.18 in cv_swot_all\n",
    "    ax.set_ylim(-100, 100)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(xmax=100, decimals=0))  # show -100%..100%\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_xlabel('CV', fontsize=14)\n",
    "    ax.set_ylabel(col, fontsize=14)\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "    # Correlation textbox (bottom-right)\n",
    "    ax.text(\n",
    "        0.98, 0.02, \"\\n\".join(ann_lines),\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=9, fontweight='bold',\n",
    "        zorder=10, clip_on=False,\n",
    "        bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.6)\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc='upper left', fontsize=9, frameon=True, framealpha=0.8)\n",
    "\n",
    "# Hide unused panels if any (not needed here but safe)\n",
    "for ax in axes[len(bias_cols):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcef84",
   "metadata": {},
   "source": [
    "### What about XTRK_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = pd.read_csv(swot_simple)\n",
    "simple = simple.dropna(subset=['node_id', 'xtrk_dist', 'spearman_corr']).copy()\n",
    "simple['xtrk_dist'] = pd.to_numeric(simple['xtrk_dist'], errors='coerce')\n",
    "simple['spearman_corr'] = pd.to_numeric(simple['spearman_corr'], errors='coerce')\n",
    "simple = simple.dropna(subset=['xtrk_dist', 'spearman_corr'])\n",
    "\n",
    "# --- One row per node_id: mean xtrk_dist + first spearman_corr (by current row order) ---\n",
    "agg = (\n",
    "    simple\n",
    "    .groupby('node_id', as_index=False)\n",
    "    .agg(\n",
    "        xtrk_dist_mean=('xtrk_dist', 'mean'),\n",
    "        spearman_corr_first=('spearman_corr', 'first')\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Scatter plot (points only) ---\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(agg['xtrk_dist_mean'], agg['spearman_corr_first'], s=60, alpha=0.7, edgecolor='k')\n",
    "\n",
    "plt.xlabel(\"Mean Cross-track Distance by node_id\", fontsize=12)\n",
    "plt.ylabel(\"Spearman Correlation (first per node_id)\", fontsize=12)\n",
    "plt.title(\"Spearman Correlation vs Mean Cross-track Distance\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_xtrk(dframe):\n",
    "    mean_dist = dframe.groupby('node_id')['xtrk_dist'].mean()\n",
    "    return mean_dist.reset_index().rename(columns={\"xtrk_dist\": \"mean_xtrk_dist\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025fc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrk = mean_xtrk(simple)\n",
    "xtrk['mean_xtrk_dist'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04540af",
   "metadata": {},
   "source": [
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_river_grid(df, river_order=None, suptitle=\"r vs ρ across rivers\"):\n",
    "    required = {\"river\", \"setting\", \"r\", \"rho\", \"r_p\", \"rho_p\"}\n",
    "    if missing := required - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in pd.unique(df[\"river\"]) if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Color map per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global axis limits\n",
    "    xmin = min(0.0, df[\"r\"].min()) - 0.02\n",
    "    xmax = max(1.0, df[\"r\"].max()) + 0.02\n",
    "    ymin = min(0.0, df[\"rho\"].min()) - 0.02\n",
    "    ymax = max(1.0, df[\"rho\"].max()) + 0.02\n",
    "\n",
    "    # Grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            for _, row in chunk.iterrows():\n",
    "                nonsig = (row[\"r_p\"] > 0.05) or (row[\"rho_p\"] > 0.05)\n",
    "                marker = \"x\" if nonsig else \"o\"\n",
    "                kwargs = dict(s=70, marker=marker)\n",
    "                if nonsig:\n",
    "                    kwargs.update(color=col, linewidth=1.0, alpha=0.95)\n",
    "                else:\n",
    "                    kwargs.update(facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95)\n",
    "                ax.scatter(row[\"r\"], row[\"rho\"], **kwargs)\n",
    "\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=0.6, alpha=0.4)\n",
    "        ax.axhline(0, lw=0.6, alpha=0.4)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"ρ (Spearman)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"r (Pearson)\")\n",
    "\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Legends outside\n",
    "    settings_legend = fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Segment width\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(5, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)   # OUTSIDE bottom\n",
    "    )\n",
    "\n",
    "    signif_handles = [\n",
    "        Line2D([], [], marker='o', linestyle='None',\n",
    "               markerfacecolor='white', markeredgecolor='black',\n",
    "               markersize=7, markeredgewidth=0.8, label='p ≤ 0.05 (significant)'),\n",
    "        Line2D([], [], marker='x', linestyle='None',\n",
    "               color='black', markersize=7, label='p > 0.05 (not significant)')\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=signif_handles,\n",
    "        loc=\"center left\",\n",
    "        frameon=True,\n",
    "        title=\"Significance\",\n",
    "        bbox_to_anchor=(1.01, 0.5)   # OUTSIDE right\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.9, 0.95])  # leave room\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"r\":0.44,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.05},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"r\":0.41,\"r_p\":0.06,\"rho\":0.37,\"rho_p\":0.09},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.02,\"rho\":0.47,\"rho_p\":0.04},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"r\":0.44,\"r_p\":0.06,\"rho\":0.38,\"rho_p\":0.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"r\":0.62,\"r_p\":2.5e-4,\"rho\":0.65,\"rho_p\":9.3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"r\":0.83,\"r_p\":4.3e-8,\"rho\":0.70,\"rho_p\":3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"r\":0.35,\"r_p\":0.07,\"rho\":0.56,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"r\":0.60,\"r_p\":0.002,\"rho\":0.59,\"rho_p\":0.002},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"r\":0.49,\"r_p\":0.009,\"rho\":0.54,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"23%\",\"r\":0.51,\"r_p\":0.007,\"rho\":0.60,\"rho_p\":0.001},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"r\":0.46,\"r_p\":0.013,\"rho\":0.59,\"rho_p\":0.001},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"r\":0.55,\"r_p\":0.003,\"rho\":0.63,\"rho_p\":4.7e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"r\":0.64,\"r_p\":4.6e-6,\"rho\":0.67,\"rho_p\":1e-6},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"r\":0.45,\"r_p\":0.003,\"rho\":0.51,\"rho_p\":5e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"r\":0.37,\"r_p\":0.02,\"rho\":0.35,\"rho_p\":0.02},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"r\":0.46,\"r_p\":0.002,\"rho\":0.44,\"rho_p\":0.004},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"r\":0.56,\"r_p\":1.2e-4,\"rho\":0.57,\"rho_p\":7.1e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"r\":0.54,\"r_p\":2.6e-4,\"rho\":0.53,\"rho_p\":4.2e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"r\":0.58,\"r_p\":4e-5,\"rho\":0.57,\"rho_p\":7.5e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"r\":0.65,\"r_p\":2.5e-6,\"rho\":0.69,\"rho_p\":3.8e-7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"r\":0.54,\"r_p\":1.7e-4,\"rho\":0.52,\"rho_p\":3.4e-4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"r\":0.10,\"r_p\":0.52,\"rho\":-0.08,\"rho_p\":0.59},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"r\":0.63,\"r_p\":3.3e-6,\"rho\":0.7,\"rho_p\":9.7e-8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"r\":0.69,\"r_p\":4.1e-7,\"rho\":0.63,\"rho_p\":6.6e-6},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"r\":0.65,\"r_p\":2.4e-6,\"rho\":0.57,\"rho_p\":6.8e-5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"r\":0.41,\"r_p\":0.006,\"rho\":0.33,\"rho_p\":0.03},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"r\":0.32,\"r_p\":0.03,\"rho\":0.29,\"rho_p\":0.056},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"r\":0.14,\"r_p\":0.4,\"rho\":0.17,\"rho_p\":0.25},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"r\":0.21,\"r_p\":0.004,\"rho\":0.3,\"rho_p\":2.8e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"r\":0.34,\"r_p\":1.6e-6,\"rho\":0.36,\"rho_p\":3.6e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"r\":0.2,\"r_p\":0.005,\"rho\":0.29,\"rho_p\":4e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"r\":0.28,\"r_p\":6.4e-5,\"rho\":0.32,\"rho_p\":4.8e-6},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"r\":0.34,\"r_p\":1.3e-6,\"rho\":0.36,\"rho_p\":3.2e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"r\":0.36,\"r_p\":5e-7,\"rho\":0.38,\"rho_p\":9.4e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"r\":0.36,\"r_p\":2.7e-7,\"rho\":0.39,\"rho_p\":2.3e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"r\":0.34,\"r_p\":2.1e-6,\"rho\":0.39,\"rho_p\":3.6e-8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"r\":0.18,\"r_p\":0.46,\"rho\":0.08,\"rho_p\":0.74},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"r\":0.24,\"r_p\":0.34,\"rho\":0.06,\"rho_p\":0.81},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"r\":0.4,\"r_p\":0.14,\"rho\":0.41,\"rho_p\":0.12},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.16,\"rho\":0.26,\"rho_p\":0.32},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"r\":0.26,\"r_p\":0.3,\"rho\":0.23,\"rho_p\":0.37},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"r\":0.26,\"r_p\":0.31,\"rho\":0.16,\"rho_p\":0.54},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"r\":0.18,\"r_p\":0.47,\"rho\":0.09,\"rho_p\":0.73},\n",
    "      \n",
    "])\n",
    "plot_river_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_p68_grid(\n",
    "    df,\n",
    "    river_order=None,\n",
    "    suptitle=\"Bias vs 68%ile Error across rivers\",\n",
    "    bias_col=\"bias_pct\",     # x-axis (e.g., -78.3 means -78.3%)\n",
    "    p68_col=\"p68_pct\",       # y-axis (e.g., 88.0 means 88%)\n",
    "):\n",
    "    \"\"\"\n",
    "    Faceted 2x3 scatter: one subplot per river with shared axes.\n",
    "    Expects tidy df columns: river, setting, bias_col, p68_col.\n",
    "\n",
    "    - Color encodes 'setting'\n",
    "    - Vertical dashed line at Bias = 0\n",
    "    - Legends placed OUTSIDE the grid (no overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    required = {\"river\", \"setting\", bias_col, p68_col}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # Stable river order\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in rivers if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Palette per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global limits (include 0 on x for reference)\n",
    "    xmin = min(0.0, df[bias_col].min()) - 2\n",
    "    xmax = max(0.0, df[bias_col].max()) + 2\n",
    "    ymin = max(0.0, min(df[p68_col].min(), 0)) - 2\n",
    "    ymax = df[p68_col].max() + 2\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # One proxy per setting for legend\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            ax.scatter(\n",
    "                chunk[bias_col], chunk[p68_col],\n",
    "                s=70, marker=\"o\", facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95\n",
    "            )\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        # Cosmetics\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=1.0, alpha=0.5, linestyle=\"--\")  # Bias = 0 reference\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Label only left/bottom axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"68th Percentile Error (%)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"Bias (%)\")\n",
    "\n",
    "    # Hide unused axes if <6 rivers\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Settings legend OUTSIDE bottom\n",
    "    fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Setting\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(6, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.98, 0.95])  # room for legend & title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"bias_pct\":-77.2,\"p68_pct\": 87.7},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"bias_pct\":-78.3,\"p68_pct\": 88.0},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"bias_pct\":-66.5,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"bias_pct\":-61.6,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"bias_pct\":53.4,\"p68_pct\": 99.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"bias_pct\":19.1,\"p68_pct\": 51.3},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"bias_pct\":64.9,\"p68_pct\": 103.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"bias_pct\":59.5,\"p68_pct\": 99.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"bias_pct\":54.3,\"p68_pct\": 95.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"23%\",\"bias_pct\":32.2,\"p68_pct\": 73.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"bias_pct\":43.4,\"p68_pct\": 91.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"bias_pct\":27.7,\"p68_pct\": 81.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"bias_pct\":-29.9,\"p68_pct\": 55.9},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"bias_pct\":-18.6,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"bias_pct\":-22.4,\"p68_pct\": 66.13},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"bias_pct\":-23.4,\"p68_pct\": 67.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"bias_pct\":-26.7,\"p68_pct\": 69.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"bias_pct\":-31.5,\"p68_pct\": 58.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"bias_pct\":-30.0,\"p68_pct\": 57.0},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"bias_pct\":-28.9,\"p68_pct\": 53.4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"bias_pct\":-41.4,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"bias_pct\":-16.3,\"p68_pct\": 77.7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"bias_pct\":-58.5,\"p68_pct\": 71.8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"bias_pct\":-44.8,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"bias_pct\":-38.6,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"bias_pct\":-25.3,\"p68_pct\": 58.1},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"bias_pct\":-13.8,\"p68_pct\": 73.2},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"bias_pct\":-3.96,\"p68_pct\": 76.6},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"bias_pct\":-13.6,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"bias_pct\":-4.14,\"p68_pct\": 56.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"bias_pct\":-10.9,\"p68_pct\": 67.7},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"bias_pct\":-3.23,\"p68_pct\": 61.8},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"bias_pct\":0.12,\"p68_pct\": 59.9},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"bias_pct\":1.8,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"bias_pct\":1.6,\"p68_pct\": 53.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"bias_pct\":14.1,\"p68_pct\": 58.6},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"bias_pct\":-77.9,\"p68_pct\": 86.2},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"bias_pct\":-78.6,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"bias_pct\":-85.1,\"p68_pct\": 89.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"bias_pct\":-84.2,\"p68_pct\": 87.4},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"bias_pct\":-82.0,\"p68_pct\": 86.8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"bias_pct\":-80.5,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"bias_pct\":-79.9,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"bias_pct\":-78.1,\"p68_pct\": 86.2},\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_p68_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd166f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "df = pd.DataFrame([\n",
    "    (\"Atrato\",              0.61, 0.62),\n",
    "    (\"Garonne\",             0.24, 0.34),\n",
    "    (\"Po\",                  0.39, 0.47),\n",
    "    (\"Sacramento\",          0.44, 0.49),\n",
    "    (\"Cape Fear\",           0.31, 0.42),\n",
    "    (\"Pee Dee River\",      -0.11, -0.25),\n",
    "    (\"Pee Dee Upstream\",   -0.25, -0.15),\n",
    "    (\"Pee Dee Downstream\",  0.37, 0.63),\n",
    "], columns=[\"River\", \"Pearson\", \"Spearman\"])\n",
    "\n",
    "# ---------------- Style ----------------\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"#F3F5F7\",\n",
    "    \"axes.edgecolor\": \"0.75\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 13.5,\n",
    "    \"ytick.labelsize\": 13.5,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.color\": \"white\",\n",
    "    \"grid.linewidth\": 1.6,\n",
    "})\n",
    "\n",
    "# Viridis colors (color-blind-friendly)\n",
    "cmap = plt.colormaps.get_cmap(\"viridis\")\n",
    "color_map = {\n",
    "    \"Pearson\":  cmap(0.25),\n",
    "    \"Spearman\": cmap(0.75),\n",
    "}\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(13, 7.2))\n",
    "\n",
    "rivers  = df[\"River\"].tolist()\n",
    "metrics = [\"Pearson\", \"Spearman\"]\n",
    "bar_w   = 0.36\n",
    "x       = np.arange(len(rivers))\n",
    "offsets = [-bar_w/2, bar_w/2]\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax.bar(x + offsets[i], df[m].values, width=bar_w,\n",
    "           label=m, color=color_map[m], edgecolor=\"none\")\n",
    "\n",
    "ax.axhline(0, color=\"0.35\", linewidth=1.2, zorder=0)   # zero line\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rivers, rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Correlation coefficient\")\n",
    "\n",
    "ymin, ymax = df[[\"Pearson\",\"Spearman\"]].min().min(), df[[\"Pearson\",\"Spearman\"]].max().max()\n",
    "pad  = max(0.05, 0.08 * (ymax - ymin or 1))\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "ax.legend(frameon=True, facecolor=\"white\", edgecolor=\"#E0E0E0\",\n",
    "          loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.82)\n",
    "\n",
    "# ---------------- EXPORT ----------------\n",
    "fig.savefig(\"/Users/daniel/Documents/UNC/Presentations/SWOT25/Posterriver_correlations_poster.png\",\n",
    "            dpi=600, bbox_inches=\"tight\", facecolor=\"none\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Saved: river_correlations_poster.png at 600 dpi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "df = pd.DataFrame([\n",
    "    (\"Atrato\",               20.4,  51.3),   # Bias, 68%ile error\n",
    "    (\"Garonne\",             -80.0,  88.7),\n",
    "    (\"Po\",                   12.0,  49.0),\n",
    "    (\"Sacramento\",          -16.3,  53.8),\n",
    "    (\"Cape Fear\",           -44.4,  75.6),\n",
    "    (\"Pee Dee River\",       -11.9,  76.5),\n",
    "    (\"Pee Dee Upstream\",     18.7,  74.4),\n",
    "    (\"Pee Dee Downstream\",  -72.9,  77.3),\n",
    "], columns=[\"River\", \"Bias\", \"Error68\"])\n",
    "\n",
    "# ---------------- Style ----------------\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"#F3F5F7\",\n",
    "    \"axes.edgecolor\": \"0.75\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 13.5,\n",
    "    \"ytick.labelsize\": 13.5,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.color\": \"white\",\n",
    "    \"grid.linewidth\": 1.6,\n",
    "})\n",
    "\n",
    "# ----------- New color-blind-safe palette (Okabe–Ito) -----------\n",
    "c_bias     = \"#D55E00\"   # warm Vermillion\n",
    "c_error68  = \"#009E73\"   # cool Blue-green\n",
    "color_map  = {\"Bias\": c_bias, \"Error68\": c_error68}\n",
    "\n",
    "# ---------------- Tick-step helper ----------------\n",
    "def nice_step(ymin, ymax, max_ticks=12):\n",
    "    span = max(1e-9, ymax - ymin)\n",
    "    for s in [1, 2, 5, 10, 20, 25, 50]:\n",
    "        if math.ceil(span / s) + 1 <= max_ticks:\n",
    "            return s\n",
    "    return span / max_ticks\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(13, 7.2))\n",
    "\n",
    "rivers  = df[\"River\"].tolist()\n",
    "metrics = [\"Bias\", \"Error68\"]\n",
    "bar_w   = 0.36\n",
    "x       = np.arange(len(rivers))\n",
    "offsets = [-bar_w/2, bar_w/2]\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax.bar(x + offsets[i], df[m].values,\n",
    "           width=bar_w, label=m, color=color_map[m], edgecolor=\"none\")\n",
    "\n",
    "ax.axhline(0, color=\"0.35\", linewidth=1.2, zorder=0)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rivers, rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Bias / 68%ile error (%)\")\n",
    "\n",
    "# --- Y limits & tick spacing ---\n",
    "ymin = float(df[[\"Bias\",\"Error68\"]].min().min())\n",
    "ymax = float(df[[\"Bias\",\"Error68\"]].max().max())\n",
    "pad  = max(1.0, 0.08 * (ymax - ymin if ymax != ymin else 1.0))\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "step = nice_step(*ax.get_ylim(), max_ticks=12)\n",
    "ax.yaxis.set_major_locator(MultipleLocator(step))\n",
    "\n",
    "# Clean look\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Legend\n",
    "ax.legend(frameon=True, facecolor=\"white\", edgecolor=\"#E0E0E0\",\n",
    "          loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.82)\n",
    "\n",
    "# ---------------- EXPORT ----------------\n",
    "fig.savefig(\"river_bias_error68_poster_altpalette.png\",\n",
    "            dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Saved: river_bias_error68_poster_altpalette.png at 600 dpi\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a915bdc",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85550f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, mapping, LineString, MultiLineString, shape\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import rasterio\n",
    "from rasterio.mask import mask as rio_mask\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.features import shapes\n",
    "from rasterio.warp import transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyproj import Transformer\n",
    "import glob\n",
    "from shapely.ops import unary_union\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import random\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import seaborn as sns\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import math\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from config_paths import DATA, OUTPUT, INTERMEDIATE\n",
    "from ast import literal_eval\n",
    "import fnmatch, yaml \n",
    "from matplotlib.ticker import PercentFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3866c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8ad655",
   "metadata": {},
   "source": [
    "This code uses bathymetry models, together with SWOT data and and cross-section shapefiles, to create raster maks of WSE. Intercepting cross-sections with the masks river widths are estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa77d0f2",
   "metadata": {},
   "source": [
    "#### 0. PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d7ea551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressions\n",
    "regress_path = pd.read_csv(\"/Users/daniel/Downloads/B77.csv\")#/dark_fr_030/8_bits/S3_15Obs_100W/jul_3_25/Po_River_huber_reg_3.csv\")) #No_Norm/dark_fr_030/8_bits/S3_15Obs_100W/jun_6_25/\n",
    "                                        #\"Po_River_huber_reg.csv\"))\n",
    "poly_path = gpd.read_file(INTERMEDIATE/\"Store/Binary_Masks/Sacramento/Shps/Sacramento_cx.shp\")\n",
    "tif_path = DATA/\"External/Bathymetries/namerica/Sacramento_River/Sacramento_EGM08.tif\"\n",
    "base_folder = INTERMEDIATE/\"Store/Binary_Masks/Sacramento/\"\n",
    "#swot_simple = INTERMEDIATE/\"Store/Validation/csv/Atrato/Atrato_reg_bottom_error_simple.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b871c76",
   "metadata": {},
   "source": [
    "#### 1. Subsetting only the nodes lying inside the raster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d505ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping 2698321 rows with invalid lon/lat\n",
      "22575 observations inside raster, 6319170 total observations\n"
     ]
    }
   ],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    nod = src.nodata\n",
    "    \n",
    "    xs = regress_path[\"lon\"].values\n",
    "    ys = regress_path[\"lat\"].values\n",
    "\n",
    "    # 1) Keep only finite coords in a reasonable lat/lon range\n",
    "    valid_input = (\n",
    "        np.isfinite(xs) & np.isfinite(ys) &\n",
    "        (xs >= -180) & (xs <= 180) &\n",
    "        (ys >= -90) & (ys <= 90)\n",
    "    )\n",
    "    if not np.all(valid_input):\n",
    "        print(f\"Dropping {np.sum(~valid_input)} rows with invalid lon/lat\")\n",
    "    \n",
    "    xs_valid = xs[valid_input]\n",
    "    ys_valid = ys[valid_input]\n",
    "\n",
    "    # 2) Transform only valid coords (if raster is not already in EPSG:4326)\n",
    "    if src.crs is None:\n",
    "        raise ValueError(\"Raster has no CRS defined (src.crs is None).\")\n",
    "    \n",
    "    if str(src.crs).upper() in (\"EPSG:4326\", \"EPSG:4326.0\"):\n",
    "        # already geographic\n",
    "        xs_r = xs_valid\n",
    "        ys_r = ys_valid\n",
    "    else:\n",
    "        xs_r, ys_r = transform(\n",
    "            \"EPSG:4326\",       # source = your lon/lat\n",
    "            src.crs,           # dest   = raster CRS (likely UTM)\n",
    "            xs_valid.tolist(),\n",
    "            ys_valid.tolist(),\n",
    "        )\n",
    "\n",
    "    xs_r = np.array(xs_r)\n",
    "    ys_r = np.array(ys_r)\n",
    "\n",
    "    # 3) Now check bounds in raster CRS\n",
    "    left, bottom, right, top = src.bounds\n",
    "    inside_bounds_valid = (\n",
    "        (xs_r >= left) & (xs_r <= right) &\n",
    "        (ys_r >= bottom) & (ys_r <= top)\n",
    "    )\n",
    "\n",
    "    # 4) Prepare full-size array for sampled values (align with original df)\n",
    "    sampled = np.full(xs.shape, np.nan)\n",
    "\n",
    "    # indices in the *original* array that were valid and inside bounds\n",
    "    idx_global_valid = np.where(valid_input)[0]\n",
    "    idx_inside = idx_global_valid[inside_bounds_valid]\n",
    "\n",
    "    xs_in = xs_r[inside_bounds_valid]\n",
    "    ys_in = ys_r[inside_bounds_valid]\n",
    "\n",
    "    if len(xs_in) > 0:\n",
    "        vals = list(src.sample(zip(xs_in, ys_in)))\n",
    "        vals = np.array([v[0] for v in vals])\n",
    "        sampled[idx_inside] = vals\n",
    "\n",
    "    # 5) Mask: valid input, inside bounds, not nodata, not NaN\n",
    "    mask = valid_input & (~np.isnan(sampled)) & (\n",
    "        (nod is None) | (sampled != nod)\n",
    "    )\n",
    "\n",
    "# subset and export\n",
    "regress_path_inside = regress_path[mask]\n",
    "print(f\"{regress_path_inside.shape[0]} observations inside raster, {regress_path.shape[0]} total observations\")\n",
    "\n",
    "regress_path_inside.to_csv(\n",
    "    \"/Users/daniel/Downloads/Sacramento_validate_nodes.csv\",\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde52126",
   "metadata": {},
   "source": [
    "#### 2. Extract from bathymetry the bottom of the river channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d127e809",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(tif_path) as src:\n",
    "    # 1) show me the raster‚Äôs CRS\n",
    "    print(\"Raster CRS is:\", src.crs)\n",
    "\n",
    "    # 2) if your poly_path is still EPSG:4326, move it into the raster‚Äôs CRS\n",
    "    if poly_path.crs != src.crs:\n",
    "        poly_path = poly_path.to_crs(src.crs)\n",
    "\n",
    "    results = []\n",
    "    for _, row in poly_path.iterrows():\n",
    "        node = row[\"node_id\"]\n",
    "        geom = [mapping(row.geometry)]\n",
    "        out_image, _ = rio_mask(\n",
    "            src,\n",
    "            geom,\n",
    "            crop=True,         # keeps memory use tiny\n",
    "            all_touched=True,\n",
    "            nodata=src.nodata,\n",
    "            filled=False\n",
    "        )\n",
    "        arr = out_image[0].astype(\"float32\")\n",
    "        arr = np.ma.masked_equal(arr, src.nodata)\n",
    "        arr = np.ma.masked_where(np.isnan(arr), arr)\n",
    "        min_val = arr.min() if arr.count() > 0 else np.nan\n",
    "        results.append({\"node_id\": node, \"min_value\": float(min_val)})\n",
    "\n",
    "lprb = pd.DataFrame(results)\n",
    "lprb.to_csv(\n",
    "    os.path.join(\n",
    "        INTERMEDIATE,\n",
    "        \"Store/Validation/csv/x_track_corr/Sacramento/Sacramento_channel_min_values.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd9587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lprb = pd.read_csv(INTERMEDIATE/\"Store/Validation/csv/Pee_dee/Dow_Pee_dee_reg_bottom_error.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0cc401",
   "metadata": {},
   "source": [
    "Merge the two csv files, the one with the regression values and the one with the minimum values in the river bed, and then  \n",
    "estimate the difference and errors between the intercepts and the real bathymetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77007700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure node_id is string in both DataFrames for a valid merge\n",
    "regress_path[\"node_id\"] = regress_path[\"node_id\"].astype(str)\n",
    "lprb[\"node_id\"] = lprb[\"node_id\"].astype(str)\n",
    "\n",
    "merged = regress_path.merge(\n",
    "    lprb[[\"node_id\", \"min_value\"]],\n",
    "    on=\"node_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Choose intercept:\n",
    "# - If 'intercept2' column exists, use it when not NaN,\n",
    "#   otherwise fall back to 'intercept1'\n",
    "# - If 'intercept2' does NOT exist, use only 'intercept1'\n",
    "if \"intercept2\" in merged.columns:\n",
    "    intercept = merged[\"intercept2\"].fillna(merged[\"intercept1\"])\n",
    "else:\n",
    "    intercept = merged[\"intercept1\"]\n",
    "\n",
    "merged[\"error\"] = intercept - merged[\"min_value\"]\n",
    "merged[\"abs_error\"] = merged[\"error\"].abs()\n",
    "merged[\"rel_error_%\"] = merged[\"error\"] / merged[\"min_value\"] * 100\n",
    "\n",
    "merged.to_csv(\n",
    "    os.path.join(\n",
    "        INTERMEDIATE,\n",
    "        \"Store/Validation/csv/x_track_corr/Sacramento/Sacramento_reg_bottom_error.csv\"\n",
    "    ),\n",
    "    index=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b64e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_csv_path = INTERMEDIATE/\"Store/Validation/csv/x_track_corr/Sacramento/Sacramento_reg_bottom_error.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f13409f",
   "metadata": {},
   "source": [
    "Create folders for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530dc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Base directory where you want to create the folders\n",
    "#    For example: \"/path/to/output_folders\"\n",
    "\n",
    "\n",
    "# 3) Read the master CSV into a DataFrame\n",
    "master_df = pd.read_csv(master_csv_path)\n",
    "\n",
    "# 4) Extract unique node_id values (as strings)\n",
    "unique_nodes = master_df[\"node_id\"].astype(str).unique()\n",
    "\n",
    "# 5) Write the list of node_id codes to a CSV (no header, one per line)\n",
    "list_csv_path = os.path.join(base_folder, \"node_ids_list.csv\")\n",
    "pd.Series(unique_nodes).to_csv(list_csv_path, index=False, header=False)\n",
    "\n",
    "# 6) Loop over each unique node_id and create a folder if it doesn't already exist\n",
    "for node_id in unique_nodes:\n",
    "    node_folder = os.path.join(base_folder, node_id)\n",
    "    # exist_ok=True means ‚Äúdo nothing if the folder already exists‚Äù\n",
    "    os.makedirs(node_folder, exist_ok=True)\n",
    "    print(f\"Ensured folder exists: {node_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f0bcba",
   "metadata": {},
   "source": [
    "Create in each node_id folder, a csv file with a list of WSE extracted from the regression csv for each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f484f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_wse_csvs(\n",
    "    master_csv_path: str,\n",
    "    base_folder: str,\n",
    "    subtract_value: float = 2.04,\n",
    "    apply_subtraction: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    For each node_id in master_df, write:\n",
    "        1) raw WSEs ‚Üí base_folder/node_id/node_id.csv\n",
    "        2) (optionally) adjusted WSEs (WSE - subtract_value)\n",
    "            ‚Üí base_folder/node_id/node_id_adjusted.csv\n",
    "\n",
    "    Args:\n",
    "        master_csv_path: path to CSV with columns 'node_id' and 'wse'\n",
    "        base_folder: directory that will contain subfolders named by node_id\n",
    "        subtract_value: the value to subtract from each WSE when apply_subtraction=True\n",
    "        apply_subtraction: if True, also create the adjusted CSV\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(master_csv_path):\n",
    "        print(f\"Error: File '{master_csv_path}' does not exist.\")\n",
    "        return\n",
    "\n",
    "    # Read the master table\n",
    "    df = pd.read_csv(master_csv_path)\n",
    "\n",
    "    # Loop through each distinct node_id\n",
    "    for node_id in df[\"node_id\"].astype(str).unique():\n",
    "        # Grab all the WSE values for this node\n",
    "        wse_values = df.loc[df[\"node_id\"].astype(str) == node_id, \"wse\"]\n",
    "\n",
    "        # Ensure the node's folder exists\n",
    "        node_folder = os.path.join(base_folder, node_id)\n",
    "        os.makedirs(node_folder, exist_ok=True)\n",
    "\n",
    "        # 1) Write the raw WSEs\n",
    "        raw_path = os.path.join(node_folder, f\"{node_id}.csv\")\n",
    "        wse_values.to_csv(raw_path, index=False, header=False)\n",
    "        print(f\"‚úî Saved {len(wse_values)} raw WSEs to {raw_path}\")\n",
    "\n",
    "        # 2) If requested, write the adjusted WSEs\n",
    "        if apply_subtraction:\n",
    "            adjusted = wse_values - subtract_value\n",
    "            adj_path = os.path.join(node_folder, f\"{node_id}_adjusted.csv\")\n",
    "            adjusted.to_csv(adj_path, index=False, header=False)\n",
    "            print(f\"‚úî Saved {len(adjusted)} adjusted WSEs to {adj_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bcf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just export raw WSE lists:\n",
    "export_wse_csvs(master_csv_path, base_folder)\n",
    "\n",
    "# Export raw WSE lists *and* create adjusted CSVs (subtract 2.04):\n",
    "#export_wse_csvs(master_csv_path, base_folder, apply_subtraction=True)\n",
    "\n",
    "# Export with a different subtraction value, say 1.5:\n",
    "#export_wse_csvs(master_csv_path, base_folder, subtract_value=1.5, apply_subtraction=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d3e509",
   "metadata": {},
   "source": [
    "#### 3. Create a mask for each WSE in each node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b52b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_to    = None                     # or e.g. [\"21406100080691\", \"21406100080702\"]\n",
    "\n",
    "# Tunable: number of worker threads for writing masks\n",
    "_max_workers = min(8, os.cpu_count() or 4)\n",
    "\n",
    "def _write_mask_single(out_path, profile, dem, valid, nodata_mask, nodata_value, t):\n",
    "    \"\"\"Build and write one mask TIFF for threshold t.\"\"\"\n",
    "    mask = np.zeros_like(dem, dtype=np.uint8)\n",
    "    np.less_equal(dem, t, where=valid, out=mask, casting=\"unsafe\")\n",
    "    mask[nodata_mask] = 255\n",
    "    with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "        dst.write(mask, 1)\n",
    "    return out_path\n",
    "\n",
    "with rasterio.Env(GDAL_NUM_THREADS=\"ALL_CPUS\", VSI_CACHE=\"TRUE\", VSI_CACHE_SIZE=str(256*1024*1024)):\n",
    "    # === STEP 1: Read the DEM ONCE ===\n",
    "    with rasterio.open(tif_path) as src:\n",
    "        dem = src.read(1, resampling=Resampling.nearest)\n",
    "        profile = src.profile.copy()\n",
    "        nodata_value = src.nodata if src.nodata is not None else -9999\n",
    "\n",
    "    profile.update({\n",
    "        \"dtype\": rasterio.uint8,\n",
    "        \"count\": 1,\n",
    "        \"nodata\": 255,\n",
    "        \"compress\": \"lzw\"\n",
    "    })\n",
    "\n",
    "    nodata_mask = (dem == nodata_value)\n",
    "    valid = ~nodata_mask\n",
    "\n",
    "    # === STEP 2: Locate all node_id subfolders ===\n",
    "    node_folders = [d for d in glob.glob(os.path.join(base_folder, \"*\")) if os.path.isdir(d)]\n",
    "    if limit_to is not None:\n",
    "        node_folders = [d for d in node_folders if os.path.basename(d) in limit_to]\n",
    "\n",
    "    print(f\"üîç Found {len(node_folders)} node‚Äêfolders to process.\\n\")\n",
    "\n",
    "    # === STEP 3: Process each folder ===\n",
    "    for node_folder in node_folders:\n",
    "        node_id = os.path.basename(node_folder)\n",
    "        print(f\" Processing node_id = {node_id}\")\n",
    "\n",
    "        csv_files = glob.glob(os.path.join(node_folder, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            print(f\" No CSVs found inside {node_folder}. Skipping.\\n\")\n",
    "            continue\n",
    "\n",
    "        for csv_path in csv_files:\n",
    "            try:\n",
    "                df_wse = pd.read_csv(csv_path, header=None, names=[\"wse\"], dtype=float)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {csv_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            wse_values = df_wse[\"wse\"].dropna().values\n",
    "            if wse_values.size == 0:\n",
    "                print(f\"No valid WSE values in {csv_path}. Skipping this file.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Found {len(wse_values)} thresholds in {os.path.basename(csv_path)}\")\n",
    "\n",
    "            masks_folder = os.path.join(node_folder, \"masks\")\n",
    "            os.makedirs(masks_folder, exist_ok=True)\n",
    "\n",
    "            futures = []\n",
    "            with ThreadPoolExecutor(max_workers=_max_workers) as ex:\n",
    "                for t in wse_values:\n",
    "                    t_str = str(t).replace(\".\", \"_\").replace(\"-\", \"m\")\n",
    "                    out_name = f\"mask_{t_str}.tif\"\n",
    "                    out_path = os.path.join(masks_folder, out_name)\n",
    "\n",
    "                    if os.path.exists(out_path):\n",
    "                        print(f\"      ‚Ä¢ {out_name} already exists. Skipping.\")\n",
    "                        continue\n",
    "\n",
    "                    futures.append(\n",
    "                        ex.submit(_write_mask_single, out_path, profile, dem, valid, nodata_mask, nodata_value, t)\n",
    "                    )\n",
    "\n",
    "                for f in as_completed(futures):\n",
    "                    try:\n",
    "                        done_path = f.result()\n",
    "                        print(f\"Saved {os.path.basename(done_path)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"      ‚Ä¢ Failed to write a mask: {e}\")\n",
    "\n",
    "        print(\"\")  # blank line\n",
    "\n",
    "    print(\"All node‚Äêfolders processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454f46b",
   "metadata": {},
   "source": [
    "#### 4. Measuring river widths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7343cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=rasterio.errors.NotGeoreferencedWarning)\n",
    "\n",
    "def ensure_mask_georeferenced(mask_path, ref_crs, ref_transform):\n",
    "    with rasterio.open(mask_path, 'r+') as dst:\n",
    "        needs_update = False\n",
    "        if dst.crs is None:\n",
    "            dst.crs = ref_crs\n",
    "            needs_update = True\n",
    "        if dst.transform.is_identity:\n",
    "            dst.transform = ref_transform\n",
    "            needs_update = True\n",
    "        if needs_update:\n",
    "            print(f\"Updated georeferencing for: {os.path.basename(mask_path)}\")\n",
    "\n",
    "def fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform):\n",
    "    for node_folder in os.listdir(base_folder):\n",
    "        masks_dir = os.path.join(base_folder, node_folder, \"masks\")\n",
    "        if os.path.isdir(masks_dir):\n",
    "            for mask_path in glob.glob(os.path.join(masks_dir, \"mask_*.tif\")):\n",
    "                ensure_mask_georeferenced(mask_path, ref_crs, ref_transform)\n",
    "\n",
    "def extract_wse(mask_filename):\n",
    "    # strip extension and split on underscores\n",
    "    name = os.path.splitext(os.path.basename(mask_filename))[0]\n",
    "    parts = name.split('_')   # [\"mask\", \"50\", \"535\"]\n",
    "    if len(parts) == 3:\n",
    "        return float(f\"{parts[1]}.{parts[2]}\")  # \"50\" + \".\" + \"535\" ‚Üí 50.535\n",
    "    return None\n",
    "\n",
    "def possible_node_id_strings(node_id):\n",
    "    candidates = set()\n",
    "    candidates.add(str(node_id))\n",
    "    try:\n",
    "        as_float = float(node_id)\n",
    "        candidates.add(str(int(as_float)))\n",
    "        candidates.add(f\"{as_float:.3f}\")\n",
    "        candidates.add(f\"{as_float:.1f}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return list(candidates)\n",
    "\n",
    "def measure_mask_segments_by_sampling(polyline, mask_path, gdf_crs, spacing=1):\n",
    "    with rasterio.open(mask_path) as src:\n",
    "        raster_crs = src.crs\n",
    "\n",
    "        # ‚îÄ‚îÄ‚îÄ REPLACED: use the mask/raster CRS (e.g. EPSG:6339) instead of Italy UTM ‚îÄ‚îÄ‚îÄ\n",
    "        utm_crs = raster_crs\n",
    "\n",
    "        if gdf_crs and gdf_crs != utm_crs:\n",
    "            polyline_proj = gpd.GeoSeries([polyline], crs=gdf_crs).to_crs(utm_crs).iloc[0]\n",
    "        else:\n",
    "            polyline_proj = polyline\n",
    "\n",
    "        length = polyline_proj.length\n",
    "        if length < spacing:\n",
    "            return []\n",
    "\n",
    "        num_points = int(length / spacing)\n",
    "        distances = np.linspace(0, length, num_points)\n",
    "        points = [polyline_proj.interpolate(d) for d in distances]\n",
    "\n",
    "        coords = gpd.GeoSeries(points, crs=utm_crs).to_crs(raster_crs)\n",
    "        coords = [(pt.x, pt.y) for pt in coords]\n",
    "\n",
    "        vals = [v[0] for v in src.sample(coords)]\n",
    "        vals = [int(round(v)) if v in [0, 1] else 0 for v in vals]\n",
    "\n",
    "        segments = []\n",
    "        in_segment = False\n",
    "        start_dist = None\n",
    "\n",
    "        for i in range(1, len(vals)):\n",
    "            prev, curr = vals[i - 1], vals[i]\n",
    "            if not in_segment and prev == 0 and curr == 1:\n",
    "                in_segment = True\n",
    "                start_dist = distances[i]\n",
    "            elif in_segment and prev == 1 and curr == 0:\n",
    "                end_dist = distances[i]\n",
    "                seg_len = end_dist - start_dist\n",
    "                if seg_len >= 20:\n",
    "                    segments.append(seg_len)\n",
    "                in_segment = False\n",
    "\n",
    "        return segments\n",
    "\n",
    "def process_single_polyline(args):\n",
    "    idx, row, base_folder, node_id_field, gdf_crs = args\n",
    "    node_id_raw = row[node_id_field]\n",
    "    possible_names = possible_node_id_strings(node_id_raw)\n",
    "    folder_path = None\n",
    "    used_node_id = None\n",
    "    for node_id in possible_names:\n",
    "        test_path = os.path.join(base_folder, node_id, 'masks')\n",
    "        if os.path.exists(test_path):\n",
    "            folder_path = test_path\n",
    "            used_node_id = node_id\n",
    "            break\n",
    "    if folder_path is None:\n",
    "        return []\n",
    "\n",
    "    polyline = row.geometry\n",
    "    mask_files = sorted(glob.glob(os.path.join(folder_path, 'mask_*.tif')))\n",
    "    if not mask_files:\n",
    "        return []\n",
    "\n",
    "    row_results = []\n",
    "    max_segments = 0\n",
    "    for mask_file in mask_files:\n",
    "        wse_val = extract_wse(os.path.basename(mask_file))\n",
    "        if wse_val is None:\n",
    "            continue\n",
    "        segment_lengths = measure_mask_segments_by_sampling(polyline, mask_file, gdf_crs)\n",
    "        if segment_lengths:\n",
    "            width = max(segment_lengths)\n",
    "            n_segments = len(segment_lengths)\n",
    "            others = sorted([s for s in segment_lengths if s != width], reverse=True)\n",
    "        else:\n",
    "            width = 0.0\n",
    "            n_segments = 0\n",
    "            others = []\n",
    "        max_segments = max(max_segments, len(others))\n",
    "        row_results.append([used_node_id, wse_val, width, n_segments] + others)\n",
    "    return row_results, max_segments\n",
    "\n",
    "def process_all_parallel(base_folder, shapefile_path, output_csv_path, reference_raster_path, node_id_field='node_id', n_workers=None):\n",
    "    gdf = gpd.read_file(shapefile_path)\n",
    "    all_results = []\n",
    "    max_segments_overall = 0\n",
    "\n",
    "    with rasterio.open(reference_raster_path) as ref:\n",
    "        ref_crs = ref.crs\n",
    "        ref_transform = ref.transform\n",
    "\n",
    "    print(\"Checking and fixing georeferencing for all masks...\")\n",
    "    fix_all_masks_in_base_folder(base_folder, ref_crs, ref_transform)\n",
    "    print(\"Georeference check complete.\\n\")\n",
    "\n",
    "    if n_workers is None:\n",
    "        n_workers = multiprocessing.cpu_count()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "        args_iterable = [\n",
    "            (idx, row, base_folder, node_id_field, gdf.crs)\n",
    "            for idx, row in gdf.iterrows()\n",
    "        ]\n",
    "        futures = [executor.submit(process_single_polyline, args) for args in args_iterable]\n",
    "        for i, future in enumerate(as_completed(futures), 1):\n",
    "            res = future.result()\n",
    "            if res:\n",
    "                row_results, max_segs = res\n",
    "                all_results.extend(row_results)\n",
    "                max_segments_overall = max(max_segments_overall, max_segs)\n",
    "            if i % 5 == 0 or i == len(futures):\n",
    "                print(f\"Processed {i}/{len(futures)} polylines...\")\n",
    "\n",
    "    columns = ['node_id', 'wse', 'width', 'n_segments'] + [f'seg_{i+1}' for i in range(max_segments_overall)]\n",
    "    padded_results = []\n",
    "    for row in all_results:\n",
    "        base = row[:4]\n",
    "        segs = row[4:]\n",
    "        segs = segs + [float('nan')] * (max_segments_overall - len(segs))\n",
    "        padded_results.append(base + segs)\n",
    "\n",
    "    df = pd.DataFrame(padded_results, columns=columns)\n",
    "    df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "\n",
    "# Example usage (edit paths as needed):\n",
    "path2poly = os.path.join(os.getcwd(), \"2_intermediate/Store/Binary_Masks/Sacramento/Shps/Sacramento_cx.shp\")\n",
    "process_all_parallel(base_folder, path2poly, os.path.join(os.getcwd(),\"2_intermediate/Store/Binary_Masks/Sacramento/Bath_widths.csv\"), \n",
    "                    tif_path, n_workers=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c5c451",
   "metadata": {},
   "source": [
    "### 5. OLS regression for dynamic threshold: ###\n",
    "\n",
    "This means that piecewise models can include either two segments (one breakpoint) or three segments (two breakpoints), provided that each segment spans more than a fixed percentage of the river width. This constraint prevents unrealistically short segments of only 2‚Äì5 m, which would be impossible for SWOT to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66850fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Kept for compatibility (now unused) ---\n",
    "def compute_aic(rss, n, k):\n",
    "    return n * np.log(rss / n) + 2 * k\n",
    "\n",
    "# --- Renamed behavior only: still called \"fit_huber\", but now does plain OLS ---\n",
    "def fit_huber(X_input, y_input):\n",
    "    \"\"\"\n",
    "    Performs Ordinary Least Squares (OLS) but keeps the original function name and signature.\n",
    "    Returns a model object with .coef_, .intercept_, and .predict like sklearn's API,\n",
    "    plus y_pred and residuals for downstream use.\n",
    "    \"\"\"\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_input, y_input)\n",
    "    y_pred = model.predict(X_input)\n",
    "    residuals = y_input - y_pred\n",
    "    return model, y_pred, residuals\n",
    "\n",
    "def fit_regressions_for_group(\n",
    "    X, y,\n",
    "    min_width_range=None,   # kept in signature (unused)\n",
    "    alpha=0.30,             # kept in signature (unused)\n",
    "    basis='median'          # kept in signature (unused)\n",
    "):\n",
    "    \"\"\"\n",
    "    SIMPLE OLS ONLY.\n",
    "    - No piecewise models\n",
    "    - No breakpoints\n",
    "    - No dynamic thresholds or AIC selection\n",
    "\n",
    "    Returns the same keys as before, but with only the simple results populated.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    results = {\n",
    "        'best_model_type': 'simple',\n",
    "        'bath_slope1': None,\n",
    "        'bath_slope2': None,\n",
    "        'bath_slope3': None,\n",
    "        'r2': None,\n",
    "        'intercepts': (None, None, None),\n",
    "        'bp': None\n",
    "    }\n",
    "\n",
    "    # Simple linear regression (OLS)\n",
    "    model_s, y_pred_s, res_s = fit_huber(X.reshape(-1, 1), y)\n",
    "    slope_simple = model_s.coef_[0]\n",
    "    intercept_simple = model_s.intercept_\n",
    "    r2_simple = r2_score(y, y_pred_s)\n",
    "\n",
    "    # Store simple-only results (keep original keys)\n",
    "    results['type'] = 'simple'\n",
    "    results['bath_slope1'] = slope_simple\n",
    "    results['intercepts'] = (intercept_simple, None, None)\n",
    "    results['r2'] = r2_simple\n",
    "    # bath_slope2/bath_slope3 remain None; bp remains None\n",
    "\n",
    "    return results\n",
    "\n",
    "def groupwise_regression(\n",
    "    csv_path,\n",
    "    min_width_range=None,  # kept in signature (unused)\n",
    "    alpha=0.30,            # kept in signature (unused)\n",
    "    basis='median',        # kept in signature (unused)\n",
    "    global_stat=False      # kept in signature (unused)\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the CSV and runs SIMPLE OLS per node_id.\n",
    "    No dynamic spans, no AIC, no piecewise. Parameters kept for compatibility but unused.\n",
    "\n",
    "    Output columns (simple-only, keeping names where relevant):\n",
    "      - node_id\n",
    "      - bath_slope1\n",
    "      - bath_intercept\n",
    "      - r2\n",
    "      - type  (always 'simple')\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    result_rows = []\n",
    "    group_results = {}\n",
    "\n",
    "    for node_id, group in df.groupby('node_id'):\n",
    "        group = group.dropna(subset=['width', 'wse'])\n",
    "        if len(group) < 2:\n",
    "            # Need at least 2 points for an OLS line\n",
    "            continue\n",
    "\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "\n",
    "        res = fit_regressions_for_group(\n",
    "            X, y,\n",
    "            min_width_range=min_width_range,\n",
    "            alpha=alpha,\n",
    "            basis=basis\n",
    "        )\n",
    "\n",
    "        # SIMPLE-ONLY OUTPUT (keep names; omit piecewise-specific fields)\n",
    "        result_rows.append({\n",
    "            'node_id': node_id,\n",
    "            'bath_slope1': res['bath_slope1'],\n",
    "            'bath_intercept': res['intercepts'][0],\n",
    "            'r2': res['r2'],\n",
    "            'type': res['type'],\n",
    "        })\n",
    "        group_results[node_id] = (group, res)\n",
    "\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    return result_df, group_results\n",
    "\n",
    "def plot_random_regressions(group_results, n=8):\n",
    "    \"\"\"\n",
    "    Simple plotter for the OLS-only results.\n",
    "    Keeps the function name, but removes piecewise logic.\n",
    "    \"\"\"\n",
    "    if not group_results:\n",
    "        print(\"No groups to plot.\")\n",
    "        return\n",
    "\n",
    "    plot_ids = random.sample(list(group_results.keys()), min(n, len(group_results)))\n",
    "    rows = 2\n",
    "    cols = 4 if n >= 8 else max(1, min(4, len(plot_ids)))\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(20, 10)) if (rows*cols) > 1 else plt.subplots(1, 1, figsize=(8, 6))\n",
    "    axs = np.array(axs).flatten() if isinstance(axs, np.ndarray) else np.array([axs])\n",
    "\n",
    "    for i, node_id in enumerate(plot_ids):\n",
    "        group, res = group_results[node_id]\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "\n",
    "        ax.scatter(X, y, label=\"Data\")\n",
    "        x_plot = np.linspace(X.min(), X.max(), 100)\n",
    "\n",
    "        # simple line\n",
    "        slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "        if slope is not None and intercept is not None:\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, linewidth=2, label=f\"Simple: slope={slope:.3f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR2={res['r2']:.3f}\" if res['r2'] is not None else f\"node_id={node_id}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486c2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "path2w_wse = INTERMEDIATE/\"Store/Binary_Masks/Pee_dee/Bath_widths.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1e865",
   "metadata": {},
   "source": [
    "#### 5.1. calling the function and plotting:\n",
    "csv_path = path2w_wse\n",
    "Path to a CSV with at least the columns node_id, width, and wse. The function groups rows by node_id and fits regressions per group.\n",
    "\n",
    "min_width_range = float(\"inf\")\n",
    "Sets a fixed minimum width span each segment must cover to accept a breakpoint. Using ‚àû means no finite segment can satisfy it ‚Üí all breakpoint models are rejected, so every group falls back to the simple (single-slope) Huber regression.\n",
    "\n",
    "global_stat = False\n",
    "If you were using a dynamic threshold (i.e., min_width_range=None), this would decide whether to compute the width statistic globally or per group. Since you set a fixed threshold, this flag is ignored.\n",
    "\n",
    "alpha = 0.20 and basis = 'mean'\n",
    "These only matter when min_width_range=None (dynamic span = alpha * {median|mean}(width)). With a fixed threshold, both are ignored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5b39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df, group_results = groupwise_regression(path2w_wse)\n",
    "# Optionally, save to CSV\n",
    "result_df.to_csv(INTERMEDIATE/\"Store/Binary_Masks/Pee_dee/Ups_Bath_simple.csv\", index=False)\n",
    "# Plot 8 random node_id fits\n",
    "plot_random_regressions(group_results, n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b820d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_regressions(group_results):\n",
    "    import math\n",
    "\n",
    "    n = len(group_results)\n",
    "    if n == 0:\n",
    "        print(\"No groups to plot.\")\n",
    "        return\n",
    "\n",
    "    ncols = 4\n",
    "    nrows = math.ceil(n / ncols)\n",
    "\n",
    "    fig, axs = plt.subplots(nrows, ncols, figsize=(5 * ncols, 4 * nrows))\n",
    "    # Ensure axs is always a flat iterable\n",
    "    if isinstance(axs, np.ndarray):\n",
    "        axs = axs.flatten()\n",
    "    else:\n",
    "        axs = [axs]\n",
    "\n",
    "    for i, (node_id, (group, res)) in enumerate(group_results.items()):\n",
    "        X = group['width'].values\n",
    "        y = group['wse'].values\n",
    "        ax = axs[i]\n",
    "        ax.scatter(X, y, label=\"Data\", color='steelblue')\n",
    "\n",
    "        x_plot = np.linspace(X.min(), X.max(), 300)\n",
    "        model_type = res['type']\n",
    "\n",
    "        if model_type == 'simple':\n",
    "            slope, intercept = res['bath_slope1'], res['intercepts'][0]\n",
    "            y_fit = slope * x_plot + intercept\n",
    "            ax.plot(x_plot, y_fit, color='crimson', linewidth=2, label=f\"Simple: slope={slope:.4f}\")\n",
    "\n",
    "        elif model_type == 'piecewise':\n",
    "            slope1, intercept1 = res['bath_slope1'], res['intercepts'][0]\n",
    "            slope2, intercept2 = res['bath_slope2'], res['intercepts'][1]\n",
    "            bp = res['bp']\n",
    "            y_left = slope2 * x_plot + intercept2\n",
    "            y_right = slope1 * x_plot + intercept1\n",
    "            ax.plot(x_plot[x_plot < bp], y_left[x_plot < bp], color='crimson', label=f\"Left: slope={slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp], y_right[x_plot >= bp], color='crimson', linestyle='--', label=f\"Right: slope={slope1:.4f}\")\n",
    "            ax.axvline(bp, color='gray', linestyle=':', label=f\"Breakpoint = {bp:.2f}\")\n",
    "\n",
    "        elif model_type == 'piecewise_2bp':\n",
    "            slope1, slope2, slope3 = res['bath_slope1'], res['bath_slope2'], res['bath_slope3']\n",
    "            intercept1, intercept2, intercept3 = res['intercepts']\n",
    "            bp1, bp2 = res['bp']\n",
    "\n",
    "            y_fit_left = slope3 * x_plot + intercept3\n",
    "            y_fit_mid = slope2 * x_plot + intercept2\n",
    "            y_fit_right = slope1 * x_plot + intercept1\n",
    "\n",
    "            ax.plot(x_plot[x_plot < bp1], y_fit_left[x_plot < bp1], color='crimson', label=f\"Left: {slope3:.4f}\")\n",
    "            ax.plot(x_plot[(x_plot >= bp1) & (x_plot < bp2)], y_fit_mid[(x_plot >= bp1) & (x_plot < bp2)], color='crimson', linestyle='--', label=f\"Mid: {slope2:.4f}\")\n",
    "            ax.plot(x_plot[x_plot >= bp2], y_fit_right[x_plot >= bp2], color='crimson', linestyle=':', label=f\"Right: {slope1:.4f}\")\n",
    "            ax.axvline(bp1, color='gray', linestyle=':', label=f\"BP1 = {bp1:.2f}\")\n",
    "            ax.axvline(bp2, color='gray', linestyle=':', label=f\"BP2 = {bp2:.2f}\")\n",
    "\n",
    "        ax.set_title(f\"node_id={node_id}\\nR¬≤ = {res['r2']:.2f}\")\n",
    "        ax.set_xlabel(\"Width\")\n",
    "        ax.set_ylabel(\"WSE\")\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove any extra unused subplots\n",
    "    total_axes = nrows * ncols\n",
    "    for j in range(n, total_axes):\n",
    "        fig.delaxes(axs[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843f3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_all_regressions(group_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aec3ba",
   "metadata": {},
   "source": [
    "#### * Merge csv file with Bathymetry slopes with the Regression master csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb88b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv(master_csv, complementary_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Merge two CSV files on 'node_id' and save the result.\n",
    "    \"\"\"\n",
    "    master_df = pd.read_csv(master_csv)\n",
    "    comp_df = pd.read_csv(complementary_csv)\n",
    "\n",
    "    # Merge on 'node_id'\n",
    "    merged_df = master_df.merge(comp_df, on='node_id', how='inner')\n",
    "\n",
    "    # Save the merged DataFrame\n",
    "    merged_df.to_csv(output_csv, index=False)\n",
    "    print(f\"Merged CSV saved to {output_csv}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "#### Names of files to use\n",
    "# Bath_slopes_simple.csv\n",
    "# Bath_slopes3.csv #Dynamically fitted\n",
    "\n",
    "path2slope = INTERMEDIATE/\"Store/Binary_Masks/Pee_dee/Ups_Bath_simple.csv\"\n",
    "outpath = INTERMEDIATE/\"Store/Validation/csv/Pee_dee/Ups_Pee_dee_reg_slope_simple.csv\"\n",
    "# output dynamically fitted slopes: reg_slope_bath3.csv\n",
    "# output simple fitted slopes: reg_slope_simple.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26992158",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath = merge_csv(master_csv_path, path2slope, outpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b2fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort = reg_slope_bath.sort_values('p_dist_out', ascending=False).groupby('node_id', as_index=False).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa0ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Outlier removal in SWOT and bathymetry\n",
    "Q1, Q3 = reg_slope_bath_sort['slope1'].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "l1, u1 = Q1 - 1.5*IQR, Q3 + 1.5*IQR\n",
    "\n",
    "# bath_slope1 fences\n",
    "Q1b, Q3b = reg_slope_bath_sort['bath_slope1'].quantile([0.25, 0.75])\n",
    "IQRb   = Q3b - Q1b\n",
    "l2, u2 = Q1b - 1.5*IQRb, Q3b + 1.5*IQRb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b444e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (reg_slope_bath_sort['slope1']      >= l1) & (reg_slope_bath_sort['slope1']      <= u1) &\n",
    "    (reg_slope_bath_sort['bath_slope1'] >= l2) & (reg_slope_bath_sort['bath_slope1'] <= u2)\n",
    ")\n",
    "removed_iqr = reg_slope_bath_sort.loc[~mask].copy()\n",
    "removed_iqr['removal_stage'] = 'IQR_fences'\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    _rel = 100.0 * (removed_iqr['slope1'] - removed_iqr['bath_slope1']) / removed_iqr['bath_slope1']\n",
    "removed_iqr['rel_error'] = _rel\n",
    "removed_iqr['abs_rel_error'] = removed_iqr['rel_error'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27523505",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_slope_bath_sort = reg_slope_bath_sort[mask]\n",
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976e3a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 0) Work on a copy so you can compare before/after if needed\n",
    "df = reg_slope_bath_sort.copy()\n",
    "\n",
    "# --- 1) Compute rel error (percent) in-place-friendly columns\n",
    "# (safe divide: ignore zero/NaN denominators in the trimming stats)\n",
    "rel = 100 * (df['slope1'] - df['bath_slope1']) / df['bath_slope1']\n",
    "df['rel_error'] = rel\n",
    "df['abs_rel_error'] = rel.abs()\n",
    "\n",
    "# --- 2) Build a UNIQUE view (like your CDF step) for threshold estimation only\n",
    "uniq = df[['slope1','bath_slope1','rel_error']].dropna().drop_duplicates(subset=['slope1','bath_slope1'])\n",
    "\n",
    "# --- 3) MAD right-tail rule (robust, no small-slope dropping)\n",
    "med  = uniq['rel_error'].median()\n",
    "mad0 = (uniq['rel_error'] - med).abs().median()\n",
    "mad  = 1.4826 * mad0                # #1.4826 makes MAD behave like std deviation\n",
    "k    = 3.0                          # tweakable: 2.5‚Äì4. tweak: bigger = keep more, smaller = remove more. it‚Äôs analogous to saying ‚Äúflag anything more than 3œÉ above the median.‚Äù\n",
    "thr_mad = med + k * mad\n",
    "\n",
    "mask_out_mad = uniq['rel_error'] > thr_mad\n",
    "\n",
    "# --- 4) Optional percentile tail shave (set q=None to skip)\n",
    "q = 0.99                            # top 1% cutoff; set to None to disable\n",
    "if q is not None:\n",
    "    thr_q = uniq['rel_error'].quantile(q)\n",
    "    mask_out_pct = uniq['rel_error'] > thr_q\n",
    "    mask_out_all = mask_out_mad | mask_out_pct\n",
    "else:\n",
    "    mask_out_all = mask_out_mad\n",
    "\n",
    "# --- 5) Map the unique outliers back to the full df (keep structure)\n",
    "out_pairs = uniq.loc[mask_out_all, ['slope1','bath_slope1']]\n",
    "out_pairs['__key__'] = 1\n",
    "df_key = df[['slope1','bath_slope1']].copy()\n",
    "df_key['__key__'] = 1\n",
    "\n",
    "# mark matches\n",
    "to_drop = df_key.merge(out_pairs, on=['__key__','slope1','bath_slope1'], how='left', indicator=True)\n",
    "is_rel_outlier = (to_drop['_merge'] == 'both').values\n",
    "\n",
    "# (optional) keep a flag and a log before dropping\n",
    "df['is_rel_outlier'] = is_rel_outlier\n",
    "removed = df[df['is_rel_outlier']].copy()\n",
    "kept    = df[~df['is_rel_outlier']].copy()\n",
    "\n",
    "removed = removed.copy()\n",
    "removed['removal_stage'] = 'MAD_or_pct'\n",
    "\n",
    "_deleted_cols = ['node_id', 'slope1', 'slope_bathy', 'rel_error', 'abs_rel_error', 'removal_stage']\n",
    "deleted_nodes = pd.concat(\n",
    "    [\n",
    "        removed_iqr.reindex(columns=_deleted_cols, fill_value=np.nan),\n",
    "        removed.reindex(columns=_deleted_cols, fill_value=np.nan)\n",
    "    ],\n",
    "    ignore_index=True\n",
    ")\n",
    "# Keep a single record per node_id (remove this line if you prefer all occurrences)\n",
    "deleted_nodes = deleted_nodes.drop_duplicates(subset=['node_id']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# --- 6) Replace reg_slope_bath_sort with the cleaned version (same columns as before)\n",
    "# If you want the exact original schema, drop the helper columns:\n",
    "reg_slope_bath_sort = kept.drop(columns=['rel_error','abs_rel_error','is_rel_outlier'])\n",
    "\n",
    "reg_slope_bath_sort['node_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad2e4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### For Intercept (Thalweg heights)\n",
    "\n",
    "# ---------- Helper: compute and print errors-only metrics ----------\n",
    "def _compute_and_print_metrics(x_raw, y_raw, label_x, label_y):\n",
    "    \"\"\"\n",
    "    Errors-only metrics between x and y.\n",
    "    y is treated as the reference (truth) in the relative error: (x - y) / y\n",
    "    \"\"\"\n",
    "    mask = np.isfinite(x_raw) & np.isfinite(y_raw)\n",
    "    x = x_raw[mask]\n",
    "    y = y_raw[mask]\n",
    "\n",
    "    if x.size < 2:\n",
    "        print(f\"[{label_x} vs {label_y}] Not enough paired data after NaN filtering.\")\n",
    "        return\n",
    "\n",
    "    mae  = mean_absolute_error(y, x)\n",
    "    rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "    rel_error = 100 * (x - y) / y\n",
    "    mean_bias = np.mean(rel_error)\n",
    "    mean_abs_rel_error = np.mean(np.abs(rel_error))\n",
    "    percentile_68 = np.percentile(np.abs(rel_error), 68)\n",
    "\n",
    "    print(f\"\\n=== {label_x} vs {label_y} ===\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Relative Error (Bias): {mean_bias:.2f}%\")\n",
    "    print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f}%\")\n",
    "    print(f\"68th percentile Absolute Relative Error: {percentile_68:.2f}%\")\n",
    "\n",
    "# ===================== INTERCEPT METRICS (place before slope metrics) =====================\n",
    "\n",
    "# 1) intercept1 (SWOT) vs min_value (bathymetry, real) ‚Äî ref = min_value\n",
    "_compute_and_print_metrics(\n",
    "    reg_slope_bath_sort['intercept1'].values,\n",
    "    reg_slope_bath_sort['min_value'].values,\n",
    "    label_x=\"intercept1 (SWOT)\",\n",
    "    label_y=\"min_value (bathymetry, real)\"\n",
    ")\n",
    "\n",
    "# 2) intercept1 (SWOT) vs bath_intercept (bathymetry OLS) ‚Äî ref = bath_intercept\n",
    "_compute_and_print_metrics(\n",
    "    reg_slope_bath_sort['intercept1'].values,\n",
    "    reg_slope_bath_sort['bath_intercept'].values,\n",
    "    label_x=\"intercept1 (SWOT)\",\n",
    "    label_y=\"bath_intercept (bathymetry OLS)\"\n",
    ")\n",
    "\n",
    "# 3) bath_intercept (bathymetry OLS) vs min_value (bathymetry, real) ‚Äî ref = min_value\n",
    "_compute_and_print_metrics(\n",
    "    reg_slope_bath_sort['bath_intercept'].values,\n",
    "    reg_slope_bath_sort['min_value'].values,\n",
    "    label_x=\"bath_intercept (bathymetry OLS)\",\n",
    "    label_y=\"min_value (bathymetry, real)\"\n",
    ")\n",
    "\n",
    "# ===================== SLOPE METRICS (errors only, same format) =====================\n",
    "\n",
    "x = reg_slope_bath_sort['slope1'].values        # SWOT slope\n",
    "y = reg_slope_bath_sort['bath_slope1'].values   # Bathymetry slope (reference)\n",
    "\n",
    "mask = np.isfinite(x) & np.isfinite(y)\n",
    "x = x[mask]; y = y[mask]\n",
    "\n",
    "if x.size < 2:\n",
    "    print(\"\\n[SWOT slope1 vs bath_slope1] Not enough paired data after NaN filtering.\")\n",
    "else:\n",
    "    mae  = mean_absolute_error(y, x)\n",
    "    rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "    rel_error = 100 * (x - y) / y\n",
    "    mean_bias = np.mean(rel_error)\n",
    "    mean_abs_rel_error = np.mean(np.abs(rel_error))\n",
    "    percentile_68 = np.percentile(np.abs(rel_error), 68)\n",
    "\n",
    "    print(f\"\\n=== slope1 (SWOT) vs bath_slope1 (bathymetry) ===\")\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    print(f\"Mean Relative Error (Bias): {mean_bias:.2f}%\")\n",
    "    print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f}%\")\n",
    "    print(f\"68th percentile Absolute Relative Error: {percentile_68:.2f}%\")\n",
    "\n",
    "# ---------- Keep your existing summary of deletions ----------\n",
    "print(f\"\\nTotal deleted nodes: {len(deleted_nodes)}\")\n",
    "print(\"Deleted node_ids:\", deleted_nodes['node_id'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f9a546",
   "metadata": {},
   "source": [
    "Error metrics and coefficient of correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f84571",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For Slope\n",
    "\n",
    "x = reg_slope_bath_sort['slope1'].values\n",
    "y = reg_slope_bath_sort['bath_slope1'].values\n",
    "\n",
    "# Classic metrics\n",
    "pearson_corr, pearson_p = pearsonr(x, y)\n",
    "spearman_corr, spearman_p = spearmanr(x, y)\n",
    "mae = mean_absolute_error(y, x)\n",
    "rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "# Relative errors\n",
    "rel_error = 100 * (x - y) / y\n",
    "mean_bias = np.mean(rel_error)\n",
    "mean_abs_rel_error = np.mean(np.abs(rel_error))\n",
    "percentile_68 = np.percentile(np.abs(rel_error), 68)\n",
    "\n",
    "print(f\"Pearson correlation: {pearson_corr:.3f} (p={pearson_p:.2g})\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.3f} (p={spearman_p:.2g})\")\n",
    "#print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "#print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "##print(f\"Mean Relative Error (Bias): {mean_bias:.2f}%\")\n",
    "#rint(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f}%\")\n",
    "#print(f\"68th percentile Absolute Relative Error: {percentile_68:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal deleted nodes: {len(deleted_nodes)}\")\n",
    "print(\"Deleted node_ids:\", deleted_nodes['node_id'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2789e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Bootstrap correlations + append one river to a CSV (now with 68th percentiles) ======\n",
    "import numpy as np, csv, os\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# where to accumulate all rivers\n",
    "OUT_CSV = \"/Volumes/Science_SSD/Dissertation/2_intermediate/Store/Validation/csv/Per_river_stats_accumulated_OLS.csv\"\n",
    "\n",
    "BOOTSTRAP_B = 2000   # resamples per river\n",
    "MIN_SAMPLES = 10     # skip if too few pairs\n",
    "RNG_SEED    = 42\n",
    "\n",
    "def _clean_xy(x, y):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    y = np.asarray(y, dtype=float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    return x[m], y[m]\n",
    "\n",
    "def _abs_rel_error_percent(x, y):\n",
    "    \"\"\"Return (signed) relative error (%) and absolute relative error (%), guarding y==0.\"\"\"\n",
    "    m = (y != 0) & np.isfinite(x) & np.isfinite(y)\n",
    "    rel = np.full_like(x, np.nan, dtype=float)\n",
    "    rel[m] = 100.0 * (x[m] - y[m]) / y[m]   # signed %\n",
    "    abs_rel = np.abs(rel[np.isfinite(rel)])  # magnitude %\n",
    "    return rel, abs_rel\n",
    "\n",
    "def _bootstrap_corrs(x, y, B=1000, seed=None):\n",
    "    \"\"\"Bootstrap distributions of Pearson r and Spearman œÅ.\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(x)\n",
    "    pr = np.empty(B); sr = np.empty(B)\n",
    "    for b in range(B):\n",
    "        idx = rng.integers(0, n, size=n, endpoint=False)\n",
    "        xb, yb = x[idx], y[idx]\n",
    "        # robust to constant resamples\n",
    "        try:   pr[b] = pearsonr(xb, yb)[0]\n",
    "        except Exception: pr[b] = np.nan\n",
    "        try:   sr[b] = spearmanr(xb, yb)[0]\n",
    "        except Exception: sr[b] = np.nan\n",
    "    pr = pr[np.isfinite(pr)]\n",
    "    sr = sr[np.isfinite(sr)]\n",
    "    return pr, sr\n",
    "\n",
    "def _qstats(a):\n",
    "    if a.size == 0: return np.nan, np.nan, np.nan, np.nan\n",
    "    q1, q2, q3 = np.nanpercentile(a, [25, 50, 75])\n",
    "    return q1, q2, q3, (q3 - q1)\n",
    "\n",
    "def append_river_with_bootstrap(label, x, y,\n",
    "                                out_csv=OUT_CSV,\n",
    "                                B=BOOTSTRAP_B,\n",
    "                                min_samples=MIN_SAMPLES,\n",
    "                                seed=RNG_SEED):\n",
    "    \"\"\"\n",
    "    Compute metrics + bootstrap IQRs of Pearson/Spearman and append one row to CSV.\n",
    "    Also stores:\n",
    "      - bias_mean_% (mean signed relative error, %)\n",
    "      - rel_p68_% (68th percentile of signed relative error, %)\n",
    "      - abs_rel_p68_% (68th percentile of absolute relative error, %)\n",
    "      - abs_rel_q1_%, abs_rel_q3_%, abs_rel_iqr_% (magnitude)\n",
    "    \"\"\"\n",
    "    x, y = _clean_xy(x, y)\n",
    "    n = len(x)\n",
    "    if n < min_samples:\n",
    "        print(f\"‚ö†Ô∏è  Skipped '{label}' (n={n} < {min_samples})\")\n",
    "        return\n",
    "\n",
    "    # point estimates (originals)\n",
    "    pearson_r, pearson_p     = pearsonr(x, y)\n",
    "    spearman_rho, spearman_p = spearmanr(x, y)\n",
    "    mae  = mean_absolute_error(y, x)\n",
    "    rmse = np.sqrt(mean_squared_error(y, x))\n",
    "\n",
    "    # relative errors\n",
    "    rel, abs_rel = _abs_rel_error_percent(x, y)\n",
    "    bias_mean   = np.nanmean(rel) if np.any(np.isfinite(rel)) else np.nan          # %\n",
    "    mare        = np.nanmean(abs_rel) if abs_rel.size else np.nan                 # %\n",
    "    abs_q1      = np.nanpercentile(abs_rel, 25) if abs_rel.size else np.nan\n",
    "    abs_q3      = np.nanpercentile(abs_rel, 75) if abs_rel.size else np.nan\n",
    "    abs_iqr     = (abs_q3 - abs_q1) if abs_rel.size else np.nan\n",
    "    # 68th percentiles\n",
    "    rel_p68     = np.nanpercentile(rel, 68) if np.any(np.isfinite(rel)) else np.nan        # signed %\n",
    "    abs_rel_p68 = np.nanpercentile(abs_rel, 68) if abs_rel.size else np.nan                # magnitude %\n",
    "\n",
    "    # bootstrap coefficient variability\n",
    "    pr_dist, sr_dist = _bootstrap_corrs(x, y, B=B, seed=seed)\n",
    "    p_q1, p_med, p_q3, p_iqr = _qstats(pr_dist)\n",
    "    s_q1, s_med, s_q3, s_iqr = _qstats(sr_dist)\n",
    "\n",
    "    print(f\"River: {label} (n={n})\")\n",
    "    print(f\"  Pearson r = {pearson_r:.3f}  | Bootstrap Q1/med/Q3 = {p_q1:.3f}/{p_med:.3f}/{p_q3:.3f}\")\n",
    "    print(f\"  Spearman œÅ = {spearman_rho:.3f} | Bootstrap Q1/med/Q3 = {s_q1:.3f}/{s_med:.3f}/{s_q3:.3f}\")\n",
    "    print(f\"  MAE={mae:.4f}  RMSE={rmse:.4f}  Bias={bias_mean:.2f}%\")\n",
    "    print(f\"  |Rel error|: MARE={mare:.2f}%, Q1={abs_q1:.2f}%, Q3={abs_q3:.2f}%, IQR={abs_iqr:.2f}%\")\n",
    "    print(f\"  Rel 68th=% {rel_p68:.2f}   |Rel| 68th=% {abs_rel_p68:.2f}\")\n",
    "\n",
    "    row = {\n",
    "        \"river\": str(label), \"n\": int(n),\n",
    "        \"pearson_r\": pearson_r, \"pearson_p\": pearson_p,\n",
    "        \"spearman_rho\": spearman_rho, \"spearman_p\": spearman_p,\n",
    "\n",
    "        # bootstrap whiskers for coefficients (to plot)\n",
    "        \"pearson_q1\": p_q1, \"pearson_q2\": p_med, \"pearson_q3\": p_q3, \"pearson_iqr\": p_iqr,\n",
    "        \"spearman_q1\": s_q1, \"spearman_q2\": s_med, \"spearman_q3\": s_q3, \"spearman_iqr\": s_iqr,\n",
    "\n",
    "        # error stats (percent)\n",
    "        \"mae\": mae, \"rmse\": rmse,\n",
    "        \"bias_mean_%\": bias_mean,           # signed mean bias (%)\n",
    "        \"rel_p68_%\": rel_p68,               # 68th percentile of signed relative error (%)\n",
    "        \"abs_rel_p68_%\": abs_rel_p68,       # 68th percentile of absolute relative error (%)\n",
    "        \"mare_%\": mare,                     # mean absolute relative error (%)\n",
    "        \"abs_rel_q1_%\": abs_q1, \"abs_rel_q3_%\": abs_q3, \"abs_rel_iqr_%\": abs_iqr,\n",
    "    }\n",
    "\n",
    "    file_exists = os.path.isfile(out_csv)\n",
    "    with open(out_csv, \"a\", newline=\"\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=list(row.keys()))\n",
    "        if not file_exists:\n",
    "            w.writeheader()\n",
    "        w.writerow(row)\n",
    "\n",
    "    print(f\"‚úî Appended ‚Üí {out_csv}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac6afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your arrays:\n",
    "x = reg_slope_bath_sort['slope1'].values\n",
    "y = reg_slope_bath_sort['bath_slope1'].values  # <- your bath column\n",
    "\n",
    "# Give this run a label (river name/id)\n",
    "append_river_with_bootstrap(\"Ups_Pee_Dee\", x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2cbf64",
   "metadata": {},
   "source": [
    "#### Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f616aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_error = 100 * (reg_slope_bath_sort['slope1'] - reg_slope_bath_sort['bath_slope1']) / reg_slope_bath_sort['bath_slope1']\n",
    "abs_rel_error = np.abs(rel_error)\n",
    "\n",
    "# Drop duplicates for (slope1, bath_slope1) pairs (optional but matches your logic)\n",
    "df_rel = pd.DataFrame({\n",
    "    'slope1': reg_slope_bath_sort['slope1'],\n",
    "    'bath_slope1': reg_slope_bath_sort['bath_slope1'],\n",
    "    'rel_error': rel_error,\n",
    "    'abs_rel_error': abs_rel_error\n",
    "})\n",
    "df_rel_unique = df_rel.drop_duplicates(subset=['slope1', 'bath_slope1'], keep='first')\n",
    "\n",
    "# Sorted arrays for CDFs\n",
    "bias_sorted = np.sort(df_rel_unique['rel_error'].values)\n",
    "cdf_bias = np.arange(1, len(bias_sorted) + 1) / len(bias_sorted)\n",
    "\n",
    "abs_sorted = np.sort(df_rel_unique['abs_rel_error'].values)\n",
    "cdf_abs = np.arange(1, len(abs_sorted) + 1) / len(abs_sorted)\n",
    "\n",
    "# Metrics\n",
    "mean_bias = df_rel_unique['rel_error'].mean()\n",
    "mean_abs_rel_error = df_rel_unique['abs_rel_error'].mean()\n",
    "spearman_rho, spearman_p = spearmanr(df_rel_unique['slope1'], df_rel_unique['bath_slope1'])\n",
    "p68 = np.percentile(df_rel_unique['abs_rel_error'], 68)\n",
    "\n",
    "# Colors (customize as desired)\n",
    "color_bias = '#ff5722'\n",
    "color_abs = '#fbc02d'\n",
    "color_p68 = 'dimgrey'\n",
    "color_zero = '#ffa726'\n",
    "color_mean = '#2979ff'\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Plotting ---\n",
    "# ------------------------------------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.set_facecolor('#f5f5f5')\n",
    "\n",
    "# Plot Relative Error CDF\n",
    "ax.scatter(\n",
    "    bias_sorted, cdf_bias,\n",
    "    color=color_bias,\n",
    "    label='Relative Error CDF (bias, %)',\n",
    "    s=70,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='o',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Plot Absolute Relative Error CDF\n",
    "ax.scatter(\n",
    "    abs_sorted, cdf_abs,\n",
    "    color=color_abs,\n",
    "    label='Absolute Relative Error CDF (%)',\n",
    "    s=25,\n",
    "    edgecolors='black',\n",
    "    linewidth=0.5,\n",
    "    marker='D',\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# Zero-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=0,\n",
    "    color=color_zero,\n",
    "    linestyle='--',\n",
    "    linewidth=1.8,\n",
    "    label='Zero Bias'\n",
    ")\n",
    "\n",
    "# Mean-Bias vertical line\n",
    "ax.axvline(\n",
    "    x=mean_bias,\n",
    "    color=color_mean,\n",
    "    linestyle='-.',\n",
    "    linewidth=2,\n",
    "    label=f'Mean Bias = {mean_bias:.2f} %'\n",
    ")\n",
    "\n",
    "# 68th-percentile vertical/horizontal lines\n",
    "ax.axvline(\n",
    "    x=p68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5,\n",
    "    label=f'|68%ile|: {p68:.2f} %'\n",
    ")\n",
    "ax.axhline(\n",
    "    y=0.68,\n",
    "    color=color_p68,\n",
    "    linestyle=':',\n",
    "    linewidth=2.5\n",
    ")\n",
    "\n",
    "# Axis limits and ticks\n",
    "xmin = np.floor(min(bias_sorted.min(), abs_sorted.min())) - 1\n",
    "xmax = np.ceil(max(bias_sorted.max(), abs_sorted.max(), p68)) + 1\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_xticks(np.arange(xmin, xmax + 1, 20))\n",
    "\n",
    "# Labels and title\n",
    "ax.set_xlabel('Relative Error (%)', fontsize=14)\n",
    "ax.set_ylabel('Cumulative Probability', fontsize=14)\n",
    "ax.set_title(\n",
    "    'CDF of Slope Relative Error, Mean Bias and 68%tile\\n',\n",
    "    fontsize=16,\n",
    "    weight='bold'\n",
    ")\n",
    "\n",
    "# Grid and legend\n",
    "ax.grid(True, which='major', linestyle='--', alpha=0.5)\n",
    "ax.legend(fontsize=12, loc='lower right', frameon=True, fancybox=True)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# --- Marginal KDE plot for bias (inset at top) ---\n",
    "# ------------------------------------------------------------------------------\n",
    "ax_kde = inset_axes(\n",
    "    ax,\n",
    "    width=\"100%\", height=\"20%\",\n",
    "    loc='upper center',\n",
    "    bbox_to_anchor=(0, 1.05, 1, 0.3),\n",
    "    bbox_transform=ax.transAxes,\n",
    "    borderpad=0\n",
    ")\n",
    "sns.kdeplot(\n",
    "    x=df_rel_unique['rel_error'],\n",
    "    ax=ax_kde,\n",
    "    fill=True,\n",
    "    linewidth=2,\n",
    "    alpha=0.3\n",
    ")\n",
    "ax_kde.set_xlim(xmin, xmax)\n",
    "ax_kde.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print metrics summary ---\n",
    "print(f\"Spearman correlation: {spearman_rho:.3f} (p={spearman_p:.2g})\")\n",
    "print(f\"Mean Bias (rel error): {mean_bias:.2f} %\")\n",
    "print(f\"Mean Absolute Relative Error: {mean_abs_rel_error:.2f} %\")\n",
    "print(f\"68th percentile abs rel error: {p68:.2f} %\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cefa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['bath_slope1']\n",
    "x  = reg_slope_bath_sort['p_dist_out']\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ figure setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "fig = plt.figure(figsize=(15,6), facecolor='white')\n",
    "gs  = GridSpec(1, 2, width_ratios=[5, 1.5], wspace=0.15)\n",
    "\n",
    "# vibrant palette\n",
    "c1, c2 = '#1f77b4', '#ff7f0e'\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ left: line vs. distance with thinner lines ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax_main = fig.add_subplot(gs[0])\n",
    "\n",
    "ax_main.plot(\n",
    "    x, s1,\n",
    "    color=c1,\n",
    "    linestyle='-',\n",
    "    linewidth=1.5,          # ‚Üê thinner line\n",
    "    marker='o',\n",
    "    markersize=5,           # ‚Üê small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c1,\n",
    "    markeredgewidth=1.0,\n",
    "    label='SWOT slope'\n",
    ")\n",
    "ax_main.plot(\n",
    "    x, s2,\n",
    "    color=c2,\n",
    "    linestyle='--',\n",
    "    linewidth=1.5,          # ‚Üê thinner line\n",
    "    marker='D',\n",
    "    markersize=5,           # ‚Üê small markers\n",
    "    markerfacecolor='white',\n",
    "    markeredgecolor=c2,\n",
    "    markeredgewidth=1.0,\n",
    "    label='Bathymetry slope'\n",
    ")\n",
    "\n",
    "# gentle fill under curves\n",
    "ax_main.fill_between(x, s1, alpha=0.1, color=c1)\n",
    "ax_main.fill_between(x, s2, alpha=0.1, color=c2)\n",
    "\n",
    "ax_main.set_xlabel('Distance to outlet (m)', fontsize=12, weight='bold')\n",
    "ax_main.set_ylabel('Slope value', fontsize=12, weight='bold')\n",
    "ax_main.set_title('Comparison between SWOT and Bathymetry Slopes', fontsize=16, weight='bold')\n",
    "ax_main.grid(alpha=0.3, linestyle='--')\n",
    "ax_main.invert_xaxis()\n",
    "ax_main.legend(loc='upper left', frameon=True, facecolor='white', edgecolor='black')\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ right: side boxplot sharing the same y-axis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "ax_box = fig.add_subplot(gs[1], sharey=ax_main)\n",
    "\n",
    "bp = ax_box.boxplot(\n",
    "    [s1, s2],\n",
    "    positions=[1,2],\n",
    "    widths=0.6,\n",
    "    patch_artist=True,\n",
    "    showfliers=False,\n",
    "    medianprops=dict(color='black'),\n",
    "    whiskerprops=dict(color='black'),\n",
    "    capprops=dict(color='black'),\n",
    ")\n",
    "\n",
    "# color the boxes with semi-transparent fill\n",
    "for patch, color in zip(bp['boxes'], [c1, c2]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.3)\n",
    "\n",
    "# overlay the raw points on top\n",
    "for i, (y, color) in enumerate(zip([s1, s2], [c1, c2]), start=1):\n",
    "    xi = np.random.normal(i, 0.04, size=len(y))\n",
    "    ax_box.scatter(\n",
    "        xi, y,\n",
    "        color=color,\n",
    "        alpha=0.6,\n",
    "        s=12,\n",
    "        edgecolors='none',\n",
    "        zorder=10\n",
    "    )\n",
    "\n",
    "ax_box.set_xticks([1,2])\n",
    "ax_box.set_xticklabels(['SWOT', 'Bathymetry'], fontsize=10, weight='bold')\n",
    "ax_box.margins(x=0.3)\n",
    "ax_box.yaxis.tick_right()\n",
    "ax_box.yaxis.set_label_position(\"right\")\n",
    "ax_box.grid(True, axis='y', linestyle='--', alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6817c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']\n",
    "s2 = reg_slope_bath_sort['bath_slope1']\n",
    "\n",
    "# set up the figure\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(s1, s2, \n",
    "            color='red',      # match your SWOT color\n",
    "            alpha=0.7, \n",
    "            edgecolors='none', \n",
    "            s=30)\n",
    "\n",
    "# 1:1 line\n",
    "lims = [\n",
    "    np.min([s1.min(), s2.min()]), \n",
    "    np.max([s1.max(), s2.max()])\n",
    "]\n",
    "plt.plot(lims, lims, \n",
    "        ls='--', \n",
    "        lw=1.5, \n",
    "        color='green', \n",
    "        label='1:1 line')\n",
    "\n",
    "margin = 0.001\n",
    "low, high = lims\n",
    "\n",
    "plt.xlim(low - margin, high + margin)\n",
    "plt.ylim(low - margin, high + margin)\n",
    "\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('SWOT vs. Bathymetry Slopes', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1934da",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = reg_slope_bath_sort['slope1']           # SWOT slope\n",
    "s2 = reg_slope_bath_sort['bath_slope1']      # Bathymetry slope\n",
    "\n",
    "# 1) Hexbin + 1:1 line\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.hexbin(s1, s2, gridsize=100, mincnt=1, cmap='Blues')  # no color specified ‚Üí default cmap\n",
    "lims = [min(s1.min(), s2.min()), max(s1.max(), s2.max())]\n",
    "plt.plot(lims, lims, ls='--', lw=1.5, color='green', label='1:1 line')\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Bathymetry slope', fontsize=12, weight='bold')\n",
    "plt.title('Hexbin: SWOT vs Bath Slopes', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 2) Bland‚ÄìAltman (Difference vs Average)\n",
    "diff = s1 - s2\n",
    "avg  = 0.5 * (s1 + s2)\n",
    "mean_diff = diff.mean()\n",
    "std_diff  = diff.std()\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(avg, diff, s=20, alpha=0.5)\n",
    "plt.axhline(mean_diff, color='red', linestyle='--', label='Mean diff')\n",
    "plt.axhline(mean_diff + 1.96*std_diff, color='gray', linestyle=':', label='¬±1.96œÉ')\n",
    "plt.axhline(mean_diff - 1.96*std_diff, color='gray', linestyle=':')\n",
    "plt.xlabel('Average slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Difference (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Bland‚ÄìAltman: Difference vs. Average', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# 3) Residuals vs SWOT slope\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(s1, diff, s=20, alpha=0.5)\n",
    "plt.axhline(0, color='black', linestyle='--', lw=1)\n",
    "plt.xlabel('SWOT slope', fontsize=12, weight='bold')\n",
    "plt.ylabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.title('Residuals vs. SWOT Slope', fontsize=14, weight='bold')\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 4) CDF of Absolute Residuals (|SWOT ‚Äì Bath|)\n",
    "abs_diff = np.abs(diff)\n",
    "abs_diff = abs_diff[np.isfinite(abs_diff)]\n",
    "xs = np.sort(abs_diff)\n",
    "cdf = np.arange(1, len(xs) + 1) / len(xs)\n",
    "\n",
    "p50 = np.percentile(abs_diff, 50)\n",
    "p68 = np.percentile(abs_diff, 68)\n",
    "p95 = np.percentile(abs_diff, 95)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(xs, cdf, lw=2)\n",
    "plt.axvline(p68, linestyle='--', label=f'68%tile = {p68:.4f}')\n",
    "plt.axvline(p50, linestyle=':',  label=f'Median = {p50:.4f}')\n",
    "plt.axvline(p95, linestyle=':',  label=f'95%tile = {p95:.4f}')\n",
    "plt.xlabel('|Residual| (|SWOT ‚Äì Bath|)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Cumulative Probability', fontsize=12, weight='bold')\n",
    "plt.title('CDF of Absolute Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "# (Alternative) Histogram of signed residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=30, density=True, alpha=0.7)\n",
    "plt.axvline(mean_diff, linestyle='--', label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Density', fontsize=12, weight='bold')\n",
    "plt.title('Residual Distribution', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Histogram of Residuals\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(diff[np.isfinite(diff)], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.axvline(0, color='black', linestyle='--', lw=1, label='Zero line')\n",
    "plt.axvline(mean_diff, color='red', linestyle='--', lw=1, label=f'Mean = {mean_diff:.4f}')\n",
    "plt.xlabel('Residual (SWOT ‚Äì Bath)', fontsize=12, weight='bold')\n",
    "plt.ylabel('Count', fontsize=12, weight='bold')\n",
    "plt.title('Histogram of Residuals', fontsize=14, weight='bold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Clean residuals\n",
    "diff_clean = np.asarray(diff[np.isfinite(diff)])\n",
    "\n",
    "# Stats\n",
    "q1, med, q3 = np.percentile(diff_clean, [25, 50, 75])\n",
    "mean_val    = diff_clean.mean()\n",
    "p68         = np.percentile(diff_clean, 68)\n",
    "\n",
    "# Keep comparable limits\n",
    "ymin = np.percentile(diff_clean, 1)\n",
    "ymax = np.percentile(diff_clean, 99)\n",
    "pad  = 0.02 * (ymax - ymin)\n",
    "ylim = (ymin - pad, ymax + pad)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "\n",
    "# Violin plot with same proportional look\n",
    "ax = sns.violinplot(\n",
    "    y=diff_clean,\n",
    "    inner='box',\n",
    "    color='lightblue',\n",
    "    cut=0,\n",
    "    scale='width',\n",
    "    bw=0.2,\n",
    "    linewidth=1.2\n",
    ")\n",
    "\n",
    "# Zero line\n",
    "ax.axhline(0, color='black', linestyle='--', lw=1)\n",
    "\n",
    "# Markers for stats\n",
    "ax.scatter(0, mean_val, s=60, marker='D', color='#d62728', zorder=5, label=f\"Mean = {mean_val:.4f}\")\n",
    "ax.scatter(0, med,      s=60, marker='o', facecolors='white', edgecolors='black', zorder=5, label=f\"Median = {med:.4f}\")\n",
    "ax.scatter(0, q1,       s=80, marker='_', color='#2ca02c', zorder=5, label=f\"Q1 = {q1:.4f}\")\n",
    "ax.scatter(0, q3,       s=80, marker='_', color='#1f77b4', zorder=5, label=f\"Q3 = {q3:.4f}\")\n",
    "ax.axhline(p68, color='purple', linestyle=':', lw=1.2, label=f\"68%tile = {p68:.4f}\")\n",
    "\n",
    "# Labels, limits, legend\n",
    "ax.set_ylim(ylim)\n",
    "ax.set_ylabel('Residual (SWOT - Bath)', fontsize=12, weight='bold')\n",
    "ax.set_title('Residual Distribution Around Zero', fontsize=14, weight='bold')\n",
    "ax.set_xticks([])\n",
    "\n",
    "ax.legend(loc='lower left', fontsize=9, frameon=True)  # legend in lower-left\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ce331",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a4f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "from matplotlib.lines import Line2D\n",
    "# -------------------------\n",
    "# Data\n",
    "# -------------------------\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Pee Dee up', 'Pee Dee down', 'Po', 'Garonne'],\n",
    "    'r_simple':  [0.31, 0.61, 0.44, -0.11, -0.25, 0.37, 0.39, 0.24],\n",
    "    'œÅ_simple':  [0.42, 0.62, 0.49, -0.25, -0.15, 0.63, 0.47, 0.34],\n",
    "    #'r_20':      [0.35, 0.49, 0.56, 0.55, np.nan, np.nan, 0.34, 0.11],\n",
    "    #'œÅ_20':      [0.44, 0.65, 0.57, 0.49, np.nan, np.nan, 0.36, 0.19],\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# -------------------------\n",
    "# Control which approaches to include\n",
    "# -------------------------\n",
    "approaches_to_plot = [\"simple\", \"20\"]\n",
    "\n",
    "# -------------------------\n",
    "# Reshape into long format\n",
    "# -------------------------\n",
    "suffixes = []\n",
    "for c in df.columns:\n",
    "    if c.startswith(\"r_\"):\n",
    "        suf = c[2:]\n",
    "        if f\"œÅ_{suf}\" in df.columns and suf in approaches_to_plot:\n",
    "            suffixes.append(suf)\n",
    "\n",
    "long_rows = []\n",
    "for s in suffixes:\n",
    "    tmp = pd.DataFrame({\n",
    "        'River': df['River'],\n",
    "        'Approach': s.replace('avrg', 'average'),\n",
    "        'r': df[f'r_{s}'],\n",
    "        'œÅ': df[f'œÅ_{s}'],\n",
    "    })\n",
    "    long_rows.append(tmp)\n",
    "\n",
    "df_long = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "# -------------------------\n",
    "# Assign unique color+marker per river\n",
    "# -------------------------\n",
    "colors = cycle(plt.cm.tab10.colors)   # use tab10 palette cycling\n",
    "markers = cycle(['o', 's', '^', 'D', 'P', 'X', 'v', '<', '>'])\n",
    "\n",
    "rivers = df['River'].unique()\n",
    "river_styles = {}\n",
    "for river, color, marker in zip(rivers, colors, markers):\n",
    "    river_styles[river] = {\"color\": color, \"marker\": marker}\n",
    "\n",
    "# -------------------------\n",
    "# Plot\n",
    "# -------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "\n",
    "for _, row in df_long.iterrows():\n",
    "    style = river_styles[row['River']]\n",
    "    ax.scatter(\n",
    "        row['r'],\n",
    "        row['œÅ'],\n",
    "        color=style[\"color\"],\n",
    "        marker=style[\"marker\"],\n",
    "        s=100,\n",
    "        edgecolor='black',\n",
    "        linewidth=0.7\n",
    "    )\n",
    "\n",
    "# Axes, style\n",
    "ax.set_xlabel('r', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('œÅ', fontsize=12, fontweight='bold')\n",
    "ax.set_title('r vs œÅ by River', fontsize=14, fontweight='bold')\n",
    "ax.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.axvline(0, color='gray', linestyle='--', linewidth=0.8)\n",
    "ax.set_xlim(-0.5, 1.05)\n",
    "ax.set_ylim(-0.5, 1.05)\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Legend (only rivers)\n",
    "river_handles = [\n",
    "    Line2D([0], [0], marker=style[\"marker\"], color='w',\n",
    "           label=river, markerfacecolor=style[\"color\"],\n",
    "           markeredgecolor='black', markersize=10)\n",
    "    for river, style in river_styles.items()\n",
    "]\n",
    "ax.legend(handles=river_handles, title=\"Rivers\", bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08988ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "threshold = 0.40\n",
    "outline_color = 'limegreen'\n",
    "dash_on, dash_off = 1.5, 2.0  # dotted look\n",
    "lw = 2\n",
    "\n",
    "# =========================\n",
    "# Data\n",
    "# =========================\n",
    "data = {\n",
    "    'River': ['Cape Fear', 'Atrato', 'Sacramento', 'Pee Dee', 'Po', 'Garonne'],\n",
    "    'r_40':  [0.44, 0.62, 0.64, 0.54, 0.21, 0.18],\n",
    "    'œÅ_40':  [0.42, 0.65, 0.67, 0.52, 0.30, 0.08],\n",
    "    'r_80':  [0.41, 0.83, 0.45, 0.1, 0.34, 0.24],\n",
    "    'œÅ_80':  [0.37, 0.70, 0.51, -0.08, 0.36, 0.06],\n",
    "}\n",
    "df = pd.DataFrame(data).set_index('River')\n",
    "df = df[['r_40', 'œÅ_40', 'r_80', 'œÅ_80']]\n",
    "\n",
    "# =========================\n",
    "# Heatmap\n",
    "# =========================\n",
    "vals = df.values\n",
    "vmax = np.nanmax(np.abs(vals))\n",
    "norm = TwoSlopeNorm(vmin=-vmax, vcenter=0.0, vmax=vmax)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.8, 5.0))\n",
    "im = ax.imshow(vals, aspect='auto', cmap='RdBu_r', norm=norm, origin='upper')\n",
    "\n",
    "ax.set_xticks(np.arange(df.shape[1]))\n",
    "ax.set_xticklabels(df.columns, fontsize=11, fontweight='bold')\n",
    "ax.set_yticks(np.arange(df.shape[0]))\n",
    "ax.set_yticklabels(df.index, fontsize=11)\n",
    "\n",
    "# annotate values\n",
    "for i in range(vals.shape[0]):\n",
    "    for j in range(vals.shape[1]):\n",
    "        ax.text(j, i, f\"{vals[i, j]:.2f}\",\n",
    "                ha='center', va='center', fontsize=9,\n",
    "                color='black' if abs(vals[i, j]) < vmax*0.55 else 'white')\n",
    "\n",
    "ax.set_title('Correlation Heatmap (centered at 0)', fontsize=13, fontweight='bold', pad=10)\n",
    "ax.set_xlabel('metric@Segment Width', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel('River', fontsize=11, fontweight='bold')\n",
    "ax.set_frame_on(False)\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, shrink=0.9, pad=0.02)\n",
    "cbar.set_label('Correlation', fontsize=11, fontweight='bold')\n",
    "\n",
    "# =========================\n",
    "# Boxy perimeter for r > threshold OR œÅ > threshold\n",
    "# =========================\n",
    "mask = vals > threshold  # includes both Pearson and Spearman\n",
    "\n",
    "rows, cols = mask.shape\n",
    "edges = set()\n",
    "\n",
    "def add_edge(p1, p2):\n",
    "    edges.add((p1, p2) if p1 < p2 else (p2, p1))\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        if not mask[i, j]:\n",
    "            continue\n",
    "        x0, x1 = j - 0.5, j + 0.5\n",
    "        y0, y1 = i - 0.5, i + 0.5\n",
    "        if i == 0 or not mask[i-1, j]:      # top\n",
    "            add_edge((x0, y0), (x1, y0))\n",
    "        if i == rows-1 or not mask[i+1, j]: # bottom\n",
    "            add_edge((x0, y1), (x1, y1))\n",
    "        if j == 0 or not mask[i, j-1]:      # left\n",
    "            add_edge((x0, y0), (x0, y1))\n",
    "        if j == cols-1 or not mask[i, j+1]: # right\n",
    "            add_edge((x1, y0), (x1, y1))\n",
    "\n",
    "segments = [list(edge) for edge in edges]\n",
    "\n",
    "if segments:\n",
    "    lc = LineCollection(\n",
    "        segments,\n",
    "        colors=outline_color,\n",
    "        linewidths=3,\n",
    "        linestyles=(0, (dash_on, dash_off)),\n",
    "        capstyle='butt',\n",
    "        joinstyle='miter',\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.add_collection(lc)\n",
    "\n",
    "# keep full pixel grid visible\n",
    "ax.set_xlim(-0.5, cols - 0.5)\n",
    "ax.set_ylim(rows - 0.5, -0.5)\n",
    "\n",
    "# =========================\n",
    "# Legend outside the axes\n",
    "# =========================\n",
    "outline_handle = Line2D([0], [0], color=outline_color, lw=lw, linestyle=(0, (dash_on, dash_off)))\n",
    "fig.legend([outline_handle], [f'Pearson or Spearman > {threshold:.2f}'],\n",
    "           loc='lower right', bbox_to_anchor=(0.98, -0.05), frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68919f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Data (keep all approaches; lengths may differ)\n",
    "# -------------------------\n",
    "data = {\n",
    "    'River':  ['Cape Fear','Atrato','Sacramento','Pee Dee', 'Pee Dee up', 'Pee Dee down', 'Po','Garonne'],\n",
    "    'bias_simple':[-44.4, 20.4, -16.3, -11.9, 18.7, -72.9, 12.0, -80.0],\n",
    "    'p68_simple': [75.6, 51.3,  53.8,  76.5, 74.4, 77.3, 49.0,  88.7],\n",
    "    #'bias_avrg': [-54.6, 19.6, -26.8, -26.2,  13.7, -81.2],          # shorter\n",
    "    #'p68_avrg':  [78.4, 47.9,  41.5,  70.6,  58.5,  89.2],           # shorter\n",
    "    #'bias_20':   [-71.0, 54.3, -26.7, -38.6, 0.12, -83.1, np.nan, np.nan],\n",
    "    #'p68_20':    [81.9, 95.1,  69.5,  56.0,  59.9, 89.6, np.nan, np.nan],\n",
    "}\n",
    "\n",
    "# Build base df and safely align any shorter columns\n",
    "df = pd.DataFrame({'River': data['River']})\n",
    "for k, v in data.items():\n",
    "    if k == 'River': \n",
    "        continue\n",
    "    s = pd.Series(v)\n",
    "    df[k] = s.reindex(range(len(df))).values  # pad with NaN to match length\n",
    "\n",
    "# -------------------------\n",
    "# Choose which approaches to include (ignore 'avrg')\n",
    "# -------------------------\n",
    "INCLUDE = ['simple', '20']   # <- change here to toggle approaches\n",
    "# INCLUDE = ['simple']       # e.g., only simple\n",
    "# INCLUDE = ['simple', 'avrg', '20']  # to include all\n",
    "\n",
    "# -------------------------\n",
    "# Auto-detect suffixes, then filter by INCLUDE\n",
    "# -------------------------\n",
    "suffixes = []\n",
    "for c in df.columns:\n",
    "    if c.startswith('bias_'):\n",
    "        suf = c[5:]\n",
    "        if f'p68_{suf}' in df.columns:\n",
    "            suffixes.append(suf)\n",
    "\n",
    "if INCLUDE is not None:\n",
    "    suffixes = [s for s in suffixes if s in INCLUDE]\n",
    "\n",
    "# -------------------------\n",
    "# Long format for plotting\n",
    "# -------------------------\n",
    "long_rows = []\n",
    "for s in suffixes:\n",
    "    tmp = pd.DataFrame({\n",
    "        'River': df['River'],\n",
    "        'Approach': s.replace('avrg', 'average'),\n",
    "        'Bias': df[f'bias_{s}'],\n",
    "        'P68':  df[f'p68_{s}'],\n",
    "    })\n",
    "    long_rows.append(tmp)\n",
    "\n",
    "long = pd.concat(long_rows, ignore_index=True)\n",
    "\n",
    "# Drop rows that are all-NaN for the selected approach\n",
    "long = long.dropna(subset=['Bias', 'P68'], how='all')\n",
    "\n",
    "# -------------------------\n",
    "# Assign unique color + marker per river (no approach legend)\n",
    "# -------------------------\n",
    "rivers = df['River'].unique()\n",
    "colors = cycle(plt.cm.tab10.colors)\n",
    "markers = cycle(['o','s','^','D','P','X','v','<','>'])\n",
    "\n",
    "river_styles = {}\n",
    "for river, color, marker in zip(rivers, colors, markers):\n",
    "    river_styles[river] = {'color': color, 'marker': marker}\n",
    "\n",
    "# -------------------------\n",
    "# Plot\n",
    "# -------------------------\n",
    "fig, ax = plt.subplots(figsize=(8.6, 6.4))\n",
    "\n",
    "for _, row in long.iterrows():\n",
    "    st = river_styles[row['River']]\n",
    "    ax.scatter(\n",
    "        row['Bias'], row['P68'],\n",
    "        s=120, alpha=0.9,\n",
    "        color=st['color'],\n",
    "        marker=st['marker'],\n",
    "        edgecolor='black', linewidth=0.7\n",
    "    )\n",
    "\n",
    "# Axes & style\n",
    "ax.axvline(0, linestyle='--', linewidth=0.8, color='gray')\n",
    "ax.set_xlabel('Bias (%)', fontweight='bold')\n",
    "ax.set_ylabel('68th Percentile Error (%)', fontweight='bold')\n",
    "ax.set_title('Bias vs 68%ile Error ‚Äî by River', fontweight='bold')\n",
    "ax.grid(True, linestyle=':', alpha=0.35)\n",
    "\n",
    "# Legend: only rivers\n",
    "river_handles = [\n",
    "    Line2D([0], [0], marker=st['marker'], color='w', label=river,\n",
    "           markerfacecolor=st['color'], markeredgecolor='black', markersize=10)\n",
    "    for river, st in river_styles.items()\n",
    "]\n",
    "ax.legend(handles=river_handles, title='Rivers',\n",
    "          bbox_to_anchor=(1.02, 1), loc='upper left', frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304275ab",
   "metadata": {},
   "source": [
    "## Width variability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e40ed6",
   "metadata": {},
   "source": [
    "Testing how strongly width variabilty affects errors, bias and correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48386aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_swot = pd.read_csv(swot_simple)\n",
    "to_exclude = deleted_nodes['node_id'].unique().tolist()\n",
    "df_swot = df_swot[~df_swot['node_id'].isin(to_exclude)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a538f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bath = pd.read_csv(path2w_wse)\n",
    "to_exclude_bath = deleted_nodes['node_id'].unique().tolist()\n",
    "df_bath = df_bath[~df_bath['node_id'].isin(to_exclude_bath)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a8b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coefvar(dframe):\n",
    "    cv_width = dframe.groupby('node_id')['width'].agg(lambda x: x.std()/x.mean())\n",
    "    return cv_width.reset_index().rename(columns={\"width\": \"CV_width\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = coefvar(df_bath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369dad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a658d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f1b0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_cv = cv['CV_width'].mean()\n",
    "mean_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_rivers ={\n",
    "    'River':  [\"Cape Fear\", \"Atrato\", \"Sacramento\", \"Pee Dee\",\"Pee Dee up\", \"Pee Dee down\", \"Po\", \"Garonne\"],\n",
    "    'cv_Bath_group':      [0.16, 0.42, 0.30, 0.29, 0.09, 0.95, 0.20, 0.07],\n",
    "    #'cv_Bath_all':        [0.33, 0.55, 0.39, 0.42, 0.31, 0.11],\n",
    "    'cv_swot_group':      [0.22, 0.26, 0.20, 0.42, 0.09, 0.87, 0.17, 0.27],\n",
    "    #'cv_swot_all':      [0.33, 0.37, 0.35, 0.86, 0.31, 0.33],\n",
    "    'œÅ_simple':       [0.42, 0.62, 0.49, -0.25, -0.15, 0.63, 0.47, 0.34],\n",
    "    #'œÅ_avrg':       [0.31, 0.74, 0.67, 0.06,0.39, 0.23],\n",
    "    #'œÅ_20%':       [0.44, 0.54, 0.57, 0.49, 0.36, 0.19],\n",
    "    #'œÅ_23%':       [0.44, 0.60 ,0.53, 0.33, 0.38, 0.19],\n",
    "    #'œÅ_25%':       [0.44, 0.59, 0.57, 0.29, 0.39, 0.19],\n",
    "    #'œÅ_30%':       [0.44, 0.63, 0.69, 0.17, 0.39,0.19],\n",
    "    'bias(%) simple': [-44.4, 20.4, -16.3, -12.6, 18.7, -72.9, 12.0, -80.0],\n",
    "    #'bias(%) avrg': [-54.6, 19.6, -26.8, -26.2, 13.7, -81.2],\n",
    "    #'bias(%) 20%': [-71.0, 54.3, -26.7, -34.9, 0.12, -83.11],\n",
    "    #'bias(%) 23%': [-70.95, 32.19, -31.52, -25.26, 1.77, -82.62],\n",
    "    #'bias(%) 25%': [-70.95, 43.42, -30.00, -13.78, 1.60, -82.62],\n",
    "    #'bias(%) 30%': [-70.95, 27.68, -28.89, -3.96, 14.07, -82.62],\n",
    "    '68\"%\"ile simple': [75.6, 51.3, 53.8, 76.8, 74.4, 77.3, 49.0, 88.7],\n",
    "    #'68\"%\"ile avrg': [78.4, 47.9, 41.5, 70.6, 58.5, 89.2],\n",
    "    #'68\"%\"ile 20%': [81.9, 95.1, 69.5, 61.0, 60.0, 89.6],\n",
    "    #'68\"%\"ile 23%': [81.91, 73.08, 58.52, 58.14, 56.04, 89.63],\n",
    "    #'68\"%\"ile 25%': [81.91, 91.57, 56.98, 73.22, 52.98, 89.63],\n",
    "    #'68\"%\"ile 30%': [81.91, 81.39, 53.38, 76.25, 58.63, 89.63],\n",
    "}\n",
    "\n",
    "summary_cv = pd.DataFrame(CV_rivers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2917d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "import yaml\n",
    "import re, fnmatch\n",
    "\n",
    "# --- column normalization (accept your original headers) ---\n",
    "RENAME_MAP = {\n",
    "    'œÅ_simple': 'rho_simple',\n",
    "    'œÅ_20%': 'rho_20pct',\n",
    "    'bias(%) simple': 'bias_simple_pct',\n",
    "    'bias(%) 20%': 'bias_20_pct',\n",
    "    '68\"%\"ile simple': 'p68_simple_pct',\n",
    "    '68\"%\"ile 20%': 'p68_20_pct',\n",
    "}\n",
    "\n",
    "def load_data(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df.rename(columns=RENAME_MAP)\n",
    "    # keep everything; just sanity-check key CV columns exist\n",
    "    expected_any = {'cv_swot_group'}\n",
    "    missing = [c for c in expected_any if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "def load_config(yaml_path: str|None=None, fallback: dict|None=None) -> dict:\n",
    "    if yaml_path:\n",
    "        with open(yaml_path, 'r') as f:\n",
    "            cfg = yaml.safe_load(f)\n",
    "    else:\n",
    "        cfg = fallback or {}\n",
    "    # Defaults (unchanged behavior unless you set them in YAML)\n",
    "    cfg.setdefault('x_col', 'cv_swot_group')\n",
    "    cfg.setdefault('y_cols', ['rho_simple','rho_20pct'])\n",
    "    cfg.setdefault('percent_axis', True)\n",
    "    cfg.setdefault('label_offset', {})\n",
    "    cfg.setdefault('x_max', 1.8)\n",
    "    cfg.setdefault('y_mode', 'auto')                     # 'auto' | 'percent' | 'fraction'\n",
    "    cfg.setdefault('fraction_as_percent_labels', False)\n",
    "    # NEW: optional filter for plotting only specific rivers (keeps the rest of the data)\n",
    "    cfg.setdefault('include_rivers', None)               # e.g., ['Pee Dee up','Pee Dee down']\n",
    "    return cfg\n",
    "\n",
    "def expand_y_cols(df: pd.DataFrame, patterns) -> list[str]:\n",
    "    cols = set()\n",
    "    for pat in patterns:\n",
    "        if isinstance(pat, str) and pat.startswith('/') and pat.endswith('/'):\n",
    "            rx = re.compile(pat[1:-1])\n",
    "            cols |= {c for c in df.columns if rx.search(c)}\n",
    "        else:\n",
    "            cols |= set(fnmatch.filter(df.columns, pat))\n",
    "    return [c for c in df.columns if c in cols]\n",
    "\n",
    "def is_bias(col: str) -> bool:\n",
    "    return 'bias' in col\n",
    "\n",
    "def _classify(col: str) -> str:\n",
    "    \"\"\"\n",
    "    Return one of: 'bias_pct' (‚àí100..100), 'p68_pct' (0..100), 'fraction' (‚àí0.3..1)\n",
    "    \"\"\"\n",
    "    if col.startswith('bias_') and col.endswith('_pct'):\n",
    "        return 'bias_pct'\n",
    "    if col.startswith('p68_') and col.endswith('_pct'):\n",
    "        return 'p68_pct'\n",
    "    if col.startswith('rho_'):\n",
    "        return 'fraction'\n",
    "    return 'bias_pct' if (is_bias(col) or col.endswith('_pct')) else 'fraction'\n",
    "\n",
    "def plot_panels(df: pd.DataFrame, cfg: dict):\n",
    "    X_COL = cfg['x_col']\n",
    "    y_cols = expand_y_cols(df, cfg['y_cols'])\n",
    "    PERCENT = bool(cfg['percent_axis'])\n",
    "    LABEL_OFFSET = cfg['label_offset']\n",
    "    x_max = float(cfg.get('x_max', 1.8))\n",
    "    Y_MODE = cfg.get('y_mode', 'auto')\n",
    "    FRAC_AS_PCT = bool(cfg.get('fraction_as_percent_labels', False))\n",
    "    INCLUDE_RIVERS = cfg.get('include_rivers', None)\n",
    "\n",
    "    # --- NEW: filter only for plotting (does not modify the original df) ---\n",
    "    df_plot = df if not INCLUDE_RIVERS else df[df['River'].isin(INCLUDE_RIVERS)].copy()\n",
    "    if df_plot.empty:\n",
    "        raise ValueError(\"No rows to plot after applying include_rivers filter.\")\n",
    "    for c in [X_COL, 'River', *y_cols]:\n",
    "        if c not in df_plot.columns:\n",
    "            raise KeyError(f\"Column '{c}' not found after normalization.\")\n",
    "\n",
    "    # prepare figure with up to 2 panels (as your current code)\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for ax, col in zip(axes, y_cols):\n",
    "        x = df_plot[X_COL]\n",
    "        y = df_plot[col]\n",
    "\n",
    "        ax.scatter(x, y, s=100, zorder=2)\n",
    "\n",
    "        # River labels\n",
    "        for xi, yi, name in zip(x, y, df_plot['River']):\n",
    "            dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "            ax.annotate(\n",
    "                name, xy=(xi, yi), xytext=(dx, dy), textcoords='offset points',\n",
    "                ha='left' if dx >= 0 else 'right', va='center', fontsize=12,\n",
    "                arrowprops=dict(arrowstyle='-', lw=0.7, alpha=0.7),\n",
    "                zorder=0, clip_on=False\n",
    "            )\n",
    "\n",
    "        ax.set_xlim(0, x_max)\n",
    "\n",
    "        # ---- Y axis behavior (unchanged) ----\n",
    "        if Y_MODE == 'percent':\n",
    "            ax.set_ylim(-100, 100)\n",
    "            ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "            ax.yaxis.set_major_formatter(PercentFormatter(xmax=100))\n",
    "\n",
    "        elif Y_MODE == 'fraction':\n",
    "            ax.set_ylim(-0.3, 1.0)\n",
    "            ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "            if FRAC_AS_PCT:\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0))\n",
    "            else:\n",
    "                ax.yaxis.set_major_formatter('{x:.2f}')\n",
    "\n",
    "        else:\n",
    "            kind = _classify(col)\n",
    "            if kind == 'bias_pct':\n",
    "                ax.set_ylim(-100, 100)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=100) if PERCENT else '{x:.0f}')\n",
    "            elif kind == 'p68_pct':\n",
    "                ax.set_ylim(0, 100)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(10))\n",
    "                ax.yaxis.set_major_formatter(PercentFormatter(xmax=100) if PERCENT else '{x:.0f}')\n",
    "            else:\n",
    "                ax.set_ylim(-0.3, 1.0)\n",
    "                ax.yaxis.set_major_locator(MultipleLocator(0.2))\n",
    "                if FRAC_AS_PCT or PERCENT:\n",
    "                    ax.yaxis.set_major_formatter(PercentFormatter(xmax=1.0))\n",
    "                else:\n",
    "                    ax.yaxis.set_major_formatter('{x:.2f}')\n",
    "\n",
    "        ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "        ax.set_xlabel('CV', fontsize=14)\n",
    "        ax.set_ylabel(col, fontsize=14)\n",
    "\n",
    "        ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "        ax.minorticks_on()\n",
    "        ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "        # Stats (on visible points)\n",
    "        try:\n",
    "            r_s, _ = spearmanr(x, y)\n",
    "            r_p, _ = pearsonr(x, y)\n",
    "            ax.text(0.95, 0.05, f\"œÅ = {r_s:.2f}\",\n",
    "                    transform=ax.transAxes, ha='right', va='bottom',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "            ax.text(0.95, 0.05, f\"r = {r_p:.2f}\",\n",
    "                    transform=ax.transAxes, ha='right', va='top',\n",
    "                    fontsize=10, fontweight='bold')\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Hide unused panels if fewer y_cols\n",
    "    for ax in axes[len(y_cols):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d46f0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rivers = load_data(INTERMEDIATE/\"Store/Figures/cv_rivers.csv\")\n",
    "cfg = load_config(INTERMEDIATE/\"STORE/Figures/plot_config.yaml\")  # or load_config(fallback={\"y_cols\":[\"rho_*\"]})\n",
    "plot_panels(df_rivers, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b9f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'River':  [\"Cape Fear\", \"Atrato\", \"Sacramento\", \"Pee Dee Down\", \"Po\", \"Garonne\"],\n",
    "    'cv_swot_group':      [0.22, 0.26, 0.20, 0.87, 0.17, 0.27],\n",
    "    'cv_Bath_group':      [0.16, 0.42, 0.30, 0.95, 0.20, 0.07],\n",
    "    'cv_swot_all':      [0.33, 0.37, 0.35, 0.86, 0.31, 0.33],\n",
    "    'cv_Bath_all':        [0.33, 0.55, 0.39, 0.42, 0.31, 0.11]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Scatter plot ---\n",
    "fig, ax = plt.subplots(figsize=(7, 6))\n",
    "ax.scatter(df['cv_swot_group'], df['cv_Bath_group'], s=100, zorder=2)\n",
    "\n",
    "# Add river names as labels\n",
    "for i, row in df.iterrows():\n",
    "    ax.text(row['cv_swot_group'] + 0.01,  # small x-offset\n",
    "            row['cv_Bath_group'] + 0.01,  # use Bath_group to match y-axis\n",
    "            row['River'],\n",
    "            fontsize=10, va='center', ha='left')\n",
    "\n",
    "# Axis labels\n",
    "ax.set_xlabel(\"CV SWOT group\", fontsize=14)\n",
    "ax.set_ylabel(\"CV Bath group\", fontsize=14)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# Correlation stats (between same x and y you plotted)\n",
    "rho_s, p_s = spearmanr(df['cv_swot_group'], df['cv_Bath_group'])\n",
    "rho_p, p_p = pearsonr(df['cv_swot_group'], df['cv_Bath_group'])\n",
    "\n",
    "ax.text(0.95, 0.05, f\"œÅ = {rho_s:.2f}\\nr = {rho_p:.2f}\",\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e378d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "x = summary_cv['cv_Bath_group']\n",
    "y = summary_cv['cv_swot_group']\n",
    "\n",
    "# Scatter\n",
    "ax.scatter(x, y, s=100, zorder=2)\n",
    "\n",
    "# River labels\n",
    "for xi, yi, name in zip(x, y, summary_cv['River']):\n",
    "    dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "    ax.annotate(\n",
    "        name, xy=(xi, yi), xytext=(dx, dy),\n",
    "        textcoords='offset points',\n",
    "        ha='left' if dx >= 0 else 'right',\n",
    "        va='center', fontsize=12,\n",
    "        arrowprops=dict(arrowstyle='-', lw=0.7, alpha=0.7),\n",
    "        zorder=0, clip_on=False\n",
    "    )\n",
    "\n",
    "# Axes\n",
    "ax.set_xlim(0, 0.5)\n",
    "ax.set_ylim(0, 0.5)\n",
    "ax.set_xlabel(\"CV Bathymetry Group\", fontsize=14)\n",
    "ax.set_ylabel(\"CV SWOT Group\", fontsize=14)\n",
    "ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "# Grid\n",
    "ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "ax.minorticks_on()\n",
    "ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "# Correlation between them\n",
    "r, _ = spearmanr(x, y)\n",
    "ax.text(0.95, 0.05, f\"œÅ = {r:.2f}\",\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0886d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Column selection & label offsets\n",
    "# -----------------------------\n",
    "bias_cols = summary_cv.filter(regex=r'^bias\\(%\\)\\s*\\d+%$').columns.tolist()\n",
    "\n",
    "LABEL_OFFSET = {\n",
    "    \"Cape Fear\": (-10, 10),\n",
    "    \"Po\":        ( 10, 10),\n",
    "    \"Sacramento\":( 10, 10),\n",
    "    \"Atrato\":    ( 10, 10),\n",
    "    \"Pee Dee\":   (-10, 10),\n",
    "    \"Garonne\":   ( 10, -4),\n",
    "}\n",
    "\n",
    "# X series to compare (name, legend label, marker)\n",
    "x_cols = [\n",
    "    ('cv_swot_all',   'SWOT all',   'o'),\n",
    "    ('cv_swot_group', 'SWOT group', 's'),\n",
    "    ('cv_Bath_all',   'Bath all',   '^'),\n",
    "    ('cv_Bath_group', 'Bath group', 'D'),\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Plot\n",
    "# -----------------------------\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, bias_cols):\n",
    "    y = summary_cv[col]\n",
    "\n",
    "    ann_lines = []  # lines for correlation textbox\n",
    "\n",
    "    # Plot each CV series\n",
    "    for j, (xname, label, marker) in enumerate(x_cols):\n",
    "        x = summary_cv[xname]\n",
    "        ax.scatter(x, y, s=90, marker=marker, label=label, zorder=2)\n",
    "\n",
    "        # Correlations for this series\n",
    "        rs, _ = spearmanr(x, y)\n",
    "        rp, _ = pearsonr(x, y)\n",
    "        ann_lines.append(f\"{label}: œÅ={rs:.2f}, r={rp:.2f}\")\n",
    "\n",
    "        # Add river labels only once (first series) to avoid 4x duplicates\n",
    "        if j == 0:\n",
    "            for xi, yi, name in zip(x, y, summary_cv['River']):\n",
    "                dx, dy = LABEL_OFFSET.get(name, (10, 6))\n",
    "                ax.annotate(\n",
    "                    name, xy=(xi, yi), xytext=(dx, dy),\n",
    "                    textcoords='offset points',\n",
    "                    ha='left' if dx >= 0 else 'right',\n",
    "                    va='center', fontsize=10,\n",
    "                    arrowprops=dict(arrowstyle='-', lw=0.6, alpha=0.6),\n",
    "                    zorder=0, clip_on=False\n",
    "                )\n",
    "\n",
    "    # Axes styling\n",
    "    ax.set_xlim(0, 1.2)  # up to 1.18 in cv_swot_all\n",
    "    ax.set_ylim(-100, 100)\n",
    "    ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    ax.yaxis.set_major_formatter(PercentFormatter(xmax=100, decimals=0))  # show -100%..100%\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
    "    ax.set_xlabel('CV', fontsize=14)\n",
    "    ax.set_ylabel(col, fontsize=14)\n",
    "\n",
    "    # Grid\n",
    "    ax.grid(True, which='major', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    ax.minorticks_on()\n",
    "    ax.grid(True, which='minor', linestyle=':', linewidth=0.6, alpha=0.35)\n",
    "\n",
    "    # Correlation textbox (bottom-right)\n",
    "    ax.text(\n",
    "        0.98, 0.02, \"\\n\".join(ann_lines),\n",
    "        transform=ax.transAxes, ha='right', va='bottom',\n",
    "        fontsize=9, fontweight='bold',\n",
    "        zorder=10, clip_on=False,\n",
    "        bbox=dict(boxstyle='round,pad=0.2', facecolor='white', alpha=0.6)\n",
    "    )\n",
    "\n",
    "    # Legend\n",
    "    ax.legend(loc='upper left', fontsize=9, frameon=True, framealpha=0.8)\n",
    "\n",
    "# Hide unused panels if any (not needed here but safe)\n",
    "for ax in axes[len(bias_cols):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dcef84",
   "metadata": {},
   "source": [
    "### What about XTRK_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a0fedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = pd.read_csv(swot_simple)\n",
    "simple = simple.dropna(subset=['node_id', 'xtrk_dist', 'spearman_corr']).copy()\n",
    "simple['xtrk_dist'] = pd.to_numeric(simple['xtrk_dist'], errors='coerce')\n",
    "simple['spearman_corr'] = pd.to_numeric(simple['spearman_corr'], errors='coerce')\n",
    "simple = simple.dropna(subset=['xtrk_dist', 'spearman_corr'])\n",
    "\n",
    "# --- One row per node_id: mean xtrk_dist + first spearman_corr (by current row order) ---\n",
    "agg = (\n",
    "    simple\n",
    "    .groupby('node_id', as_index=False)\n",
    "    .agg(\n",
    "        xtrk_dist_mean=('xtrk_dist', 'mean'),\n",
    "        spearman_corr_first=('spearman_corr', 'first')\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Scatter plot (points only) ---\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.scatter(agg['xtrk_dist_mean'], agg['spearman_corr_first'], s=60, alpha=0.7, edgecolor='k')\n",
    "\n",
    "plt.xlabel(\"Mean Cross-track Distance by node_id\", fontsize=12)\n",
    "plt.ylabel(\"Spearman Correlation (first per node_id)\", fontsize=12)\n",
    "plt.title(\"Spearman Correlation vs Mean Cross-track Distance\", fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aca3988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_xtrk(dframe):\n",
    "    mean_dist = dframe.groupby('node_id')['xtrk_dist'].mean()\n",
    "    return mean_dist.reset_index().rename(columns={\"xtrk_dist\": \"mean_xtrk_dist\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025fc122",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrk = mean_xtrk(simple)\n",
    "xtrk['mean_xtrk_dist'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba2fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04540af",
   "metadata": {},
   "source": [
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "## ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0b706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "def plot_river_grid(df, river_order=None, suptitle=\"r vs œÅ across rivers\"):\n",
    "    required = {\"river\", \"setting\", \"r\", \"rho\", \"r_p\", \"rho_p\"}\n",
    "    if missing := required - set(df.columns):\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in pd.unique(df[\"river\"]) if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Color map per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global axis limits\n",
    "    xmin = min(0.0, df[\"r\"].min()) - 0.02\n",
    "    xmax = max(1.0, df[\"r\"].max()) + 0.02\n",
    "    ymin = min(0.0, df[\"rho\"].min()) - 0.02\n",
    "    ymax = max(1.0, df[\"rho\"].max()) + 0.02\n",
    "\n",
    "    # Grid\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            for _, row in chunk.iterrows():\n",
    "                nonsig = (row[\"r_p\"] > 0.05) or (row[\"rho_p\"] > 0.05)\n",
    "                marker = \"x\" if nonsig else \"o\"\n",
    "                kwargs = dict(s=70, marker=marker)\n",
    "                if nonsig:\n",
    "                    kwargs.update(color=col, linewidth=1.0, alpha=0.95)\n",
    "                else:\n",
    "                    kwargs.update(facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95)\n",
    "                ax.scatter(row[\"r\"], row[\"rho\"], **kwargs)\n",
    "\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=0.6, alpha=0.4)\n",
    "        ax.axhline(0, lw=0.6, alpha=0.4)\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"œÅ (Spearman)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"r (Pearson)\")\n",
    "\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Legends outside\n",
    "    settings_legend = fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Segment width\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(5, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)   # OUTSIDE bottom\n",
    "    )\n",
    "\n",
    "    signif_handles = [\n",
    "        Line2D([], [], marker='o', linestyle='None',\n",
    "               markerfacecolor='white', markeredgecolor='black',\n",
    "               markersize=7, markeredgewidth=0.8, label='p ‚â§ 0.05 (significant)'),\n",
    "        Line2D([], [], marker='x', linestyle='None',\n",
    "               color='black', markersize=7, label='p > 0.05 (not significant)')\n",
    "    ]\n",
    "    fig.legend(\n",
    "        handles=signif_handles,\n",
    "        loc=\"center left\",\n",
    "        frameon=True,\n",
    "        title=\"Significance\",\n",
    "        bbox_to_anchor=(1.01, 0.5)   # OUTSIDE right\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.9, 0.95])  # leave room\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d6381d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"r\":0.44,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.05},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"r\":0.41,\"r_p\":0.06,\"rho\":0.37,\"rho_p\":0.09},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.02,\"rho\":0.47,\"rho_p\":0.04},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"r\":0.44,\"r_p\":0.06,\"rho\":0.38,\"rho_p\":0.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"r\":0.35,\"r_p\":0.15,\"rho\":0.44,\"rho_p\":0.06},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"r\":0.62,\"r_p\":2.5e-4,\"rho\":0.65,\"rho_p\":9.3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"r\":0.83,\"r_p\":4.3e-8,\"rho\":0.70,\"rho_p\":3e-5},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"r\":0.35,\"r_p\":0.07,\"rho\":0.56,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"r\":0.60,\"r_p\":0.002,\"rho\":0.59,\"rho_p\":0.002},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"r\":0.49,\"r_p\":0.009,\"rho\":0.54,\"rho_p\":0.003},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"23%\",\"r\":0.51,\"r_p\":0.007,\"rho\":0.60,\"rho_p\":0.001},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"r\":0.46,\"r_p\":0.013,\"rho\":0.59,\"rho_p\":0.001},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"r\":0.55,\"r_p\":0.003,\"rho\":0.63,\"rho_p\":4.7e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"r\":0.64,\"r_p\":4.6e-6,\"rho\":0.67,\"rho_p\":1e-6},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"r\":0.45,\"r_p\":0.003,\"rho\":0.51,\"rho_p\":5e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"r\":0.37,\"r_p\":0.02,\"rho\":0.35,\"rho_p\":0.02},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"r\":0.46,\"r_p\":0.002,\"rho\":0.44,\"rho_p\":0.004},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"r\":0.56,\"r_p\":1.2e-4,\"rho\":0.57,\"rho_p\":7.1e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"r\":0.54,\"r_p\":2.6e-4,\"rho\":0.53,\"rho_p\":4.2e-4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"r\":0.58,\"r_p\":4e-5,\"rho\":0.57,\"rho_p\":7.5e-5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"r\":0.65,\"r_p\":2.5e-6,\"rho\":0.69,\"rho_p\":3.8e-7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"r\":0.54,\"r_p\":1.7e-4,\"rho\":0.52,\"rho_p\":3.4e-4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"r\":0.10,\"r_p\":0.52,\"rho\":-0.08,\"rho_p\":0.59},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"r\":0.63,\"r_p\":3.3e-6,\"rho\":0.7,\"rho_p\":9.7e-8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"r\":0.69,\"r_p\":4.1e-7,\"rho\":0.63,\"rho_p\":6.6e-6},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"r\":0.65,\"r_p\":2.4e-6,\"rho\":0.57,\"rho_p\":6.8e-5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"r\":0.41,\"r_p\":0.006,\"rho\":0.33,\"rho_p\":0.03},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"r\":0.32,\"r_p\":0.03,\"rho\":0.29,\"rho_p\":0.056},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"r\":0.14,\"r_p\":0.4,\"rho\":0.17,\"rho_p\":0.25},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"r\":0.21,\"r_p\":0.004,\"rho\":0.3,\"rho_p\":2.8e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"r\":0.34,\"r_p\":1.6e-6,\"rho\":0.36,\"rho_p\":3.6e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"r\":0.2,\"r_p\":0.005,\"rho\":0.29,\"rho_p\":4e-5},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"r\":0.28,\"r_p\":6.4e-5,\"rho\":0.32,\"rho_p\":4.8e-6},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"r\":0.34,\"r_p\":1.3e-6,\"rho\":0.36,\"rho_p\":3.2e-7},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"r\":0.36,\"r_p\":5e-7,\"rho\":0.38,\"rho_p\":9.4e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"r\":0.36,\"r_p\":2.7e-7,\"rho\":0.39,\"rho_p\":2.3e-8},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"r\":0.34,\"r_p\":2.1e-6,\"rho\":0.39,\"rho_p\":3.6e-8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"r\":0.18,\"r_p\":0.46,\"rho\":0.08,\"rho_p\":0.74},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"r\":0.24,\"r_p\":0.34,\"rho\":0.06,\"rho_p\":0.81},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"r\":0.53,\"r_p\":0.04,\"rho\":0.42,\"rho_p\":0.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"r\":0.4,\"r_p\":0.14,\"rho\":0.41,\"rho_p\":0.12},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"r\":0.35,\"r_p\":0.16,\"rho\":0.26,\"rho_p\":0.32},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"r\":0.26,\"r_p\":0.3,\"rho\":0.23,\"rho_p\":0.37},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"r\":0.26,\"r_p\":0.31,\"rho\":0.16,\"rho_p\":0.54},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"r\":0.18,\"r_p\":0.47,\"rho\":0.09,\"rho_p\":0.73},\n",
    "      \n",
    "])\n",
    "plot_river_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f885667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_p68_grid(\n",
    "    df,\n",
    "    river_order=None,\n",
    "    suptitle=\"Bias vs 68%ile Error across rivers\",\n",
    "    bias_col=\"bias_pct\",     # x-axis (e.g., -78.3 means -78.3%)\n",
    "    p68_col=\"p68_pct\",       # y-axis (e.g., 88.0 means 88%)\n",
    "):\n",
    "    \"\"\"\n",
    "    Faceted 2x3 scatter: one subplot per river with shared axes.\n",
    "    Expects tidy df columns: river, setting, bias_col, p68_col.\n",
    "\n",
    "    - Color encodes 'setting'\n",
    "    - Vertical dashed line at Bias = 0\n",
    "    - Legends placed OUTSIDE the grid (no overlap)\n",
    "    \"\"\"\n",
    "\n",
    "    required = {\"river\", \"setting\", bias_col, p68_col}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "    # Stable river order\n",
    "    rivers = list(pd.unique(df[\"river\"]))\n",
    "    if river_order:\n",
    "        rivers = [r for r in river_order if r in rivers] + [r for r in rivers if r not in river_order]\n",
    "    rivers = rivers[:6]\n",
    "\n",
    "    # Palette per setting\n",
    "    setting_order = list(pd.unique(df[\"setting\"].astype(str)))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", max(1, len(setting_order)))\n",
    "    color_map = {s: cmap(i) for i, s in enumerate(setting_order)}\n",
    "\n",
    "    # Global limits (include 0 on x for reference)\n",
    "    xmin = min(0.0, df[bias_col].min()) - 2\n",
    "    xmax = max(0.0, df[bias_col].max()) + 2\n",
    "    ymin = max(0.0, min(df[p68_col].min(), 0)) - 2\n",
    "    ymax = df[p68_col].max() + 2\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(13, 7), dpi=140, sharex=True, sharey=True)\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # One proxy per setting for legend\n",
    "    setting_handles = {}\n",
    "\n",
    "    for ax, river in zip(axes, rivers):\n",
    "        sub = df[df[\"river\"] == river]\n",
    "\n",
    "        for setting, chunk in sub.groupby(\"setting\"):\n",
    "            col = color_map[setting]\n",
    "            ax.scatter(\n",
    "                chunk[bias_col], chunk[p68_col],\n",
    "                s=70, marker=\"o\", facecolor=col, edgecolor=\"black\", linewidth=0.6, alpha=0.95\n",
    "            )\n",
    "            if setting not in setting_handles:\n",
    "                setting_handles[setting] = Line2D(\n",
    "                    [], [], marker='o', linestyle='None',\n",
    "                    markerfacecolor=col, markeredgecolor='black',\n",
    "                    markersize=7, label=str(setting)\n",
    "                )\n",
    "\n",
    "        # Cosmetics\n",
    "        ax.set_title(str(river), fontsize=11)\n",
    "        ax.grid(True, linestyle=\":\", alpha=0.35)\n",
    "        ax.axvline(0, lw=1.0, alpha=0.5, linestyle=\"--\")  # Bias = 0 reference\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "\n",
    "    # Label only left/bottom axes\n",
    "    for i, ax in enumerate(axes):\n",
    "        if i % 3 == 0:\n",
    "            ax.set_ylabel(\"68th Percentile Error (%)\")\n",
    "        if i >= 3:\n",
    "            ax.set_xlabel(\"Bias (%)\")\n",
    "\n",
    "    # Hide unused axes if <6 rivers\n",
    "    for j in range(len(rivers), 6):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    # Settings legend OUTSIDE bottom\n",
    "    fig.legend(\n",
    "        handles=[setting_handles[s] for s in setting_order if s in setting_handles],\n",
    "        labels=[s for s in setting_order if s in setting_handles],\n",
    "        title=\"Setting\",\n",
    "        loc=\"lower center\",\n",
    "        ncol=min(6, len(setting_handles)),\n",
    "        frameon=True,\n",
    "        bbox_to_anchor=(0.5, -0.12)\n",
    "    )\n",
    "\n",
    "    fig.suptitle(suptitle, y=0.99, fontsize=14)\n",
    "    plt.tight_layout(rect=[0.02, 0.05, 0.98, 0.95])  # room for legend & title\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bda7335",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"40 m\",\"bias_pct\":-77.2,\"p68_pct\": 87.7},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"80 m\",\"bias_pct\":-78.3,\"p68_pct\": 88.0},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"10%\",\"bias_pct\":-66.5,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"15%\",\"bias_pct\":-61.6,\"p68_pct\": 81.1},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"20%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"23%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"25%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Cape Fear\",\"setting\":\"30%\",\"bias_pct\":-70.9,\"p68_pct\": 81.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"40 m\",\"bias_pct\":53.4,\"p68_pct\": 99.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"80 m\",\"bias_pct\":19.1,\"p68_pct\": 51.3},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"10%\",\"bias_pct\":64.9,\"p68_pct\": 103.9},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"15%\",\"bias_pct\":59.5,\"p68_pct\": 99.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"20%\",\"bias_pct\":54.3,\"p68_pct\": 95.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"23%\",\"bias_pct\":32.2,\"p68_pct\": 73.1},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"25%\",\"bias_pct\":43.4,\"p68_pct\": 91.6},\n",
    "     {\"river\":\"Atrato\",\"setting\":\"30%\",\"bias_pct\":27.7,\"p68_pct\": 81.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"40 m\",\"bias_pct\":-29.9,\"p68_pct\": 55.9},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"80 m\",\"bias_pct\":-18.6,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"10%\",\"bias_pct\":-22.4,\"p68_pct\": 66.13},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"15%\",\"bias_pct\":-23.4,\"p68_pct\": 67.4},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"20%\",\"bias_pct\":-26.7,\"p68_pct\": 69.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"23%\",\"bias_pct\":-31.5,\"p68_pct\": 58.5},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"25%\",\"bias_pct\":-30.0,\"p68_pct\": 57.0},\n",
    "     {\"river\":\"Sacramento\",\"setting\":\"30%\",\"bias_pct\":-28.9,\"p68_pct\": 53.4},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"40 m\",\"bias_pct\":-41.4,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"80 m\",\"bias_pct\":-16.3,\"p68_pct\": 77.7},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"10%\",\"bias_pct\":-58.5,\"p68_pct\": 71.8},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"15%\",\"bias_pct\":-44.8,\"p68_pct\": 50.5},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"20%\",\"bias_pct\":-38.6,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"23%\",\"bias_pct\":-25.3,\"p68_pct\": 58.1},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"25%\",\"bias_pct\":-13.8,\"p68_pct\": 73.2},\n",
    "     {\"river\":\"Pee Dee\",\"setting\":\"30%\",\"bias_pct\":-3.96,\"p68_pct\": 76.6},\n",
    "     {\"river\":\"Po\",\"setting\":\"40 m\",\"bias_pct\":-13.6,\"p68_pct\": 66.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"80 m\",\"bias_pct\":-4.14,\"p68_pct\": 56.3},\n",
    "     {\"river\":\"Po\",\"setting\":\"10%\",\"bias_pct\":-10.9,\"p68_pct\": 67.7},\n",
    "     {\"river\":\"Po\",\"setting\":\"15%\",\"bias_pct\":-3.23,\"p68_pct\": 61.8},\n",
    "     {\"river\":\"Po\",\"setting\":\"20%\",\"bias_pct\":0.12,\"p68_pct\": 59.9},\n",
    "     {\"river\":\"Po\",\"setting\":\"23%\",\"bias_pct\":1.8,\"p68_pct\": 56.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"25%\",\"bias_pct\":1.6,\"p68_pct\": 53.0},\n",
    "     {\"river\":\"Po\",\"setting\":\"30%\",\"bias_pct\":14.1,\"p68_pct\": 58.6},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"40 m\",\"bias_pct\":-77.9,\"p68_pct\": 86.2},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"80 m\",\"bias_pct\":-78.6,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"10%\",\"bias_pct\":-85.1,\"p68_pct\": 89.1},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"15%\",\"bias_pct\":-84.2,\"p68_pct\": 87.4},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"20%\",\"bias_pct\":-82.0,\"p68_pct\": 86.8},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"23%\",\"bias_pct\":-80.5,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"25%\",\"bias_pct\":-79.9,\"p68_pct\": 86.5},\n",
    "     {\"river\":\"Garonne\",\"setting\":\"30%\",\"bias_pct\":-78.1,\"p68_pct\": 86.2},\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c49ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bias_p68_grid(df, river_order=[\"Cape Fear\",\"Atrato\",\"Sacramento\",\"Pee Dee\",\"Po\",\"Garonne\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd166f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "df = pd.DataFrame([\n",
    "    (\"Atrato\",              0.61, 0.62),\n",
    "    (\"Garonne\",             0.24, 0.34),\n",
    "    (\"Po\",                  0.39, 0.47),\n",
    "    (\"Sacramento\",          0.44, 0.49),\n",
    "    (\"Cape Fear\",           0.31, 0.42),\n",
    "    (\"Pee Dee River\",      -0.11, -0.25),\n",
    "    (\"Pee Dee Upstream\",   -0.25, -0.15),\n",
    "    (\"Pee Dee Downstream\",  0.37, 0.63),\n",
    "], columns=[\"River\", \"Pearson\", \"Spearman\"])\n",
    "\n",
    "# ---------------- Style ----------------\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"#F3F5F7\",\n",
    "    \"axes.edgecolor\": \"0.75\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 13.5,\n",
    "    \"ytick.labelsize\": 13.5,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.color\": \"white\",\n",
    "    \"grid.linewidth\": 1.6,\n",
    "})\n",
    "\n",
    "# Viridis colors (color-blind-friendly)\n",
    "cmap = plt.colormaps.get_cmap(\"viridis\")\n",
    "color_map = {\n",
    "    \"Pearson\":  cmap(0.25),\n",
    "    \"Spearman\": cmap(0.75),\n",
    "}\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(13, 7.2))\n",
    "\n",
    "rivers  = df[\"River\"].tolist()\n",
    "metrics = [\"Pearson\", \"Spearman\"]\n",
    "bar_w   = 0.36\n",
    "x       = np.arange(len(rivers))\n",
    "offsets = [-bar_w/2, bar_w/2]\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax.bar(x + offsets[i], df[m].values, width=bar_w,\n",
    "           label=m, color=color_map[m], edgecolor=\"none\")\n",
    "\n",
    "ax.axhline(0, color=\"0.35\", linewidth=1.2, zorder=0)   # zero line\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rivers, rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Correlation coefficient\")\n",
    "\n",
    "ymin, ymax = df[[\"Pearson\",\"Spearman\"]].min().min(), df[[\"Pearson\",\"Spearman\"]].max().max()\n",
    "pad  = max(0.05, 0.08 * (ymax - ymin or 1))\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "ax.legend(frameon=True, facecolor=\"white\", edgecolor=\"#E0E0E0\",\n",
    "          loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.82)\n",
    "\n",
    "# ---------------- EXPORT ----------------\n",
    "fig.savefig(\"/Users/daniel/Documents/UNC/Presentations/SWOT25/Posterriver_correlations_poster.png\",\n",
    "            dpi=600, bbox_inches=\"tight\", facecolor=\"none\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Saved: river_correlations_poster.png at 600 dpi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dcb940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "df = pd.DataFrame([\n",
    "    (\"Atrato\",               20.4,  51.3),   # Bias, 68%ile error\n",
    "    (\"Garonne\",             -80.0,  88.7),\n",
    "    (\"Po\",                   12.0,  49.0),\n",
    "    (\"Sacramento\",          -16.3,  53.8),\n",
    "    (\"Cape Fear\",           -44.4,  75.6),\n",
    "    (\"Pee Dee River\",       -11.9,  76.5),\n",
    "    (\"Pee Dee Upstream\",     18.7,  74.4),\n",
    "    (\"Pee Dee Downstream\",  -72.9,  77.3),\n",
    "], columns=[\"River\", \"Bias\", \"Error68\"])\n",
    "\n",
    "# ---------------- Style ----------------\n",
    "plt.rcParams.update({\n",
    "    \"font.family\": \"DejaVu Sans\",\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.facecolor\": \"#F3F5F7\",\n",
    "    \"axes.edgecolor\": \"0.75\",\n",
    "    \"axes.linewidth\": 0.8,\n",
    "    \"axes.labelsize\": 16,\n",
    "    \"xtick.labelsize\": 13.5,\n",
    "    \"ytick.labelsize\": 13.5,\n",
    "    \"legend.fontsize\": 14,\n",
    "    \"grid.color\": \"white\",\n",
    "    \"grid.linewidth\": 1.6,\n",
    "})\n",
    "\n",
    "# ----------- New color-blind-safe palette (Okabe‚ÄìIto) -----------\n",
    "c_bias     = \"#D55E00\"   # warm Vermillion\n",
    "c_error68  = \"#009E73\"   # cool Blue-green\n",
    "color_map  = {\"Bias\": c_bias, \"Error68\": c_error68}\n",
    "\n",
    "# ---------------- Tick-step helper ----------------\n",
    "def nice_step(ymin, ymax, max_ticks=12):\n",
    "    span = max(1e-9, ymax - ymin)\n",
    "    for s in [1, 2, 5, 10, 20, 25, 50]:\n",
    "        if math.ceil(span / s) + 1 <= max_ticks:\n",
    "            return s\n",
    "    return span / max_ticks\n",
    "\n",
    "# ---------------- Plot ----------------\n",
    "fig, ax = plt.subplots(figsize=(13, 7.2))\n",
    "\n",
    "rivers  = df[\"River\"].tolist()\n",
    "metrics = [\"Bias\", \"Error68\"]\n",
    "bar_w   = 0.36\n",
    "x       = np.arange(len(rivers))\n",
    "offsets = [-bar_w/2, bar_w/2]\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "for i, m in enumerate(metrics):\n",
    "    ax.bar(x + offsets[i], df[m].values,\n",
    "           width=bar_w, label=m, color=color_map[m], edgecolor=\"none\")\n",
    "\n",
    "ax.axhline(0, color=\"0.35\", linewidth=1.2, zorder=0)\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(rivers, rotation=15, ha=\"right\")\n",
    "ax.set_ylabel(\"Bias / 68%ile error (%)\")\n",
    "\n",
    "# --- Y limits & tick spacing ---\n",
    "ymin = float(df[[\"Bias\",\"Error68\"]].min().min())\n",
    "ymax = float(df[[\"Bias\",\"Error68\"]].max().max())\n",
    "pad  = max(1.0, 0.08 * (ymax - ymin if ymax != ymin else 1.0))\n",
    "ax.set_ylim(ymin - pad, ymax + pad)\n",
    "\n",
    "step = nice_step(*ax.get_ylim(), max_ticks=12)\n",
    "ax.yaxis.set_major_locator(MultipleLocator(step))\n",
    "\n",
    "# Clean look\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Legend\n",
    "ax.legend(frameon=True, facecolor=\"white\", edgecolor=\"#E0E0E0\",\n",
    "          loc=\"center left\", bbox_to_anchor=(1.02, 0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(right=0.82)\n",
    "\n",
    "# ---------------- EXPORT ----------------\n",
    "fig.savefig(\"river_bias_error68_poster_altpalette.png\",\n",
    "            dpi=600, bbox_inches=\"tight\", facecolor=\"white\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Saved: river_bias_error68_poster_altpalette.png at 600 dpi\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
